{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Designing Agents for the Sailing Challenge\n",
    "\n",
    "In this notebook, we'll explore how to design and implement agents for the Sailing Challenge. We'll cover:\n",
    "\n",
    "1. The requirements and interface for valid agents\n",
    "2. Understanding the greedy agent example \n",
    "3. Implementing a simple reinforcement learning agent\n",
    "\n",
    "By the end of this notebook, you'll have a clear understanding of how to create your own agents that can navigate the sailing environment effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Requirements\n",
    "\n",
    "All agents in the Sailing Challenge must implement a specific interface defined by the `BaseAgent` abstract class. Let's examine this class to understand what's required:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Add the src directory to the path\n",
    "sys.path.append(os.path.abspath('../src'))\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "# Import the BaseAgent class\n",
    "from src.agents.base_agent import BaseAgent\n",
    "\n",
    "# Display the BaseAgent class documentation\n",
    "#help(BaseAgent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required Methods\n",
    "\n",
    "As we can see from the `BaseAgent` class, any valid agent must implement:\n",
    "\n",
    "1. **`act(observation)`**: The core decision-making method that takes the current observation and returns an action\n",
    "   - Input: A numpy array containing [x, y, vx, vy, wx, wy, ...] representing position, velocity, wind, and the full wind field\n",
    "   - Output: An integer in the range [0-8] representing the action to take\n",
    "\n",
    "2. **`reset()`**: Resets the agent's internal state at the beginning of each episode\n",
    "   - This is particularly important for agents that maintain memory or state across steps\n",
    "\n",
    "3. **`seed(seed)`**: Sets the random seed for the agent to ensure reproducibility\n",
    "   - This is crucial for evaluation and comparison of different agents\n",
    "\n",
    "Additionally, while not strictly required, implementing `save()` and `load()` methods is recommended for storing and retrieving trained agent parameters.\n",
    "\n",
    "### The Validation Process\n",
    "\n",
    "When you submit an agent, it will be automatically validated against these requirements. The validation process checks:\n",
    "\n",
    "1. If the agent class inherits from `BaseAgent`\n",
    "2. If all required methods are implemented with correct parameters\n",
    "3. If the agent produces valid actions (integers in range [0-8])\n",
    "4. If the agent can interact with the environment without errors\n",
    "\n",
    "Let's create a minimal valid agent to understand this process better:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinimalAgent(BaseAgent):\n",
    "    \"\"\"A minimal valid agent that meets all interface requirements.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.np_random = np.random.default_rng()\n",
    "    \n",
    "    def act(self, observation: np.ndarray) -> int:\n",
    "        \"\"\"Choose an action randomly.\"\"\"\n",
    "        return self.np_random.integers(0, 9)  # Random action from 0-8\n",
    "    \n",
    "    def reset(self) -> None:\n",
    "        \"\"\"Reset the agent.\"\"\"\n",
    "        pass  # Nothing to reset in this simple agent\n",
    "    \n",
    "    def seed(self, seed: int = None) -> None:\n",
    "        \"\"\"Set the random seed.\"\"\"\n",
    "        self.np_random = np.random.default_rng(seed)\n",
    "\n",
    "# Create an instance of our minimal agent\n",
    "minimal_agent = MinimalAgent()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Agent's Validity\n",
    "\n",
    "Let's make the agent do a few steps to check that everything is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running the minimal agent for 5 steps:\n",
      "Step 1: Action=0, Position=[16  1], Reward=0.0\n",
      "Step 2: Action=6, Position=[15  1], Reward=0.0\n",
      "Step 3: Action=5, Position=[14  0], Reward=0.0\n",
      "Step 4: Action=3, Position=[15  0], Reward=0.0\n",
      "Step 5: Action=3, Position=[16  0], Reward=0.0\n"
     ]
    }
   ],
   "source": [
    "# Instead of validating the agent here, we'll just demonstrate it on a simple task\n",
    "from src.env_sailing import SailingEnv\n",
    "\n",
    "# Create a simple environment\n",
    "env = SailingEnv()\n",
    "observation, info = env.reset(seed=42)\n",
    "\n",
    "# Initialize our minimal agent\n",
    "minimal_agent = MinimalAgent()\n",
    "minimal_agent.seed(42)\n",
    "\n",
    "# Run the agent for a few steps\n",
    "print(\"Running the minimal agent for 5 steps:\")\n",
    "for i in range(5):\n",
    "    action = minimal_agent.act(observation)\n",
    "    observation, reward, done, truncated, info = env.step(action)\n",
    "    print(f\"Step {i+1}: Action={action}, Position={info['position']}, Reward={reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validating Your Agent\n",
    "\n",
    "After creating your agent, you'll want to ensure it meets all the requirements of the challenge. There are two ways to validate your agent:\n",
    "\n",
    "1. **Using the `validate_agent.ipynb` notebook:**\n",
    "   - This notebook provides a comprehensive interface for testing your agent\n",
    "   - It shows detailed validation results and explains any issues\n",
    "\n",
    "2. **Using the command line:**\n",
    "   ```bash\n",
    "   cd src\n",
    "   python test_agent_validity.py path/to/your_agent.py\n",
    "   ```\n",
    "\n",
    "We recommend using these tools after you've completed your agent implementation rather than trying to validate it during development.\n",
    "\n",
    "For now, let's focus on understanding agent design principles and implementing effective strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Observation and Action Space\n",
    "\n",
    "To design effective agents, it's important to understand:\n",
    "\n",
    "1. **What information is available to the agent (observations)**\n",
    "2. **What actions the agent can take**\n",
    "\n",
    "### Observation Space\n",
    "\n",
    "The observation provided to your agent is a numpy array with the following structure:\n",
    "\n",
    "`[x, y, vx, vy, wx, wy, flattened_wind_field]`\n",
    "\n",
    "\n",
    "Where:\n",
    "- `x, y`: Current position (grid coordinates)\n",
    "- `vx, vy`: Current velocity vector \n",
    "- `wx, wy`: Wind vector at the current position\n",
    "- `flattened_wind_field`: The entire wind field (can be reshaped to grid_size × grid_size × 2)\n",
    "\n",
    "For simpler agents, you might only need to use the first 6 values. More sophisticated agents can use the full wind field to plan ahead.\n",
    "\n",
    "### Action Space\n",
    "\n",
    "The agent can choose from 9 possible actions:\n",
    "\n",
    "- 0: Move North (up)\n",
    "- 1: Move Northeast\n",
    "- 2: Move East (right)\n",
    "- 3: Move Southeast\n",
    "- 4: Move South (down)\n",
    "- 5: Move Southwest\n",
    "- 6: Move West (left)\n",
    "- 7: Move Northwest\n",
    "- 8: Stay in place\n",
    "\n",
    "Each action represents a desired direction for the boat to move. However, the actual movement will be influenced by the wind and sailing physics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Naive Agent Example\n",
    "\n",
    "Let's examine the built-in `NaiveAgent`, which provides a simple baseline implementation. This agent always tries to move North (toward the goal), regardless of wind conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class NaiveAgent(BaseAgent):\n",
      "    \"\"\"\n",
      "    A naive agent for the Sailing Challenge.\n",
      "    \n",
      "    This is a very simple agent that always chooses to go North,\n",
      "    regardless of wind conditions or position. It serves as a minimal\n",
      "    working example that students can build upon.\n",
      "    \"\"\"\n",
      "    \n",
      "    def __init__(self):\n",
      "        \"\"\"Initialize the agent.\"\"\"\n",
      "        super().__init__()\n",
      "        self.np_random = np.random.default_rng()\n",
      "    \n",
      "    def act(self, observation: np.ndarray) -> int:\n",
      "        \"\"\"\n",
      "        Select an action based on the current observation.\n",
      "        \n",
      "        Args:\n",
      "            observation: A numpy array containing the current observation.\n",
      "                Format: [x, y, vx, vy, wx, wy] where:\n",
      "                - (x, y) is the current position\n",
      "                - (vx, vy) is the current velocity\n",
      "                - (wx, wy) is the current wind vector\n",
      "        \n",
      "        Returns:\n",
      "            action: An integer in [0, 8] representing the action to take:\n",
      "                - 0: Move North\n",
      "                - 1: Move Northeast\n",
      "                - 2: Move East\n",
      "                - 3: Move Southeast\n",
      "                - 4: Move South\n",
      "                - 5: Move Southwest\n",
      "                - 6: Move West\n",
      "                - 7: Move Northwest\n",
      "                - 8: Stay in place\n",
      "        \"\"\"\n",
      "        # This agent always chooses to go North (action 0)\n",
      "        return 0\n",
      "    \n",
      "    def reset(self) -> None:\n",
      "        \"\"\"Reset the agent's internal state between episodes.\"\"\"\n",
      "        # Nothing to reset for this simple agent\n",
      "        pass\n",
      "        \n",
      "    def seed(self, seed: int = None) -> None:\n",
      "        \"\"\"Set the random seed for reproducibility.\"\"\"\n",
      "        self.np_random = np.random.default_rng(seed)\n",
      "        \n",
      "    def save(self, path: str) -> None:\n",
      "        \"\"\"\n",
      "        Save the agent's learned parameters to a file.\n",
      "        \n",
      "        Args:\n",
      "            path: Path to save the agent's state\n",
      "        \"\"\"\n",
      "        # No parameters to save for this simple agent\n",
      "        pass\n",
      "        \n",
      "    def load(self, path: str) -> None:\n",
      "        \"\"\"\n",
      "        Load the agent's learned parameters from a file.\n",
      "        \n",
      "        Args:\n",
      "            path: Path to load the agent's state from\n",
      "        \"\"\"\n",
      "        # No parameters to load for this simple agent\n",
      "        pass \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the NaiveAgent\n",
    "from src.agents.agent_naive import NaiveAgent\n",
    "\n",
    "# Display the source code\n",
    "import inspect\n",
    "print(inspect.getsource(NaiveAgent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of the Naive Agent\n",
    "\n",
    "The `NaiveAgent` is extremely simple but illustrates the key requirements for a valid agent:\n",
    "\n",
    "1. **Inheritance**: It inherits from `BaseAgent`\n",
    "2. **Required Methods**: It implements all required methods (`act`, `reset`, `seed`)\n",
    "3. **Action Selection**: It always returns action `0` (North)\n",
    "4. **Simplicity**: It maintains no internal state and requires no complex logic\n",
    "\n",
    "This agent provides a good baseline, but it has obvious limitations:\n",
    "\n",
    "- It ignores wind conditions completely\n",
    "- It will struggle when the wind is coming from the North\n",
    "- It doesn't adapt its strategy based on the environment\n",
    "\n",
    "Let's test the naive agent to see how well it performs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running the naive agent on the simple_static initial windfield:\n",
      "Step 10: Position=[16  7], Reward=0.0\n",
      "Step 20: Position=[16 12], Reward=0.0\n",
      "Step 30: Position=[16 13], Reward=0.0\n",
      "Step 40: Position=[16 13], Reward=0.0\n",
      "Step 50: Position=[16 14], Reward=0.0\n",
      "Step 60: Position=[16 14], Reward=0.0\n",
      "Step 70: Position=[16 15], Reward=0.0\n",
      "Step 80: Position=[16 15], Reward=0.0\n",
      "Step 90: Position=[16 16], Reward=0.0\n",
      "Step 100: Position=[16 16], Reward=0.0\n",
      "Step 110: Position=[16 17], Reward=0.0\n",
      "Step 120: Position=[16 18], Reward=0.0\n",
      "Step 130: Position=[16 18], Reward=0.0\n",
      "Step 140: Position=[16 19], Reward=0.0\n",
      "Step 150: Position=[16 19], Reward=0.0\n",
      "Step 160: Position=[16 20], Reward=0.0\n",
      "Step 170: Position=[16 20], Reward=0.0\n",
      "Step 180: Position=[16 21], Reward=0.0\n",
      "Step 190: Position=[16 21], Reward=0.0\n",
      "Step 200: Position=[16 22], Reward=0.0\n",
      "Step 210: Position=[16 22], Reward=0.0\n",
      "Step 220: Position=[16 23], Reward=0.0\n",
      "Step 230: Position=[16 24], Reward=0.0\n",
      "Step 240: Position=[16 24], Reward=0.0\n",
      "Step 250: Position=[16 25], Reward=0.0\n",
      "Step 260: Position=[16 25], Reward=0.0\n",
      "Step 270: Position=[16 26], Reward=0.0\n",
      "Step 280: Position=[16 27], Reward=0.0\n",
      "Step 290: Position=[16 27], Reward=0.0\n",
      "Step 300: Position=[16 29], Reward=0.0\n",
      "\n",
      "Episode finished after 301 steps with reward: 100.0\n",
      "Final position: [16 30]\n",
      "Goal reached: True\n"
     ]
    }
   ],
   "source": [
    "from src.env_sailing import SailingEnv\n",
    "from src.initial_windfields import get_initial_windfield\n",
    "\n",
    "# Create an environment with a simple test initial windfield\n",
    "env = SailingEnv(**get_initial_windfield('simple_static'))\n",
    "naive_agent = NaiveAgent()\n",
    "\n",
    "# Run a single episode\n",
    "observation, info = env.reset(seed=42)\n",
    "total_reward = 0\n",
    "done = False\n",
    "truncated = False\n",
    "step_count = 0\n",
    "\n",
    "print(\"Running the naive agent on the simple_static initial windfield:\")\n",
    "while not (done or truncated) and step_count < 1000:  # Limit to 100 steps\n",
    "    action = naive_agent.act(observation)\n",
    "    observation, reward, done, truncated, info = env.step(action)\n",
    "    total_reward += reward\n",
    "    step_count += 1\n",
    "    \n",
    "    # Print every 10 steps to avoid too much output\n",
    "    if step_count % 10 == 0:\n",
    "        print(f\"Step {step_count}: Position={info['position']}, Reward={reward}\")\n",
    "\n",
    "print(f\"\\nEpisode finished after {step_count} steps with reward: {total_reward}\")\n",
    "print(f\"Final position: {info['position']}\")\n",
    "print(f\"Goal reached: {done}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving on the Naive Agent\n",
    "\n",
    "The naive agent provides a good starting point, but there are many ways to improve it:\n",
    "\n",
    "1. **Wind-Aware Agent**: Consider wind direction when choosing actions\n",
    "2. **Goal-Directed Agent**: Calculate the direction to the goal and choose actions accordingly\n",
    "3. **Physics-Based Agent**: Use sailing physics equations to determine the most efficient action\n",
    "\n",
    "The key insight for sailing is that certain directions relative to the wind are more efficient than others:\n",
    "\n",
    "- The sailing efficiency is highest when moving perpendicular to the wind (beam reach)\n",
    "- It's difficult to sail directly into the wind (the \"no-go zone\" - less than 45° to the wind)\n",
    "- The boat maintains momentum (inertia) between steps\n",
    "\n",
    "Before diving into reinforcement learning, consider implementing a simple rule-based agent that incorporates these physics principles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amélioration d'un agent naive avec le vent l'objectif et les équations physiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedSailingAgent(BaseAgent):\n",
    "    \"\"\"\n",
    "    Un agent de voile amélioré qui utilise les principes de base de la physique de la navigation à voile.\n",
    "    Cet agent tient compte de:\n",
    "    1. La direction du vent\n",
    "    2. L'angle optimal par rapport au vent\n",
    "    3. La position de l'objectif\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.np_random = np.random.default_rng()\n",
    "        # Définir les angles optimaux pour la navigation\n",
    "        self.optimal_wind_angles = [90, 120]  # Angles en degrés (perpendiculaire et grand largue)\n",
    "        # La zone de non-navigation (no-go zone) est typiquement ±45° face au vent\n",
    "        self.no_go_angle = 45\n",
    "    \n",
    "    def act(self, observation: np.ndarray) -> int:\n",
    "        \"\"\"\n",
    "        Choisir une action en fonction des conditions de vent et de la position de l'objectif.\n",
    "        \n",
    "        Args:\n",
    "            observation: Un tableau numpy contenant l'observation actuelle.\n",
    "                Format: [x, y, vx, vy, wx, wy, ...] où:\n",
    "                - (x, y) est la position actuelle\n",
    "                - (vx, vy) est la vitesse actuelle\n",
    "                - (wx, wy) est le vecteur de vent actuel\n",
    "                - le reste contient le champ de vent complet\n",
    "        \n",
    "        Returns:\n",
    "            int: L'action choisie (0-8)\n",
    "        \"\"\"\n",
    "        # Extraire les informations pertinentes de l'observation\n",
    "        position = observation[:2]  # [x, y]\n",
    "        velocity = observation[2:4]  # [vx, vy]\n",
    "        wind = observation[4:6]  # [wx, wy]\n",
    "        \n",
    "        # Dans un environnement SailingEnv typique, l'objectif est en haut de la grille\n",
    "        # Nous supposons que l'objectif est à la position [16, 31] basé sur l'exemple\n",
    "        goal_position = np.array([16, 31])\n",
    "        \n",
    "        # Calculer le vecteur vers l'objectif\n",
    "        direction_to_goal = goal_position - position\n",
    "        distance_to_goal = np.linalg.norm(direction_to_goal)\n",
    "        \n",
    "        # Si nous sommes très proches de l'objectif, essayer de l'atteindre directement\n",
    "        if distance_to_goal < 2.0:\n",
    "            return self._get_action_towards_direction(direction_to_goal)\n",
    "        \n",
    "        # Calculer l'angle du vent (en degrés)\n",
    "        wind_angle = np.degrees(np.arctan2(wind[1], wind[0]))\n",
    "        \n",
    "        # Calculer l'angle vers l'objectif (en degrés)\n",
    "        goal_angle = np.degrees(np.arctan2(direction_to_goal[1], direction_to_goal[0]))\n",
    "        \n",
    "        # Angle relatif entre le vent et la direction vers l'objectif\n",
    "        relative_angle = (goal_angle - wind_angle) % 360\n",
    "        \n",
    "        # Vérifier si l'objectif est dans la zone de non-navigation\n",
    "        if relative_angle < self.no_go_angle or relative_angle > 360 - self.no_go_angle:\n",
    "            # L'objectif est face au vent, nous devons tacker (zigzaguer)\n",
    "            return self._tacking_strategy(wind_angle, position)\n",
    "        else:\n",
    "            # Nous pouvons naviguer plus directement vers l'objectif\n",
    "            # Mais ajustons notre angle pour optimiser la vitesse\n",
    "            return self._optimize_sailing_angle(wind_angle, goal_angle, direction_to_goal)\n",
    "    \n",
    "    def _get_action_towards_direction(self, direction):\n",
    "        \"\"\"Convertit un vecteur de direction en action discrète (0-8).\"\"\"\n",
    "        # Normaliser le vecteur de direction\n",
    "        if np.linalg.norm(direction) > 0:\n",
    "            direction = direction / np.linalg.norm(direction)\n",
    "        \n",
    "        # Cartographier les 8 directions aux actions\n",
    "        # [N, NE, E, SE, S, SW, W, NW]\n",
    "        actions_map = [\n",
    "            (0, 1),    # 0: Nord\n",
    "            (1, 1),    # 1: Nord-Est\n",
    "            (1, 0),    # 2: Est\n",
    "            (1, -1),   # 3: Sud-Est\n",
    "            (0, -1),   # 4: Sud\n",
    "            (-1, -1),  # 5: Sud-Ouest\n",
    "            (-1, 0),   # 6: Ouest\n",
    "            (-1, 1),   # 7: Nord-Ouest\n",
    "        ]\n",
    "        \n",
    "        # Calculer l'action qui correspond le mieux à la direction\n",
    "        best_action = 0\n",
    "        best_similarity = -float('inf')\n",
    "        \n",
    "        for i, action_dir in enumerate(actions_map):\n",
    "            similarity = direction[0] * action_dir[0] + direction[1] * action_dir[1]\n",
    "            if similarity > best_similarity:\n",
    "                best_similarity = similarity\n",
    "                best_action = i\n",
    "        \n",
    "        return best_action\n",
    "    \n",
    "    def _tacking_strategy(self, wind_angle, position):\n",
    "        \"\"\"\n",
    "        Implémente une stratégie de tacking (virement de bord) lorsque l'objectif est face au vent.\n",
    "        Alterne entre naviguer à environ 45-60° de chaque côté du vent.\n",
    "        \"\"\"\n",
    "        # Décider de quel côté virer en fonction de la position x\n",
    "        # Cela crée un zigzag naturel\n",
    "        if position[0] < 16:  # À gauche du centre, virer à tribord\n",
    "            tack_angle = wind_angle + 60\n",
    "        else:  # À droite du centre, virer à bâbord\n",
    "            tack_angle = wind_angle - 60\n",
    "        \n",
    "        # Convertir l'angle en vecteur de direction\n",
    "        tack_direction = np.array([np.cos(np.radians(tack_angle)), np.sin(np.radians(tack_angle))])\n",
    "        \n",
    "        # Obtenir l'action correspondante\n",
    "        return self._get_action_towards_direction(tack_direction)\n",
    "    \n",
    "    def _optimize_sailing_angle(self, wind_angle, goal_angle, direction_to_goal):\n",
    "        \"\"\"\n",
    "        Optimise l'angle de navigation pour maximiser la vitesse,\n",
    "        en se basant sur les angles optimaux par rapport au vent.\n",
    "        \"\"\"\n",
    "        # Trouver l'angle optimal le plus proche de notre direction vers l'objectif\n",
    "        best_angle = goal_angle  # Par défaut, viser directement l'objectif\n",
    "        \n",
    "        # Calculer les angles absolus optimaux (dans les deux sens)\n",
    "        optimal_angles = []\n",
    "        for optimal in self.optimal_wind_angles:\n",
    "            optimal_angles.append((wind_angle + optimal) % 360)\n",
    "            optimal_angles.append((wind_angle - optimal) % 360)\n",
    "        \n",
    "        # Trouver l'angle optimal le plus proche de notre direction vers l'objectif\n",
    "        angle_diff = [abs((angle - goal_angle) % 360) for angle in optimal_angles]\n",
    "        min_diff_index = np.argmin(angle_diff)\n",
    "        \n",
    "        # Si la différence d'angle est significative, utiliser l'angle optimal\n",
    "        if angle_diff[min_diff_index] < 45:\n",
    "            best_angle = optimal_angles[min_diff_index]\n",
    "        \n",
    "        # Convertir l'angle en vecteur de direction\n",
    "        optimal_direction = np.array([np.cos(np.radians(best_angle)), np.sin(np.radians(best_angle))])\n",
    "        \n",
    "        # Mélanger la direction optimale avec la direction vers l'objectif\n",
    "        # Cela permet de progresser vers l'objectif tout en maintenant une bonne vitesse\n",
    "        mixed_direction = 0.7 * optimal_direction + 0.3 * (direction_to_goal / np.linalg.norm(direction_to_goal))\n",
    "        \n",
    "        return self._get_action_towards_direction(mixed_direction)\n",
    "    \n",
    "    def reset(self) -> None:\n",
    "        \"\"\"Réinitialiser l'agent.\"\"\"\n",
    "        pass  # Rien à réinitialiser pour cet agent simple\n",
    "    \n",
    "    def seed(self, seed: int = None) -> None:\n",
    "        \"\"\"Définir la graine aléatoire.\"\"\"\n",
    "        self.np_random = np.random.default_rng(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running the naive agent on the simple_static initial windfield:\n",
      "Step 10: Position=[ 6 11], Reward=0.0\n",
      "Step 20: Position=[ 6 11], Reward=0.0\n",
      "Step 30: Position=[ 7 12], Reward=0.0\n",
      "Step 40: Position=[ 8 13], Reward=0.0\n",
      "Step 50: Position=[ 8 13], Reward=0.0\n",
      "Step 60: Position=[ 8 15], Reward=0.0\n",
      "Step 70: Position=[ 7 17], Reward=0.0\n",
      "Step 80: Position=[ 7 17], Reward=0.0\n",
      "Step 90: Position=[ 8 18], Reward=0.0\n",
      "Step 100: Position=[ 9 19], Reward=0.0\n",
      "Step 110: Position=[ 9 19], Reward=0.0\n",
      "Step 120: Position=[10 20], Reward=0.0\n",
      "Step 130: Position=[10 20], Reward=0.0\n",
      "Step 140: Position=[11 21], Reward=0.0\n",
      "Step 150: Position=[11 21], Reward=0.0\n",
      "Step 160: Position=[11 23], Reward=0.0\n",
      "Step 170: Position=[11 24], Reward=0.0\n",
      "Step 180: Position=[12 24], Reward=0.0\n",
      "Step 190: Position=[13 25], Reward=0.0\n",
      "Step 200: Position=[13 25], Reward=0.0\n",
      "Step 210: Position=[13 27], Reward=0.0\n",
      "Step 220: Position=[13 27], Reward=0.0\n",
      "Step 230: Position=[14 28], Reward=0.0\n",
      "Step 240: Position=[14 29], Reward=0.0\n",
      "Step 250: Position=[14 30], Reward=0.0\n",
      "Step 260: Position=[14 31], Reward=0.0\n",
      "\n",
      "Episode finished after 265 steps with reward: 100.0\n",
      "Final position: [15 31]\n",
      "Goal reached: True\n"
     ]
    }
   ],
   "source": [
    "from src.env_sailing import SailingEnv\n",
    "from src.initial_windfields import get_initial_windfield\n",
    "\n",
    "# Create an environment with a simple test initial windfield\n",
    "env = SailingEnv(**get_initial_windfield('simple_static'))\n",
    "naive_agent = ImprovedSailingAgent()\n",
    "\n",
    "# Run a single episode\n",
    "observation, info = env.reset(seed=42)\n",
    "total_reward = 0\n",
    "done = False\n",
    "truncated = False\n",
    "step_count = 0\n",
    "\n",
    "print(\"Running the naive agent on the simple_static initial windfield:\")\n",
    "while not (done or truncated) and step_count < 1000:  # Limit to 100 steps\n",
    "    action = naive_agent.act(observation)\n",
    "    observation, reward, done, truncated, info = env.step(action)\n",
    "    total_reward += reward\n",
    "    step_count += 1\n",
    "    \n",
    "    # Print every 10 steps to avoid too much output\n",
    "    if step_count % 10 == 0:\n",
    "        print(f\"Step {step_count}: Position={info['position']}, Reward={reward}\")\n",
    "\n",
    "print(f\"\\nEpisode finished after {step_count} steps with reward: {total_reward}\")\n",
    "print(f\"Final position: {info['position']}\")\n",
    "print(f\"Goal reached: {done}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedSailingAgent(BaseAgent):\n",
    "    \"\"\"\n",
    "    Agent de voile avancé avec optimisation complète basée sur la physique de la voile\n",
    "    et une planification de trajectoire intelligente.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.np_random = np.random.default_rng()\n",
    "        \n",
    "        # Paramètres de navigation à voile\n",
    "        self.polar_data = self._create_sailing_polar()  # Données d'efficacité polaire\n",
    "        self.tacking_distance = 3.0  # Distance minimale avant de virer de bord\n",
    "        self.last_tack_position = None  # Pour éviter le virement de bord trop fréquent\n",
    "        self.last_actions = []  # Historique des actions pour la stabilité\n",
    "        self.action_history_size = 3  # Nombre d'actions à stocker dans l'historique\n",
    "        self.wind_prediction_horizon = 5  # Nombre de pas pour la prédiction du vent\n",
    "        \n",
    "        # Paramètres adaptatifs\n",
    "        self.adaptation_rate = 0.1  # Taux d'adaptation aux conditions\n",
    "        self.performance_history = []  # Historique des performances\n",
    "        \n",
    "        # Stratégie globale\n",
    "        self.upwind_strategy = \"tacking\"  # tacking ou long_tacking\n",
    "        self.downwind_strategy = \"direct\"  # direct ou gybing\n",
    "        \n",
    "        # État de navigation\n",
    "        self.current_leg = \"upwind\"  # upwind, reaching, downwind\n",
    "        self.tack_side = \"port\"  # port (bâbord) ou starboard (tribord)\n",
    "        \n",
    "    def _create_sailing_polar(self):\n",
    "        \"\"\"\n",
    "        Crée une table polaire d'efficacité de navigation basée sur l'angle au vent.\n",
    "        Renvoie un dictionnaire avec {angle: efficacité} où l'angle est en degrés.\n",
    "        \"\"\"\n",
    "        polar = {}\n",
    "        \n",
    "        # Zone de non-navigation (irons) - très faible efficacité\n",
    "        for angle in range(0, 46):\n",
    "            polar[angle] = 0.1\n",
    "            polar[360-angle] = 0.1\n",
    "        \n",
    "        # Près (close-hauled) - efficacité modérée\n",
    "        for angle in range(46, 80):\n",
    "            factor = (angle - 45) / 35  # Augmentation progressive de l'efficacité\n",
    "            polar[angle] = 0.3 + 0.4 * factor\n",
    "            polar[360-angle] = 0.3 + 0.4 * factor\n",
    "        \n",
    "        # Travers (beam reach) - haute efficacité\n",
    "        for angle in range(80, 120):\n",
    "            polar[angle] = 0.9 + 0.1 * np.sin((angle - 80) * np.pi / 80)\n",
    "            polar[360-angle] = 0.9 + 0.1 * np.sin((angle - 80) * np.pi / 80)\n",
    "        \n",
    "        # Grand largue (broad reach) - très haute efficacité\n",
    "        for angle in range(120, 150):\n",
    "            polar[angle] = 0.95\n",
    "            polar[360-angle] = 0.95\n",
    "        \n",
    "        # Vent arrière (running) - haute efficacité mais pas optimale\n",
    "        for angle in range(150, 181):\n",
    "            factor = (180 - angle) / 30  # Diminution progressive\n",
    "            polar[angle] = 0.85 + 0.1 * factor\n",
    "            polar[360-angle] = 0.85 + 0.1 * factor\n",
    "        \n",
    "        return polar\n",
    "    \n",
    "    def act(self, observation: np.ndarray) -> int:\n",
    "        \"\"\"\n",
    "        Sélectionne l'action optimale basée sur une analyse complète des conditions.\n",
    "        \"\"\"\n",
    "        # Extraire les informations de l'observation\n",
    "        position = observation[:2]  # [x, y]\n",
    "        velocity = observation[2:4]  # [vx, vy]\n",
    "        wind_at_boat = observation[4:6]  # [wx, wy]\n",
    "        \n",
    "        # Extraire et analyser le champ de vent complet\n",
    "        wind_field_data = observation[6:]\n",
    "        wind_field = self._reconstruct_wind_field(wind_field_data)\n",
    "        \n",
    "        # Définir la position de l'objectif (basée sur l'exemple précédent)\n",
    "        goal_position = np.array([16, 31])\n",
    "        \n",
    "        # Calcul des vecteurs et angles principaux\n",
    "        direction_to_goal = goal_position - position\n",
    "        distance_to_goal = np.linalg.norm(direction_to_goal)\n",
    "        \n",
    "        wind_speed = np.linalg.norm(wind_at_boat)\n",
    "        wind_angle = np.degrees(np.arctan2(wind_at_boat[1], wind_at_boat[0])) % 360\n",
    "        \n",
    "        goal_angle = np.degrees(np.arctan2(direction_to_goal[1], direction_to_goal[0])) % 360\n",
    "        \n",
    "        # Angle du bateau (basé sur la vitesse si disponible)\n",
    "        boat_angle = None\n",
    "        if np.linalg.norm(velocity) > 0.01:\n",
    "            boat_angle = np.degrees(np.arctan2(velocity[1], velocity[0])) % 360\n",
    "        else:\n",
    "            # Si le bateau n'a pas de vitesse significative, utiliser l'angle vers l'objectif\n",
    "            boat_angle = goal_angle\n",
    "        \n",
    "        # Angle relatif entre le vent et la direction du bateau\n",
    "        wind_relative_angle = (boat_angle - wind_angle) % 360\n",
    "        \n",
    "        # Analyse de l'environnement\n",
    "        upwind_condition = self._is_upwind(goal_angle, wind_angle)\n",
    "        downwind_condition = self._is_downwind(goal_angle, wind_angle)\n",
    "        \n",
    "        # Déterminer la stratégie en fonction de la situation\n",
    "        if distance_to_goal < 2.0:\n",
    "            # Près de l'objectif - naviguer directement\n",
    "            return self._get_action_towards_direction(direction_to_goal)\n",
    "        elif upwind_condition:\n",
    "            # Navigation contre le vent - utiliser la stratégie de louvoyage\n",
    "            return self._upwind_strategy(position, wind_angle, boat_angle, goal_position, wind_field)\n",
    "        elif downwind_condition:\n",
    "            # Navigation avec le vent arrière\n",
    "            return self._downwind_strategy(position, wind_angle, boat_angle, goal_position, wind_field)\n",
    "        else:\n",
    "            # Navigation au travers ou au largue - optimiser l'angle\n",
    "            return self._optimal_reaching_strategy(position, velocity, wind_angle, boat_angle, goal_position, wind_field)\n",
    "    \n",
    "    def _reconstruct_wind_field(self, wind_field_data):\n",
    "        \"\"\"\n",
    "        Reconstruit le champ de vent à partir des données aplaties.\n",
    "        Pour simplifier, nous retournons les données brutes ici.\n",
    "        Dans une implémentation complète, vous reconstruiriez une grille 2D.\n",
    "        \"\"\"\n",
    "        return wind_field_data\n",
    "    \n",
    "    def _is_upwind(self, goal_angle, wind_angle):\n",
    "        \"\"\"Détermine si l'objectif est face au vent (± zone de non-navigation).\"\"\"\n",
    "        relative_angle = (goal_angle - wind_angle) % 360\n",
    "        return relative_angle < 50 or relative_angle > 310\n",
    "    \n",
    "    def _is_downwind(self, goal_angle, wind_angle):\n",
    "        \"\"\"Détermine si l'objectif est sous le vent.\"\"\"\n",
    "        relative_angle = (goal_angle - wind_angle) % 360\n",
    "        return 160 < relative_angle < 200\n",
    "    \n",
    "    def _get_sailing_efficiency(self, wind_relative_angle):\n",
    "        \"\"\"Obtient l'efficacité de navigation pour un angle donné par rapport au vent.\"\"\"\n",
    "        angle = int(wind_relative_angle % 360)\n",
    "        return self.polar_data.get(angle, 0.5)  # Valeur par défaut si l'angle n'est pas dans la table\n",
    "    \n",
    "    def _upwind_strategy(self, position, wind_angle, boat_angle, goal_position, wind_field):\n",
    "        \"\"\"\n",
    "        Stratégie de navigation au près (contre le vent).\n",
    "        Implémente le louvoyage (tacking) de manière optimisée.\n",
    "        \"\"\"\n",
    "        # Calculer la ligne imaginaire de progression vers l'objectif\n",
    "        goal_direction = goal_position - position\n",
    "        \n",
    "        # Déterminer le côté du tack en fonction de la position actuelle\n",
    "        # Stratégie: zigzag autour de la ligne vers l'objectif\n",
    "        cross_track = self._cross_track_distance(position, goal_position)\n",
    "        \n",
    "        # Changement de bord si nécessaire\n",
    "        if cross_track > self.tacking_distance:\n",
    "            self.tack_side = \"port\" if self.tack_side == \"starboard\" else \"starboard\"\n",
    "            self.last_tack_position = position.copy()\n",
    "        \n",
    "        # Calculer l'angle optimal de près\n",
    "        if self.tack_side == \"port\":\n",
    "            tack_angle = (wind_angle + 50) % 360  # 50° tribord du vent\n",
    "        else:\n",
    "            tack_angle = (wind_angle - 50) % 360  # 50° bâbord du vent\n",
    "        \n",
    "        # Vecteur de direction pour cet angle\n",
    "        tack_direction = np.array([np.cos(np.radians(tack_angle)), np.sin(np.radians(tack_angle))])\n",
    "        \n",
    "        # Vérifier si nous nous éloignons trop de l'objectif\n",
    "        direction_to_goal = goal_position - position\n",
    "        angle_to_goal = np.degrees(np.arctan2(direction_to_goal[1], direction_to_goal[0])) % 360\n",
    "        \n",
    "        # Si le tack nous éloigne trop de la direction générale, envisager de virer\n",
    "        angle_diff = min((tack_angle - angle_to_goal) % 360, (angle_to_goal - tack_angle) % 360)\n",
    "        if angle_diff > 100:  # Nous nous éloignons trop\n",
    "            # Virer de bord si nous avons parcouru une distance minimale depuis le dernier virement\n",
    "            if self.last_tack_position is None or np.linalg.norm(position - self.last_tack_position) > 5.0:\n",
    "                self.tack_side = \"port\" if self.tack_side == \"starboard\" else \"starboard\"\n",
    "                self.last_tack_position = position.copy()\n",
    "                # Recalculer l'angle après le virement\n",
    "                if self.tack_side == \"port\":\n",
    "                    tack_angle = (wind_angle + 50) % 360\n",
    "                else:\n",
    "                    tack_angle = (wind_angle - 50) % 360\n",
    "                tack_direction = np.array([np.cos(np.radians(tack_angle)), np.sin(np.radians(tack_angle))])\n",
    "        \n",
    "        # Obtenir l'action correspondante avec un léger facteur de stabilité\n",
    "        action = self._get_action_towards_direction(tack_direction)\n",
    "        \n",
    "        # Stabiliser l'action en utilisant l'historique\n",
    "        if len(self.last_actions) > 0:\n",
    "            # 80% nouvelle action, 20% moyenne des actions précédentes pour éviter les oscillations\n",
    "            prev_action = self.last_actions[-1]\n",
    "            if abs(action - prev_action) <= 1 or abs(action - prev_action) >= 7:  # Actions similaires ou opposées\n",
    "                action = action  # Garder la nouvelle action\n",
    "            else:\n",
    "                # Petite stabilisation\n",
    "                action = action if self.np_random.random() < 0.8 else prev_action\n",
    "        \n",
    "        # Mettre à jour l'historique des actions\n",
    "        self.last_actions.append(action)\n",
    "        if len(self.last_actions) > self.action_history_size:\n",
    "            self.last_actions.pop(0)\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def _downwind_strategy(self, position, wind_angle, boat_angle, goal_position, wind_field):\n",
    "        \"\"\"\n",
    "        Stratégie de navigation au portant (avec le vent arrière).\n",
    "        Implémente une navigation directe ou en zigzag selon les conditions.\n",
    "        \"\"\"\n",
    "        # Direction vers l'objectif\n",
    "        direction_to_goal = goal_position - position\n",
    "        goal_angle = np.degrees(np.arctan2(direction_to_goal[1], direction_to_goal[0])) % 360\n",
    "        \n",
    "        # Pour le vent arrière, la navigation directe n'est pas toujours optimale\n",
    "        # Nous pouvons naviguer légèrement de biais pour plus d'efficacité\n",
    "        \n",
    "        # Calculer les angles optimaux de chaque côté\n",
    "        port_angle = (wind_angle + 150) % 360\n",
    "        starboard_angle = (wind_angle - 150) % 360\n",
    "        \n",
    "        # Déterminer quel angle est le plus proche de la direction vers l'objectif\n",
    "        port_diff = min((port_angle - goal_angle) % 360, (goal_angle - port_angle) % 360)\n",
    "        starboard_diff = min((starboard_angle - goal_angle) % 360, (goal_angle - starboard_angle) % 360)\n",
    "        \n",
    "        # Choisir l'angle optimal\n",
    "        if port_diff < starboard_diff:\n",
    "            optimal_angle = port_angle\n",
    "        else:\n",
    "            optimal_angle = starboard_angle\n",
    "        \n",
    "        # Mixer avec la direction de l'objectif pour garantir la progression\n",
    "        optimal_direction = np.array([np.cos(np.radians(optimal_angle)), np.sin(np.radians(optimal_angle))])\n",
    "        goal_direction = direction_to_goal / np.linalg.norm(direction_to_goal)\n",
    "        \n",
    "        mixed_direction = 0.6 * optimal_direction + 0.4 * goal_direction\n",
    "        \n",
    "        # Obtenir l'action correspondante\n",
    "        action = self._get_action_towards_direction(mixed_direction)\n",
    "        \n",
    "        # Stabiliser l'action\n",
    "        if len(self.last_actions) > 0:\n",
    "            prev_action = self.last_actions[-1]\n",
    "            if action != prev_action and self.np_random.random() < 0.3:\n",
    "                action = prev_action\n",
    "        \n",
    "        # Mettre à jour l'historique\n",
    "        self.last_actions.append(action)\n",
    "        if len(self.last_actions) > self.action_history_size:\n",
    "            self.last_actions.pop(0)\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def _optimal_reaching_strategy(self, position, velocity, wind_angle, boat_angle, goal_position, wind_field):\n",
    "        \"\"\"\n",
    "        Stratégie optimale pour la navigation au travers ou au largue.\n",
    "        Maximise la vitesse tout en se dirigeant vers l'objectif.\n",
    "        \"\"\"\n",
    "        # Direction et angle vers l'objectif\n",
    "        direction_to_goal = goal_position - position\n",
    "        goal_angle = np.degrees(np.arctan2(direction_to_goal[1], direction_to_goal[0])) % 360\n",
    "        \n",
    "        # Angles de navigation optimaux (au travers)\n",
    "        optimal_angles = [(wind_angle + 90) % 360, (wind_angle - 90) % 360, \n",
    "                          (wind_angle + 120) % 360, (wind_angle - 120) % 360]\n",
    "        \n",
    "        # Trouver l'angle optimal le plus proche de notre direction\n",
    "        best_angle = None\n",
    "        min_diff = 180\n",
    "        \n",
    "        for angle in optimal_angles:\n",
    "            diff = min((angle - goal_angle) % 360, (goal_angle - angle) % 360)\n",
    "            if diff < min_diff:\n",
    "                min_diff = diff\n",
    "                best_angle = angle\n",
    "        \n",
    "        # Direction optimale\n",
    "        optimal_direction = np.array([np.cos(np.radians(best_angle)), np.sin(np.radians(best_angle))])\n",
    "        \n",
    "        # Équilibrer entre direction optimale et direction vers l'objectif\n",
    "        # Plus nous sommes proches de l'objectif, plus nous privilégions la direction directe\n",
    "        distance = np.linalg.norm(direction_to_goal)\n",
    "        goal_weight = max(0.3, min(0.8, 1.0 - distance / 20.0))\n",
    "        \n",
    "        goal_direction = direction_to_goal / np.linalg.norm(direction_to_goal)\n",
    "        mixed_direction = (1 - goal_weight) * optimal_direction + goal_weight * goal_direction\n",
    "        \n",
    "        # Obtenir l'action\n",
    "        action = self._get_action_towards_direction(mixed_direction)\n",
    "        \n",
    "        # Stabilisation\n",
    "        if len(self.last_actions) > 0:\n",
    "            prev_action = self.last_actions[-1]\n",
    "            if action != prev_action and self.np_random.random() < 0.2:\n",
    "                action = prev_action\n",
    "        \n",
    "        # Mettre à jour l'historique\n",
    "        self.last_actions.append(action)\n",
    "        if len(self.last_actions) > self.action_history_size:\n",
    "            self.last_actions.pop(0)\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def _get_action_towards_direction(self, direction):\n",
    "        \"\"\"Convertit un vecteur de direction en action discrète (0-8).\"\"\"\n",
    "        # Normaliser le vecteur\n",
    "        if np.linalg.norm(direction) > 0:\n",
    "            direction = direction / np.linalg.norm(direction)\n",
    "        \n",
    "        # Cartographier les 8 directions aux actions\n",
    "        actions_map = [\n",
    "            (0, 1),    # 0: Nord\n",
    "            (1, 1),    # 1: Nord-Est\n",
    "            (1, 0),    # 2: Est\n",
    "            (1, -1),   # 3: Sud-Est\n",
    "            (0, -1),   # 4: Sud\n",
    "            (-1, -1),  # 5: Sud-Ouest\n",
    "            (-1, 0),   # 6: Ouest\n",
    "            (-1, 1),   # 7: Nord-Ouest\n",
    "        ]\n",
    "        \n",
    "        # Ajouter l'action \"rester sur place\"\n",
    "        actions_map.append((0, 0))  # 8: Rester sur place\n",
    "        \n",
    "        # Trouver l'action la plus similaire à la direction\n",
    "        best_action = 0\n",
    "        best_similarity = -float('inf')\n",
    "        \n",
    "        for i, action_dir in enumerate(actions_map):\n",
    "            # Pour \"rester sur place\", utiliser une similarité spéciale\n",
    "            if i == 8:\n",
    "                # N'utiliser cette action que si la direction est très faible\n",
    "                if np.linalg.norm(direction) < 0.2:\n",
    "                    similarity = 1.0\n",
    "                else:\n",
    "                    similarity = -1.0\n",
    "            else:\n",
    "                similarity = direction[0] * action_dir[0] + direction[1] * action_dir[1]\n",
    "            \n",
    "            if similarity > best_similarity:\n",
    "                best_similarity = similarity\n",
    "                best_action = i\n",
    "        \n",
    "        return best_action\n",
    "    \n",
    "    def _cross_track_distance(self, position, goal_position):\n",
    "        \"\"\"\n",
    "        Calcule la distance perpendiculaire (cross-track) entre la position actuelle\n",
    "        et la ligne droite allant de la position initiale à l'objectif.\n",
    "        Pour simplifier, nous supposons que la position initiale est [16, 0].\n",
    "        \"\"\"\n",
    "        start_position = np.array([16, 0])\n",
    "        \n",
    "        # Vecteur de la ligne (de départ à objectif)\n",
    "        track_vector = goal_position - start_position\n",
    "        track_length = np.linalg.norm(track_vector)\n",
    "        \n",
    "        if track_length == 0:\n",
    "            return np.linalg.norm(position - start_position)\n",
    "        \n",
    "        # Normaliser\n",
    "        track_unit = track_vector / track_length\n",
    "        \n",
    "        # Vecteur de la position actuelle au point de départ\n",
    "        pos_vector = position - start_position\n",
    "        \n",
    "        # Projection sur la ligne\n",
    "        projection = np.dot(pos_vector, track_unit)\n",
    "        \n",
    "        # Calculer le point projeté\n",
    "        projected_point = start_position + projection * track_unit\n",
    "        \n",
    "        # Distance perpendiculaire\n",
    "        return np.linalg.norm(position - projected_point)\n",
    "    \n",
    "    def reset(self) -> None:\n",
    "        \"\"\"Réinitialiser l'agent.\"\"\"\n",
    "        self.last_tack_position = None\n",
    "        self.last_actions = []\n",
    "        self.performance_history = []\n",
    "        self.tack_side = \"port\" if self.np_random.random() < 0.5 else \"starboard\"\n",
    "    \n",
    "    def seed(self, seed: int = None) -> None:\n",
    "        \"\"\"Définir la graine aléatoire.\"\"\"\n",
    "        self.np_random = np.random.default_rng(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running the naive agent on the simple_static initial windfield:\n",
      "Step 10: Position=[ 6 11], Reward=0.0\n",
      "Step 20: Position=[ 6 11], Reward=0.0\n",
      "Step 30: Position=[ 7 12], Reward=0.0\n",
      "Step 40: Position=[ 8 13], Reward=0.0\n",
      "Step 50: Position=[ 8 13], Reward=0.0\n",
      "Step 60: Position=[ 8 15], Reward=0.0\n",
      "Step 70: Position=[ 7 17], Reward=0.0\n",
      "Step 80: Position=[ 7 17], Reward=0.0\n",
      "Step 90: Position=[ 8 18], Reward=0.0\n",
      "Step 100: Position=[ 9 19], Reward=0.0\n",
      "Step 110: Position=[ 9 19], Reward=0.0\n",
      "Step 120: Position=[10 20], Reward=0.0\n",
      "Step 130: Position=[10 20], Reward=0.0\n",
      "Step 140: Position=[11 21], Reward=0.0\n",
      "Step 150: Position=[11 21], Reward=0.0\n",
      "Step 160: Position=[12 22], Reward=0.0\n",
      "Step 170: Position=[13 23], Reward=0.0\n",
      "Step 180: Position=[13 23], Reward=0.0\n",
      "Step 190: Position=[14 24], Reward=0.0\n",
      "Step 200: Position=[14 24], Reward=0.0\n",
      "Step 210: Position=[15 25], Reward=0.0\n",
      "Step 220: Position=[15 25], Reward=0.0\n",
      "Step 230: Position=[14 28], Reward=0.0\n",
      "Step 240: Position=[14 29], Reward=0.0\n",
      "Step 250: Position=[15 29], Reward=0.0\n",
      "\n",
      "Episode finished after 253 steps with reward: 100.0\n",
      "Final position: [15 30]\n",
      "Goal reached: True\n"
     ]
    }
   ],
   "source": [
    "from src.env_sailing import SailingEnv\n",
    "from src.initial_windfields import get_initial_windfield\n",
    "\n",
    "# Create an environment with a simple test initial windfield\n",
    "env = SailingEnv(**get_initial_windfield('simple_static'))\n",
    "naive_agent = AdvancedSailingAgent()\n",
    "\n",
    "# Run a single episode\n",
    "observation, info = env.reset(seed=42)\n",
    "total_reward = 0\n",
    "done = False\n",
    "truncated = False\n",
    "step_count = 0\n",
    "\n",
    "print(\"Running the naive agent on the simple_static initial windfield:\")\n",
    "while not (done or truncated) and step_count < 1000:  # Limit to 100 steps\n",
    "    action = naive_agent.act(observation)\n",
    "    observation, reward, done, truncated, info = env.step(action)\n",
    "    total_reward += reward\n",
    "    step_count += 1\n",
    "    \n",
    "    # Print every 10 steps to avoid too much output\n",
    "    if step_count % 10 == 0:\n",
    "        print(f\"Step {step_count}: Position={info['position']}, Reward={reward}\")\n",
    "\n",
    "print(f\"\\nEpisode finished after {step_count} steps with reward: {total_reward}\")\n",
    "print(f\"Final position: {info['position']}\")\n",
    "print(f\"Goal reached: {done}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing a Simple RL Agent\n",
    "\n",
    "## Implementing a Q-Learning Agent\n",
    "\n",
    "Now let's implement a basic Q-learning agent for our sailing environment. Q-learning is a model-free reinforcement learning algorithm that learns to make decisions by estimating the value of state-action pairs.\n",
    "\n",
    "Our implementation will use a simplified state representation based on:\n",
    "1. Agent's current position\n",
    "2. Agent's current velocity \n",
    "3. Local wind at the agent's position\n",
    "\n",
    "This simplified approach makes the agent more interpretable and faster to train, while still capturing essential local information for effective navigation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent(BaseAgent):\n",
    "    \"\"\"A simple Q-learning agent for the sailing environment using only local information.\"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.1, discount_factor=0.9, exploration_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.np_random = np.random.default_rng()\n",
    "        \n",
    "        # Learning parameters\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.exploration_rate = exploration_rate\n",
    "        \n",
    "        # State discretization parameters\n",
    "        self.position_bins = 8     # Discretize the grid into 8x8\n",
    "        self.velocity_bins = 4     # Discretize velocity into 4 bins\n",
    "        self.wind_bins = 8         # Discretize wind directions into 8 bins\n",
    "        \n",
    "        # Initialize Q-table\n",
    "        # State space: position_x, position_y, velocity_direction, wind_direction\n",
    "        # Action space: 9 possible actions\n",
    "        self.q_table = {}\n",
    "        \n",
    "    def discretize_state(self, observation):\n",
    "        \"\"\"Convert continuous observation to discrete state for Q-table lookup.\"\"\"\n",
    "        # Extract position, velocity and wind from observation\n",
    "        x, y = observation[0], observation[1]\n",
    "        vx, vy = observation[2], observation[3]\n",
    "        wx, wy = observation[4], observation[5]\n",
    "        \n",
    "        # Discretize position (assume 32x32 grid)\n",
    "        grid_size = 32\n",
    "        x_bin = min(int(x / grid_size * self.position_bins), self.position_bins - 1)\n",
    "        y_bin = min(int(y / grid_size * self.position_bins), self.position_bins - 1)\n",
    "        \n",
    "        # Discretize velocity direction (ignoring magnitude for simplicity)\n",
    "        v_magnitude = np.sqrt(vx**2 + vy**2)\n",
    "        if v_magnitude < 0.1:  # If velocity is very small, consider it as a separate bin\n",
    "            v_bin = 0\n",
    "        else:\n",
    "            v_direction = np.arctan2(vy, vx)  # Range: [-pi, pi]\n",
    "            v_bin = int(((v_direction + np.pi) / (2 * np.pi) * (self.velocity_bins-1)) + 1) % self.velocity_bins\n",
    "        \n",
    "        # Discretize wind direction\n",
    "        wind_direction = np.arctan2(wy, wx)  # Range: [-pi, pi]\n",
    "        wind_bin = int(((wind_direction + np.pi) / (2 * np.pi) * self.wind_bins)) % self.wind_bins\n",
    "        \n",
    "        # Return discrete state tuple\n",
    "        return (x_bin, y_bin, v_bin, wind_bin)\n",
    "        \n",
    "    def act(self, observation):\n",
    "        \"\"\"Choose an action using epsilon-greedy policy.\"\"\"\n",
    "        # Discretize the state\n",
    "        state = self.discretize_state(observation)\n",
    "        \n",
    "        # Epsilon-greedy action selection\n",
    "        if self.np_random.random() < self.exploration_rate:\n",
    "            # Explore: choose a random action\n",
    "            return self.np_random.integers(0, 9)\n",
    "        else:\n",
    "            # Exploit: choose the best action according to Q-table\n",
    "            if state not in self.q_table:\n",
    "                # If state not in Q-table, initialize it\n",
    "                self.q_table[state] = np.zeros(9)\n",
    "            \n",
    "            # Return action with highest Q-value\n",
    "            return np.argmax(self.q_table[state])\n",
    "    \n",
    "    def learn(self, state, action, reward, next_state):\n",
    "        \"\"\"Update Q-table based on observed transition.\"\"\"\n",
    "        # Initialize Q-values if states not in table\n",
    "        if state not in self.q_table:\n",
    "            self.q_table[state] = np.zeros(9)\n",
    "        if next_state not in self.q_table:\n",
    "            self.q_table[next_state] = np.zeros(9)\n",
    "        \n",
    "        # Q-learning update\n",
    "        best_next_action = np.argmax(self.q_table[next_state])\n",
    "        td_target = reward + self.discount_factor * self.q_table[next_state][best_next_action]\n",
    "        td_error = td_target - self.q_table[state][action]\n",
    "        self.q_table[state][action] += self.learning_rate * td_error\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset the agent for a new episode.\"\"\"\n",
    "        # Nothing to reset for Q-learning agent\n",
    "        pass\n",
    "        \n",
    "    def seed(self, seed=None):\n",
    "        \"\"\"Set the random seed.\"\"\"\n",
    "        self.np_random = np.random.default_rng(seed)\n",
    "        \n",
    "    def save(self, path):\n",
    "        \"\"\"Save the Q-table to a file.\"\"\"\n",
    "        import pickle\n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump(self.q_table, f)\n",
    "            \n",
    "    def load(self, path):\n",
    "        \"\"\"Load the Q-table from a file.\"\"\"\n",
    "        import pickle\n",
    "        with open(path, 'rb') as f:\n",
    "            self.q_table = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Q-Learning Agent\n",
    "\n",
    "Now let's train our Q-learning agent on a simple initial windfield. We'll start with a small number of episodes (10) to demonstrate the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training with 10 episodes (debug run)...\n",
      "Episode 1: Steps=1000, Reward=0.0, Position=[ 8 30], Goal reached=True\n",
      "Episode 2: Steps=1000, Reward=0.0, Position=[ 1 30], Goal reached=True\n",
      "Episode 3: Steps=1000, Reward=0.0, Position=[ 0 31], Goal reached=True\n",
      "Episode 4: Steps=1000, Reward=0.0, Position=[ 1 24], Goal reached=True\n",
      "Episode 5: Steps=1000, Reward=0.0, Position=[ 4 30], Goal reached=True\n",
      "Episode 6: Steps=1000, Reward=0.0, Position=[ 0 27], Goal reached=True\n",
      "Episode 7: Steps=1000, Reward=0.0, Position=[ 4 31], Goal reached=True\n",
      "Episode 8: Steps=1000, Reward=0.0, Position=[ 7 29], Goal reached=True\n",
      "Episode 9: Steps=1000, Reward=0.0, Position=[ 1 31], Goal reached=True\n",
      "Episode 10: Steps=716, Reward=100.0, Position=[15 30], Goal reached=True\n",
      "\n",
      "Debug training completed!\n",
      "Q-table size: 215 states\n"
     ]
    }
   ],
   "source": [
    "# Create our Q-learning agent\n",
    "ql_agent = QLearningAgent(learning_rate=0.1, discount_factor=0.99, exploration_rate=0.2)\n",
    "\n",
    "# Set fixed seed for reproducibility\n",
    "np.random.seed(42)\n",
    "ql_agent.seed(42)\n",
    "\n",
    "# Create environment with a simple initial windfield\n",
    "env = SailingEnv(**get_initial_windfield('simple_static'))\n",
    "\n",
    "# Training parameters\n",
    "num_episodes = 10  # Small number for debugging\n",
    "max_steps = 1000\n",
    "\n",
    "# Training loop\n",
    "print(\"Starting training with 10 episodes (debug run)...\")\n",
    "for episode in range(num_episodes):\n",
    "    # Reset environment and get initial state\n",
    "    observation, info = env.reset(seed=episode)  # Different seed each episode\n",
    "    state = ql_agent.discretize_state(observation)\n",
    "    \n",
    "    total_reward = 0\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        # Select action and take step\n",
    "        action = ql_agent.act(observation)\n",
    "        next_observation, reward, done, truncated, info = env.step(action)\n",
    "        next_state = ql_agent.discretize_state(next_observation)\n",
    "        \n",
    "        # Update Q-table\n",
    "        ql_agent.learn(state, action, reward, next_state)\n",
    "        \n",
    "        # Update state and total reward\n",
    "        state = next_state\n",
    "        observation = next_observation\n",
    "        total_reward += reward\n",
    "        \n",
    "        # Break if episode is done\n",
    "        if done or truncated:\n",
    "            break\n",
    "    \n",
    "    print(f\"Episode {episode+1}: Steps={step+1}, Reward={total_reward}, \" +\n",
    "          f\"Position={info['position']}, Goal reached={done}\")\n",
    "    \n",
    "    # Update exploration rate (optional: decrease exploration over time)\n",
    "    ql_agent.exploration_rate = max(0.05, ql_agent.exploration_rate * 0.95)\n",
    "\n",
    "print(\"\\nDebug training completed!\")\n",
    "print(f\"Q-table size: {len(ql_agent.q_table)} states\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "méthode hybride"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridSailingQLearningAgent(BaseAgent):\n",
    "    \"\"\"\n",
    "    Agent hybride combinant Q-learning et connaissances de navigation à voile.\n",
    "    \n",
    "    Cet agent utilise:\n",
    "    1. Q-learning pour l'adaptation et l'apprentissage des politiques optimales\n",
    "    2. Connaissances physiques de la voile pour:\n",
    "       - Une meilleure représentation des états\n",
    "       - Une initialisation intelligente du Q-table\n",
    "       - Un mécanisme d'action guidée pour accélérer l'apprentissage\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.1, discount_factor=0.99, exploration_rate=0.2):\n",
    "        super().__init__()\n",
    "        self.np_random = np.random.default_rng()\n",
    "        \n",
    "        # Paramètres d'apprentissage\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.exploration_rate = exploration_rate\n",
    "        self.initial_exploration_rate = exploration_rate\n",
    "        self.min_exploration_rate = 0.05\n",
    "        self.exploration_decay = 0.9999  # Décroissance plus lente\n",
    "        \n",
    "        # Paramètres de discrétisation améliorés\n",
    "        self.position_bins = 10     # Plus de bins pour une meilleure précision\n",
    "        self.velocity_bins = 6      # Inclut magnitude et direction\n",
    "        self.wind_angle_bins = 12   # Discrétisation plus fine des angles de vent\n",
    "        \n",
    "        # Connaissance de la voile\n",
    "        self.sailing_efficiency = self._create_sailing_efficiency()\n",
    "        self.action_mapping = self._create_action_mapping()\n",
    "        \n",
    "        # Paramètres de récompense façonnée\n",
    "        self.use_shaped_rewards = True\n",
    "        self.progress_weight = 0.3  # Poids pour la récompense de progression\n",
    "        \n",
    "        # Variables d'état\n",
    "        self.previous_position = None\n",
    "        self.previous_distance_to_goal = None\n",
    "        self.episode_steps = 0\n",
    "        self.total_episodes = 0\n",
    "        self.goal_position = np.array([16, 31])  # Basé sur l'exemple précédent\n",
    "        \n",
    "        # Q-table et expérience\n",
    "        self.q_table = {}\n",
    "        self.experience_buffer = []  # Pour l'expérience replay simplifié\n",
    "        self.buffer_size = 10000\n",
    "        self.batch_size = 32\n",
    "        self.learn_every = 5  # Apprendre tous les N pas\n",
    "        \n",
    "        # Initialisation guidée\n",
    "        self._initialize_q_table_with_sailing_knowledge()\n",
    "    \n",
    "    def _create_sailing_efficiency(self):\n",
    "        \"\"\"\n",
    "        Crée une table d'efficacité de navigation basée sur l'angle au vent.\n",
    "        Cette connaissance sera utilisée pour guider l'exploration et initialiser la Q-table.\n",
    "        \"\"\"\n",
    "        efficiency = {}\n",
    "        \n",
    "        # Zone de non-navigation (irons) - très faible efficacité\n",
    "        for angle in range(0, 46):\n",
    "            efficiency[angle] = 0.1\n",
    "            efficiency[360-angle] = 0.1\n",
    "        \n",
    "        # Près (close-hauled) - efficacité modérée\n",
    "        for angle in range(46, 80):\n",
    "            factor = (angle - 45) / 35  # Augmentation progressive de l'efficacité\n",
    "            efficiency[angle] = 0.3 + 0.4 * factor\n",
    "            efficiency[360-angle] = 0.3 + 0.4 * factor\n",
    "        \n",
    "        # Travers (beam reach) - haute efficacité\n",
    "        for angle in range(80, 120):\n",
    "            efficiency[angle] = 0.9 + 0.1 * np.sin((angle - 80) * np.pi / 80)\n",
    "            efficiency[360-angle] = 0.9 + 0.1 * np.sin((angle - 80) * np.pi / 80)\n",
    "        \n",
    "        # Grand largue (broad reach) - très haute efficacité\n",
    "        for angle in range(120, 150):\n",
    "            efficiency[angle] = 0.95\n",
    "            efficiency[360-angle] = 0.95\n",
    "        \n",
    "        # Vent arrière (running) - haute efficacité mais pas optimale\n",
    "        for angle in range(150, 181):\n",
    "            factor = (180 - angle) / 30  # Diminution progressive\n",
    "            efficiency[angle] = 0.85 + 0.1 * factor\n",
    "            efficiency[360-angle] = 0.85 + 0.1 * factor\n",
    "        \n",
    "        return efficiency\n",
    "    \n",
    "    def _create_action_mapping(self):\n",
    "        \"\"\"\n",
    "        Crée une correspondance entre les actions discrètes et les directions.\n",
    "        \"\"\"\n",
    "        return [\n",
    "            (0, 1),    # 0: Nord\n",
    "            (1, 1),    # 1: Nord-Est\n",
    "            (1, 0),    # 2: Est\n",
    "            (1, -1),   # 3: Sud-Est\n",
    "            (0, -1),   # 4: Sud\n",
    "            (-1, -1),  # 5: Sud-Ouest\n",
    "            (-1, 0),   # 6: Ouest\n",
    "            (-1, 1),   # 7: Nord-Ouest\n",
    "            (0, 0)     # 8: Rester sur place\n",
    "        ]\n",
    "    \n",
    "    def _initialize_q_table_with_sailing_knowledge(self):\n",
    "        \"\"\"\n",
    "        Initialise intelligemment la Q-table avec des connaissances de navigation.\n",
    "        \"\"\"\n",
    "        # Cette méthode pourrait être étendue pour une initialisation plus sophistiquée\n",
    "        # Pour l'instant, nous allons simplement pré-initialiser certaines valeurs clés\n",
    "        \n",
    "        # Créer des états de base pour l'initialisation\n",
    "        for x_bin in range(self.position_bins):\n",
    "            for y_bin in range(self.position_bins):\n",
    "                for wind_bin in range(self.wind_angle_bins):\n",
    "                    # Convertir le bin de vent en angle de vent approximatif\n",
    "                    wind_angle = (wind_bin / self.wind_angle_bins) * 360\n",
    "                    \n",
    "                    # État simplifié (ignore vitesse pour l'initialisation)\n",
    "                    state = (x_bin, y_bin, 0, wind_bin)\n",
    "                    \n",
    "                    if state not in self.q_table:\n",
    "                        self.q_table[state] = np.zeros(9)\n",
    "                    \n",
    "                    # Pour chaque action, attribuer une valeur initiale basée sur l'efficacité de navigation\n",
    "                    for action, direction in enumerate(self.action_mapping):\n",
    "                        if action == 8:  # Rester sur place est rarement optimal\n",
    "                            self.q_table[state][action] = 0.1\n",
    "                            continue\n",
    "                        \n",
    "                        # Calculer l'angle relatif entre le vent et la direction de l'action\n",
    "                        action_angle = np.degrees(np.arctan2(direction[1], direction[0])) % 360\n",
    "                        relative_angle = (action_angle - wind_angle) % 360\n",
    "                        \n",
    "                        # Obtenir l'efficacité pour cet angle\n",
    "                        relative_angle_int = int(relative_angle)\n",
    "                        efficiency = self.sailing_efficiency.get(relative_angle_int, 0.5)\n",
    "                        \n",
    "                        # Valeur Q initiale basée sur l'efficacité et la progression vers l'objectif\n",
    "                        # Favoriser les actions qui font progresser vers le haut de la grille\n",
    "                        progress_factor = direction[1] * 0.5  # Valeur plus élevée pour les directions vers le nord\n",
    "                        \n",
    "                        # Initialiser la valeur Q\n",
    "                        self.q_table[state][action] = 0.5 * efficiency + 0.5 * progress_factor\n",
    "    \n",
    "    def discretize_state(self, observation):\n",
    "        \"\"\"\n",
    "        Convertit l'observation continue en état discret pour la recherche dans la Q-table.\n",
    "        Cette version améliorée utilise une discrétisation plus sophistiquée.\n",
    "        \"\"\"\n",
    "        # Extraire position, vitesse et vent de l'observation\n",
    "        x, y = observation[0], observation[1]\n",
    "        vx, vy = observation[2], observation[3]\n",
    "        wx, wy = observation[4], observation[5]\n",
    "        \n",
    "        # Discrétiser la position (grille 32x32 supposée)\n",
    "        grid_size = 32\n",
    "        x_bin = min(int(x / grid_size * self.position_bins), self.position_bins - 1)\n",
    "        y_bin = min(int(y / grid_size * self.position_bins), self.position_bins - 1)\n",
    "        \n",
    "        # Discrétiser la vitesse (magnitude et direction)\n",
    "        v_magnitude = np.sqrt(vx**2 + vy**2)\n",
    "        if v_magnitude < 0.1:  # Si la vitesse est très faible\n",
    "            v_bin = 0\n",
    "        else:\n",
    "            # Combiner magnitude et direction\n",
    "            v_direction = np.arctan2(vy, vx)  # Plage: [-pi, pi]\n",
    "            v_direction_norm = ((v_direction + np.pi) / (2 * np.pi))  # Normaliser à [0, 1]\n",
    "            \n",
    "            # Créer un indice composite basé sur magnitude et direction\n",
    "            v_magnitude_norm = min(v_magnitude / 5.0, 1.0)  # Supposer que 5.0 est la vitesse max\n",
    "            v_bin = int((v_direction_norm * (self.velocity_bins-2)) + 1 + (v_magnitude_norm * 0.99)) % self.velocity_bins\n",
    "        \n",
    "        # Discrétiser la direction du vent de manière plus fine\n",
    "        wind_direction = np.arctan2(wy, wx)  # Plage: [-pi, pi]\n",
    "        wind_bin = int(((wind_direction + np.pi) / (2 * np.pi) * self.wind_angle_bins)) % self.wind_angle_bins\n",
    "        \n",
    "        # Retourner le tuple d'état discret\n",
    "        return (x_bin, y_bin, v_bin, wind_bin)\n",
    "    \n",
    "    def get_shaped_reward(self, observation, reward, done):\n",
    "        \"\"\"\n",
    "        Crée une récompense façonnée pour guider l'apprentissage.\n",
    "        \"\"\"\n",
    "        if done:  # Si l'objectif est atteint, conserver la grande récompense\n",
    "            return reward\n",
    "        \n",
    "        # Extraire la position\n",
    "        position = observation[:2]\n",
    "        \n",
    "        # Calculer la distance à l'objectif\n",
    "        current_distance = np.linalg.norm(position - self.goal_position)\n",
    "        \n",
    "        # Si c'est le premier pas, initialiser les valeurs précédentes\n",
    "        if self.previous_position is None or self.previous_distance_to_goal is None:\n",
    "            self.previous_position = position\n",
    "            self.previous_distance_to_goal = current_distance\n",
    "            return 0.0\n",
    "        \n",
    "        # Récompense basée sur le progrès vers l'objectif\n",
    "        distance_improvement = self.previous_distance_to_goal - current_distance\n",
    "        progress_reward = distance_improvement * self.progress_weight\n",
    "        \n",
    "        # Extraire vent et vitesse\n",
    "        vx, vy = observation[2], observation[3]\n",
    "        wx, wy = observation[4], observation[5]\n",
    "        \n",
    "        # Calculer l'angle relatif entre le vent et la direction du mouvement\n",
    "        v_magnitude = np.sqrt(vx**2 + vy**2)\n",
    "        \n",
    "        # Seulement si le bateau se déplace\n",
    "        efficiency_reward = 0.0\n",
    "        if v_magnitude > 0.1:\n",
    "            movement_direction = np.arctan2(vy, vx)\n",
    "            wind_direction = np.arctan2(wy, wx)\n",
    "            \n",
    "            # Calculer l'angle relatif et le normaliser à [0, 360]\n",
    "            relative_angle = (np.degrees(movement_direction - wind_direction)) % 360\n",
    "            \n",
    "            # Obtenir l'efficacité pour cet angle\n",
    "            relative_angle_int = int(relative_angle)\n",
    "            sailing_efficiency = self.sailing_efficiency.get(relative_angle_int, 0.5)\n",
    "            \n",
    "            # Récompense basée sur l'efficacité * vitesse\n",
    "            efficiency_reward = sailing_efficiency * v_magnitude * 0.01\n",
    "        \n",
    "        # Mettre à jour les valeurs précédentes\n",
    "        self.previous_position = position\n",
    "        self.previous_distance_to_goal = current_distance\n",
    "        \n",
    "        # Récompense totale façonnée\n",
    "        shaped_reward = progress_reward + efficiency_reward\n",
    "        \n",
    "        return shaped_reward\n",
    "    \n",
    "    def act(self, observation):\n",
    "        \"\"\"\n",
    "        Choisit une action en utilisant une politique epsilon-greedy améliorée.\n",
    "        \"\"\"\n",
    "        # Discrétiser l'état\n",
    "        state = self.discretize_state(observation)\n",
    "        \n",
    "        # Décroissance de l'exploration\n",
    "        self.exploration_rate = max(\n",
    "            self.min_exploration_rate, \n",
    "            self.exploration_rate * self.exploration_decay\n",
    "        )\n",
    "        \n",
    "        # Sélection d'action epsilon-greedy\n",
    "        if self.np_random.random() < self.exploration_rate:\n",
    "            # Explorer: soit aléatoirement, soit guidé par les connaissances de navigation\n",
    "            if self.np_random.random() < 0.5:  # 50% du temps, utiliser une exploration guidée\n",
    "                action = self._get_guided_exploration_action(observation)\n",
    "            else:\n",
    "                action = self.np_random.integers(0, 9)\n",
    "        else:\n",
    "            # Exploiter: choisir la meilleure action selon la Q-table\n",
    "            if state not in self.q_table:\n",
    "                # Si l'état n'est pas dans la Q-table, l'initialiser\n",
    "                self.q_table[state] = np.zeros(9)\n",
    "            \n",
    "            # Retourner l'action avec la valeur Q la plus élevée\n",
    "            action = np.argmax(self.q_table[state])\n",
    "        \n",
    "        # Incrémenter le compteur d'étapes\n",
    "        self.episode_steps += 1\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def _get_guided_exploration_action(self, observation):\n",
    "        \"\"\"\n",
    "        Génère une action d'exploration guidée par les connaissances de navigation.\n",
    "        \"\"\"\n",
    "        # Extraire position, vitesse et vent de l'observation\n",
    "        position = observation[:2]\n",
    "        vx, vy = observation[2:4]\n",
    "        wx, wy = observation[4:6]\n",
    "        \n",
    "        # Direction vers l'objectif\n",
    "        direction_to_goal = self.goal_position - position\n",
    "        goal_angle = np.degrees(np.arctan2(direction_to_goal[1], direction_to_goal[0])) % 360\n",
    "        \n",
    "        # Angle du vent\n",
    "        wind_angle = np.degrees(np.arctan2(wy, wx)) % 360\n",
    "        \n",
    "        # Angle relatif entre le vent et la direction vers l'objectif\n",
    "        relative_angle = (goal_angle - wind_angle) % 360\n",
    "        \n",
    "        # Déterminer si l'objectif est face au vent (zone de non-navigation)\n",
    "        upwind_condition = relative_angle < 45 or relative_angle > 315\n",
    "        \n",
    "        actions_probabilities = np.ones(9) * 0.01  # Probabilité de base pour toutes les actions\n",
    "        \n",
    "        if upwind_condition:\n",
    "            # Stratégie de tacking (virement de bord)\n",
    "            port_tack_angle = (wind_angle + 60) % 360\n",
    "            starboard_tack_angle = (wind_angle - 60) % 360\n",
    "            \n",
    "            # Choisir le bord qui rapproche le plus de l'objectif\n",
    "            port_diff = min((port_tack_angle - goal_angle) % 360, (goal_angle - port_tack_angle) % 360)\n",
    "            starboard_diff = min((starboard_tack_angle - goal_angle) % 360, (goal_angle - starboard_tack_angle) % 360)\n",
    "            \n",
    "            best_tack_angle = port_tack_angle if port_diff < starboard_diff else starboard_tack_angle\n",
    "            \n",
    "            # Trouver l'action la plus proche de cet angle\n",
    "            best_action = self._get_action_for_angle(best_tack_angle)\n",
    "            actions_probabilities[best_action] = 0.8  # 80% de chance de choisir cette action\n",
    "        else:\n",
    "            # Pour les autres situations, privilégier les angles efficaces\n",
    "            for action, direction in enumerate(self.action_mapping):\n",
    "                if action == 8:  # Ignorer l'action \"rester sur place\"\n",
    "                    continue\n",
    "                \n",
    "                # Calculer l'angle de cette action\n",
    "                action_angle = np.degrees(np.arctan2(direction[1], direction[0])) % 360\n",
    "                \n",
    "                # Angle relatif au vent\n",
    "                action_rel_wind = (action_angle - wind_angle) % 360\n",
    "                \n",
    "                # Obtenir l'efficacité de navigation\n",
    "                efficiency = self.sailing_efficiency.get(int(action_rel_wind), 0.5)\n",
    "                \n",
    "                # Direction vers l'objectif\n",
    "                angle_to_goal = min((action_angle - goal_angle) % 360, (goal_angle - action_angle) % 360)\n",
    "                goal_factor = max(0.0, 1.0 - angle_to_goal / 180.0)\n",
    "                \n",
    "                # Calculer la probabilité\n",
    "                actions_probabilities[action] = efficiency * 0.5 + goal_factor * 0.5\n",
    "        \n",
    "        # Normaliser et retourner une action selon ces probabilités\n",
    "        actions_probabilities = actions_probabilities / np.sum(actions_probabilities)\n",
    "        return self.np_random.choice(9, p=actions_probabilities)\n",
    "    \n",
    "    def _get_action_for_angle(self, angle_deg):\n",
    "        \"\"\"\n",
    "        Trouve l'action qui correspond le mieux à un angle donné.\n",
    "        \"\"\"\n",
    "        # Convertir l'angle en vecteur de direction\n",
    "        angle_rad = np.radians(angle_deg)\n",
    "        direction = np.array([np.cos(angle_rad), np.sin(angle_rad)])\n",
    "        \n",
    "        # Trouver l'action la plus similaire\n",
    "        best_action = 0\n",
    "        best_similarity = -float('inf')\n",
    "        \n",
    "        for action, action_dir in enumerate(self.action_mapping):\n",
    "            if action == 8:  # Ignorer l'action \"rester sur place\"\n",
    "                continue\n",
    "                \n",
    "            similarity = direction[0] * action_dir[0] + direction[1] * action_dir[1]\n",
    "            if similarity > best_similarity:\n",
    "                best_similarity = similarity\n",
    "                best_action = action\n",
    "        \n",
    "        return best_action\n",
    "    \n",
    "    def learn(self, state, action, reward, next_state):\n",
    "        \"\"\"\n",
    "        Met à jour la Q-table basée sur la transition observée.\n",
    "        Utilise la récompense façonnée si activée.\n",
    "        \"\"\"\n",
    "        # Initialiser les valeurs Q si les états ne sont pas dans la table\n",
    "        if state not in self.q_table:\n",
    "            self.q_table[state] = np.zeros(9)\n",
    "        if next_state not in self.q_table:\n",
    "            self.q_table[next_state] = np.zeros(9)\n",
    "        \n",
    "        # Mise à jour Q-learning\n",
    "        best_next_action = np.argmax(self.q_table[next_state])\n",
    "        td_target = reward + self.discount_factor * self.q_table[next_state][best_next_action]\n",
    "        td_error = td_target - self.q_table[state][action]\n",
    "        self.q_table[state][action] += self.learning_rate * td_error\n",
    "        \n",
    "        # Ajouter l'expérience au buffer pour le replay\n",
    "        self.experience_buffer.append((state, action, reward, next_state))\n",
    "        if len(self.experience_buffer) > self.buffer_size:\n",
    "            self.experience_buffer.pop(0)\n",
    "        \n",
    "        # Apprendre de mini-lots périodiquement (expérience replay simplifié)\n",
    "        if self.episode_steps % self.learn_every == 0 and len(self.experience_buffer) >= self.batch_size:\n",
    "            self._learn_from_batch()\n",
    "    \n",
    "    def _learn_from_batch(self):\n",
    "        \"\"\"\n",
    "        Apprend à partir d'un mini-lot d'expériences passées.\n",
    "        \"\"\"\n",
    "        # Échantillonner un mini-lot aléatoire\n",
    "        batch_indices = self.np_random.choice(\n",
    "            len(self.experience_buffer), \n",
    "            size=min(self.batch_size, len(self.experience_buffer)), \n",
    "            replace=False\n",
    "        )\n",
    "        \n",
    "        for i in batch_indices:\n",
    "            state, action, reward, next_state = self.experience_buffer[i]\n",
    "            \n",
    "            # Vérifier/initialiser les valeurs Q\n",
    "            if state not in self.q_table:\n",
    "                self.q_table[state] = np.zeros(9)\n",
    "            if next_state not in self.q_table:\n",
    "                self.q_table[next_state] = np.zeros(9)\n",
    "            \n",
    "            # Mise à jour Q-learning\n",
    "            best_next_action = np.argmax(self.q_table[next_state])\n",
    "            td_target = reward + self.discount_factor * self.q_table[next_state][best_next_action]\n",
    "            td_error = td_target - self.q_table[state][action]\n",
    "            self.q_table[state][action] += self.learning_rate * 0.5 * td_error  # Taux réduit pour la stabilité\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Réinitialise l'agent pour un nouvel épisode.\"\"\"\n",
    "        self.previous_position = None\n",
    "        self.previous_distance_to_goal = None\n",
    "        self.episode_steps = 0\n",
    "        self.total_episodes += 1\n",
    "        \n",
    "        # Diminuer progressivement l'exploration au fil des épisodes\n",
    "        if self.total_episodes % 100 == 0 and self.total_episodes > 0:\n",
    "            self.exploration_rate = max(\n",
    "                self.min_exploration_rate, \n",
    "                self.exploration_rate * 0.95\n",
    "            )\n",
    "    \n",
    "    def seed(self, seed=None):\n",
    "        \"\"\"Définit la graine aléatoire.\"\"\"\n",
    "        self.np_random = np.random.default_rng(seed)\n",
    "    \n",
    "    def save(self, path):\n",
    "        \"\"\"Sauvegarde la Q-table et les paramètres dans un fichier.\"\"\"\n",
    "        import pickle\n",
    "        save_data = {\n",
    "            'q_table': self.q_table,\n",
    "            'exploration_rate': self.exploration_rate,\n",
    "            'total_episodes': self.total_episodes\n",
    "        }\n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump(save_data, f)\n",
    "    \n",
    "    def load(self, path):\n",
    "        \"\"\"Charge la Q-table et les paramètres depuis un fichier.\"\"\"\n",
    "        import pickle\n",
    "        with open(path, 'rb') as f:\n",
    "            save_data = pickle.load(f)\n",
    "            self.q_table = save_data['q_table']\n",
    "            self.exploration_rate = save_data['exploration_rate']\n",
    "            self.total_episodes = save_data['total_episodes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training with 10 episodes (debug run)...\n",
      "Episode 1: Steps=1000, Reward=0.0, Position=[ 2 30], Goal reached=True\n",
      "Episode 2: Steps=1000, Reward=0.0, Position=[ 0 31], Goal reached=True\n",
      "Episode 3: Steps=1000, Reward=0.0, Position=[ 0 31], Goal reached=True\n",
      "Episode 4: Steps=1000, Reward=0.0, Position=[ 2 31], Goal reached=True\n",
      "Episode 5: Steps=1000, Reward=0.0, Position=[ 5 31], Goal reached=True\n",
      "Episode 6: Steps=1000, Reward=0.0, Position=[ 4 31], Goal reached=True\n",
      "Episode 7: Steps=1000, Reward=0.0, Position=[ 7 31], Goal reached=True\n",
      "Episode 8: Steps=1000, Reward=0.0, Position=[ 2 30], Goal reached=True\n",
      "Episode 9: Steps=1000, Reward=0.0, Position=[11 31], Goal reached=True\n",
      "Episode 10: Steps=1000, Reward=0.0, Position=[ 3 28], Goal reached=True\n",
      "\n",
      "Debug training completed!\n",
      "Q-table size: 1357 states\n"
     ]
    }
   ],
   "source": [
    "# Create our Q-learning agent\n",
    "ql_agent = HybridSailingQLearningAgent(learning_rate=0.1, discount_factor=0.99, exploration_rate=0.2)\n",
    "\n",
    "# Set fixed seed for reproducibility\n",
    "np.random.seed(42)\n",
    "ql_agent.seed(42)\n",
    "\n",
    "# Create environment with a simple initial windfield\n",
    "env = SailingEnv(**get_initial_windfield('simple_static'))\n",
    "\n",
    "# Training parameters\n",
    "num_episodes = 10  # Small number for debugging\n",
    "max_steps = 1000\n",
    "\n",
    "# Training loop\n",
    "print(\"Starting training with 10 episodes (debug run)...\")\n",
    "for episode in range(num_episodes):\n",
    "    # Reset environment and get initial state\n",
    "    observation, info = env.reset(seed=episode)  # Different seed each episode\n",
    "    state = ql_agent.discretize_state(observation)\n",
    "    \n",
    "    total_reward = 0\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        # Select action and take step\n",
    "        action = ql_agent.act(observation)\n",
    "        next_observation, reward, done, truncated, info = env.step(action)\n",
    "        next_state = ql_agent.discretize_state(next_observation)\n",
    "        \n",
    "        # Update Q-table\n",
    "        ql_agent.learn(state, action, reward, next_state)\n",
    "        \n",
    "        # Update state and total reward\n",
    "        state = next_state\n",
    "        observation = next_observation\n",
    "        total_reward += reward\n",
    "        \n",
    "        # Break if episode is done\n",
    "        if done or truncated:\n",
    "            break\n",
    "    \n",
    "    print(f\"Episode {episode+1}: Steps={step+1}, Reward={total_reward}, \" +\n",
    "          f\"Position={info['position']}, Goal reached={done}\")\n",
    "    \n",
    "    # Update exploration rate (optional: decrease exploration over time)\n",
    "    ql_agent.exploration_rate = max(0.05, ql_agent.exploration_rate * 0.95)\n",
    "\n",
    "print(\"\\nDebug training completed!\")\n",
    "print(f\"Q-table size: {len(ql_agent.q_table)} states\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Training Run\n",
    "\n",
    "Now let's train our agent for more episodes to get better performance. This will take longer but should result in a more effective agent.\n",
    "\n",
    "*Note: You might want to adjust the number of episodes based on your available time. More episodes generally lead to better performance.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting full training with 100 episodes...\n",
      "Episode 10/100: Success rate (last 10): 100.0%\n",
      "Episode 20/100: Success rate (last 10): 100.0%\n",
      "Episode 30/100: Success rate (last 10): 100.0%\n",
      "Episode 40/100: Success rate (last 10): 100.0%\n",
      "Episode 50/100: Success rate (last 10): 100.0%\n",
      "Episode 60/100: Success rate (last 10): 100.0%\n",
      "Episode 70/100: Success rate (last 10): 100.0%\n",
      "Episode 80/100: Success rate (last 10): 100.0%\n",
      "Episode 90/100: Success rate (last 10): 100.0%\n",
      "Episode 100/100: Success rate (last 10): 100.0%\n",
      "\n",
      "Training completed in 17.1 seconds!\n",
      "Success rate: 100.0%\n",
      "Average reward: 100.00\n",
      "Average steps: 202.8\n",
      "Q-table size: 514 states\n"
     ]
    }
   ],
   "source": [
    "# Create our Q-learning agent for full training\n",
    "ql_agent_full = QLearningAgent(learning_rate=0.1, discount_factor=0.99, exploration_rate=0.3)\n",
    "\n",
    "# Set fixed seed for reproducibility\n",
    "np.random.seed(42)\n",
    "ql_agent_full.seed(42)\n",
    "\n",
    "# Create environment with a simple initial windfield\n",
    "env = SailingEnv(**get_initial_windfield('training_1'))\n",
    "\n",
    "# Training parameters\n",
    "num_episodes = 100  # More episodes for better learning\n",
    "max_steps = 1000\n",
    "\n",
    "# Progress tracking\n",
    "rewards_history = []\n",
    "steps_history = []\n",
    "success_history = []\n",
    "\n",
    "# Training loop\n",
    "print(\"Starting full training with 100 episodes...\")\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    # Reset environment and get initial state\n",
    "    observation, info = env.reset(seed=episode)  # Different seed each episode\n",
    "    state = ql_agent_full.discretize_state(observation)\n",
    "    \n",
    "    total_reward = 0\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        # Select action and take step\n",
    "        action = ql_agent_full.act(observation)\n",
    "        next_observation, reward, done, truncated, info = env.step(action)\n",
    "        next_state = ql_agent_full.discretize_state(next_observation)\n",
    "        \n",
    "        # Update Q-table\n",
    "        ql_agent_full.learn(state, action, reward, next_state)\n",
    "        \n",
    "        # Update state and total reward\n",
    "        state = next_state\n",
    "        observation = next_observation\n",
    "        total_reward += reward\n",
    "        \n",
    "        # Break if episode is done\n",
    "        if done or truncated:\n",
    "            break\n",
    "    \n",
    "    # Record metrics\n",
    "    rewards_history.append(total_reward)\n",
    "    steps_history.append(step+1)\n",
    "    success_history.append(done)\n",
    "    \n",
    "    # Update exploration rate (decrease over time)\n",
    "    ql_agent_full.exploration_rate = max(0.05, ql_agent_full.exploration_rate * 0.98)\n",
    "    \n",
    "    # Print progress every 10 episodes\n",
    "    if (episode + 1) % 10 == 0:\n",
    "        success_rate = sum(success_history[-10:]) / 10 * 100\n",
    "        print(f\"Episode {episode+1}/100: Success rate (last 10): {success_rate:.1f}%\")\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "# Calculate overall success rate\n",
    "success_rate = sum(success_history) / len(success_history) * 100\n",
    "\n",
    "print(f\"\\nTraining completed in {training_time:.1f} seconds!\")\n",
    "print(f\"Success rate: {success_rate:.1f}%\")\n",
    "print(f\"Average reward: {np.mean(rewards_history):.2f}\")\n",
    "print(f\"Average steps: {np.mean(steps_history):.1f}\")\n",
    "print(f\"Q-table size: {len(ql_agent_full.q_table)} states\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting full training with 100 episodes...\n",
      "Episode 10/100: Success rate (last 10): 100.0%\n",
      "Episode 20/100: Success rate (last 10): 100.0%\n",
      "Episode 30/100: Success rate (last 10): 100.0%\n",
      "Episode 40/100: Success rate (last 10): 100.0%\n",
      "Episode 50/100: Success rate (last 10): 100.0%\n",
      "Episode 60/100: Success rate (last 10): 100.0%\n",
      "Episode 70/100: Success rate (last 10): 100.0%\n",
      "Episode 80/100: Success rate (last 10): 100.0%\n",
      "Episode 90/100: Success rate (last 10): 100.0%\n",
      "Episode 100/100: Success rate (last 10): 100.0%\n",
      "\n",
      "Training completed in 18.9 seconds!\n",
      "Success rate: 100.0%\n",
      "Average reward: 100.00\n",
      "Average steps: 191.9\n",
      "Q-table size: 2356 states\n"
     ]
    }
   ],
   "source": [
    "# Create our Q-learning agent for full training\n",
    "ql_agent_full = HybridSailingQLearningAgent(learning_rate=0.1, discount_factor=0.99, exploration_rate=0.3)\n",
    "\n",
    "# Set fixed seed for reproducibility\n",
    "np.random.seed(42)\n",
    "ql_agent_full.seed(42)\n",
    "\n",
    "# Create environment with a simple initial windfield\n",
    "env = SailingEnv(**get_initial_windfield('training_1'))\n",
    "\n",
    "# Training parameters\n",
    "num_episodes = 100  # More episodes for better learning\n",
    "max_steps = 1000\n",
    "\n",
    "# Progress tracking\n",
    "rewards_history = []\n",
    "steps_history = []\n",
    "success_history = []\n",
    "\n",
    "# Training loop\n",
    "print(\"Starting full training with 100 episodes...\")\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    # Reset environment and get initial state\n",
    "    observation, info = env.reset(seed=episode)  # Different seed each episode\n",
    "    state = ql_agent_full.discretize_state(observation)\n",
    "    \n",
    "    total_reward = 0\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        # Select action and take step\n",
    "        action = ql_agent_full.act(observation)\n",
    "        next_observation, reward, done, truncated, info = env.step(action)\n",
    "        next_state = ql_agent_full.discretize_state(next_observation)\n",
    "        \n",
    "        # Update Q-table\n",
    "        ql_agent_full.learn(state, action, reward, next_state)\n",
    "        \n",
    "        # Update state and total reward\n",
    "        state = next_state\n",
    "        observation = next_observation\n",
    "        total_reward += reward\n",
    "        \n",
    "        # Break if episode is done\n",
    "        if done or truncated:\n",
    "            break\n",
    "    \n",
    "    # Record metrics\n",
    "    rewards_history.append(total_reward)\n",
    "    steps_history.append(step+1)\n",
    "    success_history.append(done)\n",
    "    \n",
    "    # Update exploration rate (decrease over time)\n",
    "    ql_agent_full.exploration_rate = max(0.05, ql_agent_full.exploration_rate * 0.98)\n",
    "    \n",
    "    # Print progress every 10 episodes\n",
    "    if (episode + 1) % 10 == 0:\n",
    "        success_rate = sum(success_history[-10:]) / 10 * 100\n",
    "        print(f\"Episode {episode+1}/100: Success rate (last 10): {success_rate:.1f}%\")\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "# Calculate overall success rate\n",
    "success_rate = sum(success_history) / len(success_history) * 100\n",
    "\n",
    "print(f\"\\nTraining completed in {training_time:.1f} seconds!\")\n",
    "print(f\"Success rate: {success_rate:.1f}%\")\n",
    "print(f\"Average reward: {np.mean(rewards_history):.2f}\")\n",
    "print(f\"Average steps: {np.mean(steps_history):.1f}\")\n",
    "print(f\"Q-table size: {len(ql_agent_full.q_table)} states\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Potential Extensions to the Q-Learning Agent\n",
    "\n",
    "This simplified Q-learning implementation provides a good starting point but has several limitations:\n",
    "\n",
    "1. **Limited State Representation**: It only uses local information (position, velocity, and local wind) without considering the full wind field, which limits the agent's ability to plan ahead.\n",
    "\n",
    "2. **Discrete State Space**: The discretization loses information and may not capture subtle differences in states.\n",
    "\n",
    "3. **Fixed Exploration Rate**: The exploration rate doesn't adapt based on learning progress.\n",
    "\n",
    "#### How to Extend the Agent:\n",
    "\n",
    "1. **Incorporating the Full Wind Field**:\n",
    "   - You could extend the state representation to include information from the full wind field (observation indices 6 onward).\n",
    "   - Create a more sophisticated discretization that captures wind patterns relevant to planning.\n",
    "   - Example approach: Sample key grid points ahead of the boat's position or in the direction of the goal.\n",
    "\n",
    "2. **Function Approximation**:\n",
    "   - Replace the discrete Q-table with a neural network for function approximation.\n",
    "   - This would allow handling continuous state spaces more effectively.\n",
    "\n",
    "3. **Advanced Exploration Strategies**:\n",
    "   - Implement techniques like intrinsic motivation or uncertainty-based exploration.\n",
    "   - Use count-based exploration bonuses for less-visited states.\n",
    "\n",
    "4. **Multi-step Learning**:\n",
    "   - Implement n-step Q-learning or TD(λ) to improve learning efficiency.\n",
    "\n",
    "When extending the agent, remember to modify the `save_qlearning_agent()` function accordingly to properly save your enhanced implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Training Results\n",
    "\n",
    "Let's visualize how our agent improved during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1200 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate rolling averages\n",
    "window_size = 10\n",
    "rolling_rewards = np.convolve(rewards_history, np.ones(window_size)/window_size, mode='valid')\n",
    "rolling_steps = np.convolve(steps_history, np.ones(window_size)/window_size, mode='valid')\n",
    "rolling_success = np.convolve([1 if s else 0 for s in success_history], np.ones(window_size)/window_size, mode='valid') * 100\n",
    "\n",
    "# Create the plots\n",
    "# fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(10, 12), sharex=True)\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 12), sharex=True)\n",
    "\n",
    "# Plot rewards\n",
    "ax1.plot(rolling_rewards)\n",
    "ax1.set_ylabel('Average Reward')\n",
    "ax1.set_title('Training Progress (10-episode rolling average)')\n",
    "\n",
    "# Plot steps\n",
    "ax2.plot(rolling_steps)\n",
    "ax2.set_ylabel('Average Steps')\n",
    "\n",
    "# Plot success rate\n",
    "#ax3.plot(rolling_success)\n",
    "#ax3.set_ylabel('Success Rate (%)')\n",
    "#ax3.set_xlabel('Episode')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Trained Agent\n",
    "\n",
    "Now let's evaluate our trained agent with exploration turned off to see how well it performs on unseen seeds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the trained agent on 5 new episodes...\n",
      "Test Episode 1: Steps=141, Reward=100.0, Position=[16 30], Goal reached=True\n",
      "Test Episode 2: Steps=228, Reward=100.0, Position=[17 31], Goal reached=True\n",
      "Test Episode 3: Steps=97, Reward=100.0, Position=[17 30], Goal reached=True\n",
      "Test Episode 4: Steps=140, Reward=100.0, Position=[16 30], Goal reached=True\n",
      "Test Episode 5: Steps=188, Reward=100.0, Position=[17 31], Goal reached=True\n"
     ]
    }
   ],
   "source": [
    "# Turn off exploration for evaluation\n",
    "ql_agent_full.exploration_rate = 0\n",
    "\n",
    "# Create test environment\n",
    "test_env = SailingEnv(**get_initial_windfield('training_1'))\n",
    "\n",
    "# Test parameters\n",
    "num_test_episodes = 5\n",
    "max_steps = 1000\n",
    "\n",
    "print(\"Testing the trained agent on 5 new episodes...\")\n",
    "# Testing loop\n",
    "for episode in range(num_test_episodes):\n",
    "    # Reset environment\n",
    "    observation, info = test_env.reset(seed=1000 + episode)  # Different seeds from training\n",
    "    \n",
    "    total_reward = 0\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        # Select action using learned policy\n",
    "        action = ql_agent_full.act(observation)\n",
    "        observation, reward, done, truncated, info = test_env.step(action)\n",
    "        \n",
    "        total_reward += reward\n",
    "        \n",
    "        # Break if episode is done\n",
    "        if done or truncated:\n",
    "            break\n",
    "    \n",
    "    print(f\"Test Episode {episode+1}: Steps={step+1}, Reward={total_reward}, \" +\n",
    "          f\"Position={info['position']}, Goal reached={done}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Your Agent's Behavior\n",
    "\n",
    "While training metrics provide quantitative insights, actually seeing your agent navigate through the environment can reveal critical information about its behavior and strategy.\n",
    "\n",
    "#### Using evaluate_agent.ipynb for Visualization\n",
    "\n",
    "The `evaluate_agent.ipynb` notebook provides powerful visualization tools that let you see:\n",
    "- Complete trajectories across different initial windfields\n",
    "- How your agent responds to wind conditions\n",
    "- Frame-by-frame animations of navigation decisions\n",
    "\n",
    "To use these visualizations, you'll need to save your agent in the proper format first, which we'll do in the next section. Once saved, you can:\n",
    "\n",
    "1. Open `evaluate_agent.ipynb`\n",
    "2. Set `AGENT_PATH` to your saved agent file\n",
    "3. Run the evaluation cells to generate visualizations\n",
    "\n",
    "These visual insights can help you identify patterns, diagnose issues, and refine your agent's strategy in ways that metrics alone cannot reveal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Your Agent for Submission\n",
    "\n",
    "Now let's save our trained agent as a Python file that can be used for evaluation and submission. This step is crucial for three key reasons:\n",
    "\n",
    "1. **Visualization and Testing**: Saving allows you to use the `evaluate_agent.ipynb` notebook to visualize trajectories and test performance across different scenarios.\n",
    "\n",
    "2. **Validation and Evaluation**: The saved agent can be validated with `validate_agent.ipynb` and thoroughly evaluated using different seeds and initial windfields with `evaluate_agent.ipynb`. These notebooks provide important metrics and visualizations to understand your agent's performance.\n",
    "\n",
    "3. **Submission Format**: Any agent submitted to the evaluator **must** be in this format - a single standalone Python (.py) file with a class that inherits from `BaseAgent` and implements all required methods. This is the official submission format for the challenge.\n",
    "\n",
    "For Q-learning agents like ours, we've created a utility function `save_qlearning_agent()` in `src/utils/agent_utils.py` that handles the process of saving the agent with all its learned parameters. This creates a standalone Python file ready for submission.\n",
    "\n",
    "This utility function:\n",
    "1. Extracts the Q-table from your trained agent\n",
    "2. Creates a new Python file with a clean agent implementation\n",
    "3. Embeds the learned Q-values directly in the code\n",
    "4. Includes all the necessary methods (act, reset, seed, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent saved to ../src/agents/agent_trained_example.py\n",
      "The file contains 514 state-action pairs.\n",
      "You can now use this file with validate_agent.ipynb and evaluate_agent.ipynb\n"
     ]
    }
   ],
   "source": [
    "# Import the utility function for saving Q-learning agents\n",
    "from src.utils.agent_utils import save_qlearning_agent\n",
    "\n",
    "# Save our trained agent\n",
    "save_qlearning_agent(\n",
    "    agent=ql_agent_full,\n",
    "    output_path=\"../src/agents/agent_trained_example.py\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extending the Utility for Your Own Agents\n",
    "\n",
    "If you implement different types of agents (such as DQN, SARSA, or custom algorithms), you may need to create similar utility functions. Some tips:\n",
    "\n",
    "1. Make sure your save function preserves all necessary parameters and learned values\n",
    "2. Store them in a way that doesn't require additional files (embedded in the code)\n",
    "3. Ensure the saved agent includes all required methods from the BaseAgent interface\n",
    "\n",
    "When extending `save_qlearning_agent()` for different Q-learning variants, you might need to:\n",
    "- Update the state discretization logic\n",
    "- Change how parameters are stored and initialized\n",
    "- Modify the act() method's logic for your specific algorithm\n",
    "\n",
    "### Agent Types and Saving Strategies\n",
    "\n",
    "**For Rule-Based Agents:**\n",
    "- Since rule-based agents don't have learned parameters, you can simply ensure your agent class follows the `BaseAgent` interface\n",
    "- Implement all required methods: `__init__()`, `act(observation)`, `reset()`, and `seed(seed)`\n",
    "- These are typically the simplest to save as the agent's logic is entirely defined in the code\n",
    "\n",
    "**For Deep Learning-Based Agents:**\n",
    "- Include the model architecture definition directly in your Python file\n",
    "- Convert model weights to numpy arrays and include them in your code\n",
    "- Add functions to rebuild the model from these arrays\n",
    "\n",
    "### Key Requirements for Any Submission File\n",
    "\n",
    "Regardless of your agent type, ensure your submission file:\n",
    "1. **Contains everything**: All code, parameters, and data needed to run the agent\n",
    "2. **Is a single file**: No external dependencies beyond standard libraries\n",
    "3. **Follows the interface**: Properly inherits from `BaseAgent` and implements all required methods\n",
    "4. **Requires no arguments**: The agent must initialize without any required arguments\n",
    "5. **Is deterministic**: For a given seed, the agent should behave identically each time\n",
    "\n",
    "## Important Note on Import Paths\n",
    "\n",
    "When creating agent files for submission, make sure to use the correct import paths:\n",
    "- **Use**: `from agents.base_agent import BaseAgent`\n",
    "- **Not**: `from src.agents.base_agent import BaseAgent`\n",
    "\n",
    "This is because the validation and evaluation scripts run from within the `src` directory, so imports should be relative to that location. Our utility function `save_qlearning_agent` already handles this for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automating Agent Training and Evaluation\n",
    "\n",
    "The approach we've taken here is interactive and educational, but for serious agent development, you'll likely want to automate the training process. Here's where you could expand:\n",
    "\n",
    "```python\n",
    "# Your training script could look something like this:\n",
    "def train_agent(agent, initial_windfields, num_episodes, save_path):\n",
    "    # Setup training parameters\n",
    "    # ...\n",
    "    \n",
    "    # Train on multiple initial windfields\n",
    "    for initial_windfield_name, initial_windfield in initial_windfields.items():\n",
    "        # Train agent on this initial_windfields\n",
    "        # ...\n",
    "        \n",
    "    # Save the trained agent\n",
    "    # ...\n",
    "    \n",
    "    return training_metrics\n",
    "```\n",
    "\n",
    "Creating a command-line interface for training and evaluation would allow you to:\n",
    "1. Train agents with different hyperparameters\n",
    "2. Evaluate on multiple initial_windfields \n",
    "3. Create systematic experiments\n",
    "\n",
    "This is left as an exercise for you to implement based on your specific approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and Next Steps\n",
    "\n",
    "In this notebook, we've covered:\n",
    "\n",
    "1. **Agent Requirements**: Understanding the BaseAgent interface \n",
    "2. **The Naive Agent**: Examining a simple rule-based agent \n",
    "3. **Simplified Q-Learning Agent**: Implementing and training a basic RL agent that uses only local information (position, velocity, and local wind)\n",
    "\n",
    "### Next Steps for Developing Your Own Agent\n",
    "\n",
    "Now it's your turn to develop your own agent. Here are some suggestions:\n",
    "\n",
    "1. **Enhance the Q-Learning Agent**:\n",
    "   - Extend the state representation to incorporate the full wind field (not just local wind)\n",
    "   - This would allow the agent to anticipate wind changes and plan better routes\n",
    "   - Hint: Modify the `discretize_state` method to extract and process relevant features from the flattened wind field\n",
    "\n",
    "2. **Algorithmic Improvements**:\n",
    "   - Implement function approximation to handle continuous state spaces better\n",
    "   - Explore other RL algorithms like SARSA, Expected SARSA, or Deep Q-Networks\n",
    "   - Experiment with different exploration strategies that adapt over time\n",
    "\n",
    "3. **Physics-Based Approaches**:\n",
    "   - Leverage your understanding of sailing physics (from challenge_walkthrough notebook)\n",
    "   - Implement rule-based algorithms or path planning (A*, etc.) that take advantage of domain knowledge\n",
    "   - Create hybrid approaches that combine RL with domain-specific rules\n",
    "   \n",
    "### Validating and Evaluating Your Agent\n",
    "\n",
    "After you've developed your agent, the next steps are to:\n",
    "\n",
    "1. **Validate your agent** using the `validate_agent.ipynb` notebook or command-line tool\n",
    "2. **Evaluate your agent** using the `evaluate_agent.ipynb` notebook\n",
    "\n",
    "Remember that agents combining multiple techniques often perform best - consider how you might blend RL with domain knowledge of sailing physics for optimal results!\n",
    "\n",
    "Good luck with the Sailing Challenge!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.12/site-packages (2.7.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/site-packages (from torch) (80.4.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/site-packages (from torch) (2025.3.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/site-packages (from torch) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /usr/local/lib/python3.12/site-packages (from torch) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/site-packages (from torch) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/site-packages (from torch) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/site-packages (from torch) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/site-packages (from torch) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/site-packages (from torch) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.12/site-packages (from torch) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /usr/local/lib/python3.12/site-packages (from torch) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/site-packages (from torch) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/site-packages (from torch) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.0 in /usr/local/lib/python3.12/site-packages (from torch) (3.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "from src.agents.base_agent import BaseAgent\n",
    "\n",
    "class PPOSailingAgent(BaseAgent):\n",
    "    \"\"\"Agent utilisant Proximal Policy Optimization (PPO) pour le Sailing Challenge.\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 learning_rate=0.0003,\n",
    "                 gamma=0.99,\n",
    "                 gae_lambda=0.95,\n",
    "                 clip_param=0.2,\n",
    "                 value_coef=0.5,\n",
    "                 entropy_coef=0.01,\n",
    "                 max_grad_norm=0.5,\n",
    "                 ppo_epochs=4,\n",
    "                 mini_batch_size=64,\n",
    "                 use_sailing_features=True):\n",
    "        \"\"\"\n",
    "        Initialise l'agent PPO.\n",
    "        \n",
    "        Args:\n",
    "            learning_rate: Taux d'apprentissage pour l'optimiseur\n",
    "            gamma: Facteur d'actualisation pour les récompenses\n",
    "            gae_lambda: Paramètre lambda pour l'estimation d'avantage généralisée\n",
    "            clip_param: Paramètre d'écrêtage pour PPO\n",
    "            value_coef: Coefficient pour la perte de la fonction de valeur\n",
    "            entropy_coef: Coefficient pour le bonus d'entropie\n",
    "            max_grad_norm: Norme maximale du gradient pour l'écrêtage\n",
    "            ppo_epochs: Nombre d'époques pour chaque mise à jour PPO\n",
    "            mini_batch_size: Taille des mini-lots pour l'entraînement\n",
    "            use_sailing_features: Utiliser des caractéristiques spécifiques à la voile\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.np_random = np.random.default_rng()\n",
    "        \n",
    "        # Paramètres PPO\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = gamma\n",
    "        self.gae_lambda = gae_lambda\n",
    "        self.clip_param = clip_param\n",
    "        self.value_coef = value_coef\n",
    "        self.entropy_coef = entropy_coef\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "        self.ppo_epochs = ppo_epochs\n",
    "        self.mini_batch_size = mini_batch_size\n",
    "        \n",
    "        # Paramètres d'environnement\n",
    "        self.observation_dim = 2050  # x, y, vx, vy, wx, wy + wind field (32x32x2)\n",
    "        self.action_dim = 9         # 8 directions + stay in place\n",
    "        \n",
    "        # Construction des réseaux\n",
    "        self.use_sailing_features = use_sailing_features\n",
    "        hidden_dim = 256\n",
    "        \n",
    "        # Définir la taille d'entrée en fonction de l'utilisation des caractéristiques de voile\n",
    "        input_dim = self.observation_dim\n",
    "        if self.use_sailing_features:\n",
    "            # Si nous utilisons des caractéristiques extraites, la dimension d'entrée est plus petite\n",
    "            input_dim = 25  # Caractéristiques extraites au lieu de l'observation brute\n",
    "        \n",
    "        # Réseaux pour la politique et la valeur\n",
    "        self.policy_value_network = PolicyValueNetwork(input_dim, hidden_dim, self.action_dim)\n",
    "        \n",
    "        # Optimiseur\n",
    "        self.optimizer = optim.Adam(self.policy_value_network.parameters(), lr=self.lr)\n",
    "        \n",
    "        # Mémoire pour stocker les expériences\n",
    "        self.memory = PPOMemory()\n",
    "        \n",
    "        # Appareil d'exécution (CPU ou GPU)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.policy_value_network.to(self.device)\n",
    "        \n",
    "        # Compteurs\n",
    "        self.training_step = 0\n",
    "        \n",
    "        # Position de l'objectif (sera mise à jour lors de l'initialisation de l'environnement)\n",
    "        self.goal_position = np.array([16, 31])  # Valeur par défaut\n",
    "        \n",
    "        # Indice d'efficacité de navigation\n",
    "        self.sailing_efficiency = self._create_sailing_efficiency()\n",
    "    \n",
    "    def _create_sailing_efficiency(self):\n",
    "        \"\"\"\n",
    "        Crée une table d'efficacité de navigation basée sur l'angle au vent.\n",
    "        Cette connaissance sera utilisée pour l'extraction de caractéristiques.\n",
    "        \"\"\"\n",
    "        efficiency = {}\n",
    "        \n",
    "        # Zone de non-navigation (irons) - très faible efficacité\n",
    "        for angle in range(0, 46):\n",
    "            efficiency[angle] = 0.1\n",
    "            efficiency[360-angle] = 0.1\n",
    "        \n",
    "        # Près (close-hauled) - efficacité modérée\n",
    "        for angle in range(46, 80):\n",
    "            factor = (angle - 45) / 35  # Augmentation progressive de l'efficacité\n",
    "            efficiency[angle] = 0.3 + 0.4 * factor\n",
    "            efficiency[360-angle] = 0.3 + 0.4 * factor\n",
    "        \n",
    "        # Travers (beam reach) - haute efficacité\n",
    "        for angle in range(80, 120):\n",
    "            efficiency[angle] = 0.9 + 0.1 * np.sin((angle - 80) * np.pi / 80)\n",
    "            efficiency[360-angle] = 0.9 + 0.1 * np.sin((angle - 80) * np.pi / 80)\n",
    "        \n",
    "        # Grand largue (broad reach) - très haute efficacité\n",
    "        for angle in range(120, 150):\n",
    "            efficiency[angle] = 0.95\n",
    "            efficiency[360-angle] = 0.95\n",
    "        \n",
    "        # Vent arrière (running) - haute efficacité mais pas optimale\n",
    "        for angle in range(150, 181):\n",
    "            factor = (180 - angle) / 30  # Diminution progressive\n",
    "            efficiency[angle] = 0.85 + 0.1 * factor\n",
    "            efficiency[360-angle] = 0.85 + 0.1 * factor\n",
    "        \n",
    "        return efficiency\n",
    "    \n",
    "    def extract_features(self, observation):\n",
    "        \"\"\"\n",
    "        Extrait des caractéristiques pertinentes de l'observation brute.\n",
    "        Cette étape est cruciale pour réduire la dimensionnalité et incorporer\n",
    "        des connaissances spécifiques à la voile.\n",
    "        \n",
    "        Args:\n",
    "            observation: Observation brute de l'environnement\n",
    "        \n",
    "        Returns:\n",
    "            Vecteur de caractéristiques extraites\n",
    "        \"\"\"\n",
    "        # Position et vitesse actuelles\n",
    "        position = observation[:2]\n",
    "        velocity = observation[2:4]\n",
    "        wind_at_boat = observation[4:6]\n",
    "        \n",
    "        # Calculer la distance et la direction vers l'objectif\n",
    "        direction_to_goal = self.goal_position - position\n",
    "        distance_to_goal = np.linalg.norm(direction_to_goal)\n",
    "        if distance_to_goal > 0:\n",
    "            direction_to_goal = direction_to_goal / distance_to_goal  # Normaliser\n",
    "        \n",
    "        # Calcul des angles\n",
    "        wind_speed = np.linalg.norm(wind_at_boat)\n",
    "        wind_angle = np.degrees(np.arctan2(wind_at_boat[1], wind_at_boat[0])) % 360\n",
    "        \n",
    "        velocity_speed = np.linalg.norm(velocity)\n",
    "        if velocity_speed > 0.01:\n",
    "            velocity_angle = np.degrees(np.arctan2(velocity[1], velocity[0])) % 360\n",
    "        else:\n",
    "            velocity_angle = 0\n",
    "        \n",
    "        goal_angle = np.degrees(np.arctan2(direction_to_goal[1], direction_to_goal[0])) % 360\n",
    "        \n",
    "        # Angle relatif entre le vent et la direction vers l'objectif\n",
    "        wind_to_goal_angle = (goal_angle - wind_angle) % 360\n",
    "        \n",
    "        # Angle relatif entre le vent et la vitesse\n",
    "        wind_to_velocity_angle = (velocity_angle - wind_angle) % 360\n",
    "        \n",
    "        # Obtenir l'efficacité de navigation actuelle\n",
    "        current_efficiency = self.sailing_efficiency.get(int(wind_to_velocity_angle), 0.5)\n",
    "        \n",
    "        # Indicateur de zone de non-navigation (upwind)\n",
    "        upwind_condition = (wind_to_goal_angle < 45 or wind_to_goal_angle > 315)\n",
    "        \n",
    "        # Extraction de caractéristiques du champ de vent\n",
    "        wind_field = observation[6:].reshape(-1, 2)  # Supposons que c'est un champ 2D\n",
    "        \n",
    "        # Calcul de statistiques sur le vent dans différentes régions\n",
    "        # (cela pourrait être plus sophistiqué avec une segmentation appropriée)\n",
    "        mid_point = len(wind_field) // 2\n",
    "        quarter_point = mid_point // 2\n",
    "        \n",
    "        # Moyennes des vents dans différentes régions\n",
    "        wind_north = np.mean(wind_field[mid_point+quarter_point:], axis=0)\n",
    "        wind_south = np.mean(wind_field[:mid_point-quarter_point], axis=0)\n",
    "        wind_east = np.mean(wind_field[mid_point-quarter_point:mid_point+quarter_point], axis=0)\n",
    "        \n",
    "        # Caractéristiques spécifiques à la voile pour différentes manœuvres\n",
    "        # Possibilité de virement de bord (tacking)\n",
    "        port_tack_angle = (wind_angle + 60) % 360\n",
    "        starboard_tack_angle = (wind_angle - 60) % 360\n",
    "        \n",
    "        # Angles relatifs des virements par rapport à l'objectif\n",
    "        port_tack_to_goal = min((port_tack_angle - goal_angle) % 360, (goal_angle - port_tack_angle) % 360)\n",
    "        starboard_tack_to_goal = min((starboard_tack_angle - goal_angle) % 360, (goal_angle - starboard_tack_angle) % 360)\n",
    "        \n",
    "        # Créer le vecteur de caractéristiques\n",
    "        features = np.array([\n",
    "            # Position normalisée\n",
    "            position[0] / 32.0,  # Normaliser par la taille de la grille\n",
    "            position[1] / 32.0,\n",
    "            \n",
    "            # Vitesse et direction\n",
    "            velocity_speed,\n",
    "            np.sin(np.radians(velocity_angle)),\n",
    "            np.cos(np.radians(velocity_angle)),\n",
    "            \n",
    "            # Vent local\n",
    "            wind_speed,\n",
    "            np.sin(np.radians(wind_angle)),\n",
    "            np.cos(np.radians(wind_angle)),\n",
    "            \n",
    "            # Objectif\n",
    "            distance_to_goal / 45.0,  # Normaliser par la distance max possible\n",
    "            np.sin(np.radians(goal_angle)),\n",
    "            np.cos(np.radians(goal_angle)),\n",
    "            \n",
    "            # Angles relatifs\n",
    "            np.sin(np.radians(wind_to_goal_angle)),\n",
    "            np.cos(np.radians(wind_to_goal_angle)),\n",
    "            np.sin(np.radians(wind_to_velocity_angle)),\n",
    "            np.cos(np.radians(wind_to_velocity_angle)),\n",
    "            \n",
    "            # Efficacité de navigation\n",
    "            current_efficiency,\n",
    "            \n",
    "            # Indicateurs de conditions\n",
    "            1.0 if upwind_condition else 0.0,\n",
    "            \n",
    "            # Statistiques régionales du vent\n",
    "            wind_north[0], wind_north[1],\n",
    "            wind_south[0], wind_south[1],\n",
    "            wind_east[0], wind_east[1],\n",
    "            \n",
    "            # Caractéristiques de virement\n",
    "            port_tack_to_goal / 180.0,  # Normaliser\n",
    "            starboard_tack_to_goal / 180.0\n",
    "        ])\n",
    "        \n",
    "        return features\n",
    "        \n",
    "    def act(self, observation):\n",
    "        \"\"\"\n",
    "        Sélectionne une action basée sur l'observation actuelle.\n",
    "        \n",
    "        Args:\n",
    "            observation: Observation de l'environnement\n",
    "        \n",
    "        Returns:\n",
    "            L'action choisie\n",
    "        \"\"\"\n",
    "        # Convertir l'observation en caractéristiques si nécessaire\n",
    "        if self.use_sailing_features:\n",
    "            state = self.extract_features(observation)\n",
    "        else:\n",
    "            state = observation\n",
    "        \n",
    "        # Convertir en tensor et déplacer vers l'appareil d'exécution\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "        \n",
    "        # Passer le state au réseau et obtenir la distribution de politique\n",
    "        with torch.no_grad():\n",
    "            action_probs, state_value = self.policy_value_network(state)\n",
    "        \n",
    "        # Créer la distribution catégorielle\n",
    "        distribution = Categorical(action_probs)\n",
    "        \n",
    "        # Échantillonner une action\n",
    "        action = distribution.sample()\n",
    "        \n",
    "        # Stocker l'information pour l'apprentissage\n",
    "        self.memory.store_action(action.item(), distribution.log_prob(action).item(), state_value.item())\n",
    "        \n",
    "        return action.item()\n",
    "    \n",
    "    def remember(self, observation, action, reward, next_observation, done):\n",
    "        \"\"\"\n",
    "        Stocke une transition dans la mémoire.\n",
    "        \n",
    "        Args:\n",
    "            observation: Observation actuelle\n",
    "            action: Action prise\n",
    "            reward: Récompense reçue\n",
    "            next_observation: Prochaine observation\n",
    "            done: Indicateur de fin d'épisode\n",
    "        \"\"\"\n",
    "        # Convertir l'observation en caractéristiques si nécessaire\n",
    "        if self.use_sailing_features:\n",
    "            state = self.extract_features(observation)\n",
    "            next_state = self.extract_features(next_observation) if next_observation is not None else None\n",
    "        else:\n",
    "            state = observation\n",
    "            next_state = next_observation\n",
    "        \n",
    "        # Stocker la transition dans la mémoire\n",
    "        self.memory.store_transition(state, reward, done)\n",
    "        \n",
    "        # Si l'épisode est terminé, calculer les avantages et préparer pour l'apprentissage\n",
    "        if done:\n",
    "            self.memory.finish_trajectory(last_value=0)\n",
    "        \n",
    "    def learn(self):\n",
    "        \"\"\"\n",
    "        Exécute l'apprentissage PPO sur les expériences collectées.\n",
    "        \"\"\"\n",
    "        # Vérifier s'il y a suffisamment de données pour l'apprentissage\n",
    "        if len(self.memory) < self.mini_batch_size:\n",
    "            return\n",
    "        \n",
    "        # Récupérer les données de la mémoire\n",
    "        states, actions, old_log_probs, returns, advantages = self.memory.get_all()\n",
    "        \n",
    "        # Convertir en tensors\n",
    "        states = torch.FloatTensor(np.vstack(states)).to(self.device)\n",
    "        actions = torch.LongTensor(actions).to(self.device)\n",
    "        old_log_probs = torch.FloatTensor(old_log_probs).to(self.device)\n",
    "        returns = torch.FloatTensor(returns).to(self.device)\n",
    "        advantages = torch.FloatTensor(advantages).to(self.device)\n",
    "        \n",
    "        # Normaliser les avantages (améliore la stabilité)\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "        \n",
    "        # Mise à jour par mini-lots\n",
    "        indices = np.arange(len(states))\n",
    "        \n",
    "        for _ in range(self.ppo_epochs):\n",
    "            # Mélanger les indices\n",
    "            self.np_random.shuffle(indices)\n",
    "            \n",
    "            # Parcourir les mini-lots\n",
    "            for start in range(0, len(indices), self.mini_batch_size):\n",
    "                # Sélectionner le mini-lot\n",
    "                batch_indices = indices[start:start + self.mini_batch_size]\n",
    "                batch_states = states[batch_indices]\n",
    "                batch_actions = actions[batch_indices]\n",
    "                batch_old_log_probs = old_log_probs[batch_indices]\n",
    "                batch_returns = returns[batch_indices]\n",
    "                batch_advantages = advantages[batch_indices]\n",
    "                \n",
    "                # Passer les états au réseau\n",
    "                action_probs, state_values = self.policy_value_network(batch_states)\n",
    "                distribution = Categorical(action_probs)\n",
    "                \n",
    "                # Obtenir les nouveaux log probs et l'entropie\n",
    "                new_log_probs = distribution.log_prob(batch_actions)\n",
    "                entropy = distribution.entropy().mean()\n",
    "                \n",
    "                # Calculer le ratio\n",
    "                ratio = torch.exp(new_log_probs - batch_old_log_probs)\n",
    "                \n",
    "                # Calcul des termes de perte clippés et non clippés\n",
    "                surr1 = ratio * batch_advantages\n",
    "                surr2 = torch.clamp(ratio, 1.0 - self.clip_param, 1.0 + self.clip_param) * batch_advantages\n",
    "                \n",
    "                # Pertes\n",
    "                policy_loss = -torch.min(surr1, surr2).mean()\n",
    "                value_loss = F.mse_loss(state_values.squeeze(), batch_returns)\n",
    "                \n",
    "                # Perte totale\n",
    "                loss = policy_loss + self.value_coef * value_loss - self.entropy_coef * entropy\n",
    "                \n",
    "                # Mise à jour des poids\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                # Écrêtage du gradient (évite les mises à jour trop grandes)\n",
    "                nn.utils.clip_grad_norm_(self.policy_value_network.parameters(), self.max_grad_norm)\n",
    "                self.optimizer.step()\n",
    "        \n",
    "        # Réinitialiser la mémoire\n",
    "        self.memory.clear()\n",
    "        \n",
    "        # Incrémenter le compteur d'étapes d'entraînement\n",
    "        self.training_step += 1\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Réinitialise l'agent pour un nouvel épisode.\"\"\"\n",
    "        # Rien à faire pour PPO ici car la mémoire est gérée séparément\n",
    "        pass\n",
    "    \n",
    "    def seed(self, seed=None):\n",
    "        \"\"\"Définit la graine aléatoire.\"\"\"\n",
    "        self.np_random = np.random.default_rng(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed(seed)\n",
    "    \n",
    "    def save(self, path):\n",
    "        \"\"\"Sauvegarde le modèle.\"\"\"\n",
    "        torch.save({\n",
    "            'policy_value_network': self.policy_value_network.state_dict(),\n",
    "            'optimizer': self.optimizer.state_dict(),\n",
    "            'training_step': self.training_step\n",
    "        }, path)\n",
    "    \n",
    "    def load(self, path):\n",
    "        \"\"\"Charge le modèle.\"\"\"\n",
    "        checkpoint = torch.load(path, map_location=self.device)\n",
    "        self.policy_value_network.load_state_dict(checkpoint['policy_value_network'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        self.training_step = checkpoint['training_step']\n",
    "\n",
    "\n",
    "class PolicyValueNetwork(nn.Module):\n",
    "    \"\"\"Réseau de politique et de valeur pour PPO.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        \"\"\"\n",
    "        Initialise le réseau.\n",
    "        \n",
    "        Args:\n",
    "            input_dim: Dimension de l'entrée (observation)\n",
    "            hidden_dim: Dimension des couches cachées\n",
    "            output_dim: Dimension de la sortie (nombre d'actions)\n",
    "        \"\"\"\n",
    "        super(PolicyValueNetwork, self).__init__()\n",
    "        \n",
    "        # Couches partagées\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Couches de politique (actor)\n",
    "        self.policy = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 2, output_dim),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        \n",
    "        # Couches de valeur (critic)\n",
    "        self.value = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 2, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Passe avant dans le réseau.\n",
    "        \n",
    "        Args:\n",
    "            x: Entrée (observation)\n",
    "        \n",
    "        Returns:\n",
    "            Tuple (probabilités d'action, valeur d'état)\n",
    "        \"\"\"\n",
    "        shared_features = self.shared(x)\n",
    "        action_probs = self.policy(shared_features)\n",
    "        state_value = self.value(shared_features)\n",
    "        \n",
    "        return action_probs, state_value\n",
    "\n",
    "\n",
    "class PPOMemory:\n",
    "    \"\"\"Mémoire pour stocker les expériences pour PPO.\"\"\"\n",
    "    \n",
    "    def __init__(self, gamma=0.99, gae_lambda=0.95):\n",
    "        \"\"\"Initialise la mémoire.\"\"\"\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.log_probs = []\n",
    "        self.rewards = []\n",
    "        self.values = []\n",
    "        self.dones = []\n",
    "        self.returns = []\n",
    "        self.advantages = []\n",
    "        \n",
    "        # Paramètres\n",
    "        self.gamma = gamma\n",
    "        self.gae_lambda = gae_lambda\n",
    "        \n",
    "        # Trajectoire en cours\n",
    "        self.current_trajectory = {\n",
    "            'states': [],\n",
    "            'actions': [],\n",
    "            'log_probs': [],\n",
    "            'rewards': [],\n",
    "            'values': [],\n",
    "            'dones': []\n",
    "        }\n",
    "    \n",
    "    def store_action(self, action, log_prob, value):\n",
    "        \"\"\"Stocke les informations d'action.\"\"\"\n",
    "        self.current_trajectory['actions'].append(action)\n",
    "        self.current_trajectory['log_probs'].append(log_prob)\n",
    "        self.current_trajectory['values'].append(value)\n",
    "    \n",
    "    def store_transition(self, state, reward, done):\n",
    "        \"\"\"Stocke une transition.\"\"\"\n",
    "        self.current_trajectory['states'].append(state)\n",
    "        self.current_trajectory['rewards'].append(reward)\n",
    "        self.current_trajectory['dones'].append(done)\n",
    "    \n",
    "    def finish_trajectory(self, last_value=0):\n",
    "        \"\"\"Finalise la trajectoire courante et calcule les avantages.\"\"\"\n",
    "        # Vérifier que les données sont cohérentes\n",
    "        num_transitions = len(self.current_trajectory['rewards'])\n",
    "        num_actions = len(self.current_trajectory['actions'])\n",
    "        \n",
    "        # S'il n'y a pas de données, ne rien faire\n",
    "        if num_transitions == 0 or num_actions == 0:\n",
    "            return\n",
    "        \n",
    "        # S'assurer que nous avons le même nombre d'actions et de transitions\n",
    "        if num_transitions != num_actions:\n",
    "            print(f\"Warning: {num_transitions} transitions but {num_actions} actions\")\n",
    "            min_length = min(num_transitions, num_actions)\n",
    "            for key in self.current_trajectory:\n",
    "                self.current_trajectory[key] = self.current_trajectory[key][:min_length]\n",
    "        \n",
    "        # Copier les données de la trajectoire\n",
    "        states = self.current_trajectory['states']\n",
    "        actions = self.current_trajectory['actions']\n",
    "        log_probs = self.current_trajectory['log_probs']\n",
    "        rewards = self.current_trajectory['rewards']\n",
    "        values = self.current_trajectory['values']\n",
    "        dones = self.current_trajectory['dones']\n",
    "        \n",
    "        # Calcul des avantages et retours\n",
    "        returns = []\n",
    "        advantages = []\n",
    "        \n",
    "        # Ajouter la valeur finale\n",
    "        next_value = last_value\n",
    "        next_advantage = 0\n",
    "        \n",
    "        # Calculer les retours et avantages en parcourant la trajectoire à l'envers\n",
    "        for t in reversed(range(len(rewards))):\n",
    "            # Calculer le retour\n",
    "            returns.insert(0, rewards[t] + self.gamma * next_value * (1 - dones[t]))\n",
    "            \n",
    "            # Calculer l'avantage avec GAE (Generalized Advantage Estimation)\n",
    "            delta = rewards[t] + self.gamma * next_value * (1 - dones[t]) - values[t]\n",
    "            advantage = delta + self.gamma * self.gae_lambda * next_advantage * (1 - dones[t])\n",
    "            advantages.insert(0, advantage)\n",
    "            \n",
    "            # Mettre à jour pour l'étape suivante\n",
    "            next_value = values[t]\n",
    "            next_advantage = advantage\n",
    "        \n",
    "        # Ajouter les données de la trajectoire à la mémoire\n",
    "        self.states.extend(states)\n",
    "        self.actions.extend(actions)\n",
    "        self.log_probs.extend(log_probs)\n",
    "        self.rewards.extend(rewards)\n",
    "        self.values.extend(values)\n",
    "        self.dones.extend(dones)\n",
    "        self.returns.extend(returns)\n",
    "        self.advantages.extend(advantages)\n",
    "        \n",
    "        # Réinitialiser la trajectoire courante\n",
    "        self.clear_trajectory()\n",
    "    \n",
    "    def clear_trajectory(self):\n",
    "        \"\"\"Efface la trajectoire courante.\"\"\"\n",
    "        for key in self.current_trajectory:\n",
    "            self.current_trajectory[key] = []\n",
    "    \n",
    "    def get_all(self):\n",
    "        \"\"\"\n",
    "        Récupère toutes les données de la mémoire.\n",
    "        \n",
    "        Returns:\n",
    "            Tuple (états, actions, log_probs, retours, avantages)\n",
    "        \"\"\"\n",
    "        return (\n",
    "            self.states,\n",
    "            self.actions,\n",
    "            self.log_probs,\n",
    "            self.returns,\n",
    "            self.advantages\n",
    "        )\n",
    "    \n",
    "    def clear(self):\n",
    "        \"\"\"Efface la mémoire.\"\"\"\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.log_probs = []\n",
    "        self.rewards = []\n",
    "        self.values = []\n",
    "        self.dones = []\n",
    "        self.returns = []\n",
    "        self.advantages = []\n",
    "        self.clear_trajectory()\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Retourne la taille de la mémoire.\"\"\"\n",
    "        return len(self.states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting PPO training...\n",
      "Episode 1: Steps=1000, Reward=0.0, Position=[0 0], Goal reached=True\n",
      "Episode 2: Steps=1000, Reward=0.0, Position=[0 2], Goal reached=True\n",
      "Episode 3: Steps=1000, Reward=0.0, Position=[0 5], Goal reached=True\n",
      "Episode 4: Steps=1000, Reward=0.0, Position=[0 1], Goal reached=True\n",
      "Episode 5: Steps=1000, Reward=0.0, Position=[2 0], Goal reached=True\n",
      "Episode 6: Steps=1000, Reward=0.0, Position=[0 5], Goal reached=True\n",
      "Episode 7: Steps=1000, Reward=0.0, Position=[0 2], Goal reached=True\n",
      "Episode 8: Steps=1000, Reward=0.0, Position=[0 0], Goal reached=True\n",
      "Episode 9: Steps=1000, Reward=0.0, Position=[0 0], Goal reached=True\n",
      "Episode 10: Steps=1000, Reward=0.0, Position=[2 0], Goal reached=True\n",
      "Average over last 10 episodes: Reward=0.00, Length=1000.00\n",
      "Episode 11: Steps=1000, Reward=0.0, Position=[0 1], Goal reached=True\n",
      "Episode 12: Steps=1000, Reward=0.0, Position=[1 0], Goal reached=True\n",
      "Episode 13: Steps=1000, Reward=0.0, Position=[0 0], Goal reached=True\n",
      "Episode 14: Steps=1000, Reward=0.0, Position=[0 0], Goal reached=True\n",
      "Episode 15: Steps=1000, Reward=0.0, Position=[0 1], Goal reached=True\n",
      "Episode 16: Steps=1000, Reward=0.0, Position=[0 0], Goal reached=True\n",
      "Episode 17: Steps=1000, Reward=0.0, Position=[3 2], Goal reached=True\n",
      "Episode 18: Steps=1000, Reward=0.0, Position=[0 1], Goal reached=True\n",
      "Episode 19: Steps=1000, Reward=0.0, Position=[2 0], Goal reached=True\n",
      "Episode 20: Steps=1000, Reward=0.0, Position=[1 0], Goal reached=True\n",
      "Average over last 10 episodes: Reward=0.00, Length=1000.00\n",
      "Episode 21: Steps=1000, Reward=0.0, Position=[0 4], Goal reached=True\n",
      "Episode 22: Steps=1000, Reward=0.0, Position=[0 0], Goal reached=True\n",
      "Episode 23: Steps=1000, Reward=0.0, Position=[0 0], Goal reached=True\n",
      "Episode 24: Steps=1000, Reward=0.0, Position=[0 0], Goal reached=True\n",
      "Episode 25: Steps=1000, Reward=0.0, Position=[0 0], Goal reached=True\n",
      "Episode 26: Steps=1000, Reward=0.0, Position=[0 1], Goal reached=True\n",
      "Episode 27: Steps=1000, Reward=0.0, Position=[0 1], Goal reached=True\n",
      "Episode 28: Steps=1000, Reward=0.0, Position=[0 1], Goal reached=True\n",
      "Episode 29: Steps=1000, Reward=0.0, Position=[0 0], Goal reached=True\n",
      "Episode 30: Steps=1000, Reward=0.0, Position=[1 0], Goal reached=True\n",
      "Average over last 10 episodes: Reward=0.00, Length=1000.00\n",
      "Episode 31: Steps=1000, Reward=0.0, Position=[0 0], Goal reached=True\n",
      "Episode 32: Steps=1000, Reward=0.0, Position=[2 0], Goal reached=True\n",
      "Episode 33: Steps=1000, Reward=0.0, Position=[0 1], Goal reached=True\n",
      "Episode 34: Steps=1000, Reward=0.0, Position=[5 0], Goal reached=True\n",
      "Episode 35: Steps=1000, Reward=0.0, Position=[0 0], Goal reached=True\n",
      "Episode 36: Steps=1000, Reward=0.0, Position=[0 0], Goal reached=True\n",
      "Episode 37: Steps=1000, Reward=0.0, Position=[3 1], Goal reached=True\n",
      "Episode 38: Steps=1000, Reward=0.0, Position=[1 1], Goal reached=True\n",
      "Episode 39: Steps=1000, Reward=0.0, Position=[0 0], Goal reached=True\n",
      "Episode 40: Steps=1000, Reward=0.0, Position=[2 0], Goal reached=True\n",
      "Average over last 10 episodes: Reward=0.00, Length=1000.00\n",
      "Episode 41: Steps=1000, Reward=0.0, Position=[0 0], Goal reached=True\n",
      "Episode 42: Steps=1000, Reward=0.0, Position=[4 1], Goal reached=True\n",
      "Episode 43: Steps=1000, Reward=0.0, Position=[0 0], Goal reached=True\n",
      "Episode 44: Steps=1000, Reward=0.0, Position=[1 0], Goal reached=True\n",
      "Episode 45: Steps=1000, Reward=0.0, Position=[1 0], Goal reached=True\n",
      "Episode 46: Steps=1000, Reward=0.0, Position=[3 0], Goal reached=True\n",
      "Episode 47: Steps=1000, Reward=0.0, Position=[0 0], Goal reached=True\n",
      "Episode 48: Steps=1000, Reward=0.0, Position=[8 0], Goal reached=True\n",
      "Episode 49: Steps=1000, Reward=0.0, Position=[5 0], Goal reached=True\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[64]\u001b[39m\u001b[32m, line 66\u001b[39m\n\u001b[32m     64\u001b[39m \u001b[38;5;66;03m# Apprendre si c'est le moment\u001b[39;00m\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m total_steps % update_frequency == \u001b[32m0\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m     \u001b[43mppo_agent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[38;5;66;03m# Sortir si l'épisode est terminé\u001b[39;00m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m done \u001b[38;5;129;01mor\u001b[39;00m truncated:\n\u001b[32m     70\u001b[39m     \u001b[38;5;66;03m# Finaliser la trajectoire\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[54]\u001b[39m\u001b[32m, line 362\u001b[39m, in \u001b[36mPPOSailingAgent.learn\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    360\u001b[39m         loss.backward()\n\u001b[32m    361\u001b[39m         \u001b[38;5;66;03m# Écrêtage du gradient (évite les mises à jour trop grandes)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m362\u001b[39m         \u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mutils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclip_grad_norm_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpolicy_value_network\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_grad_norm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    363\u001b[39m         \u001b[38;5;28mself\u001b[39m.optimizer.step()\n\u001b[32m    365\u001b[39m \u001b[38;5;66;03m# Réinitialiser la mémoire\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py:38\u001b[39m, in \u001b[36m_no_grad.<locals>._no_grad_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_no_grad_wrapper\u001b[39m(*args, **kwargs):\n\u001b[32m     37\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py:217\u001b[39m, in \u001b[36mclip_grad_norm_\u001b[39m\u001b[34m(parameters, max_norm, norm_type, error_if_nonfinite, foreach)\u001b[39m\n\u001b[32m    214\u001b[39m     parameters = [parameters]\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    216\u001b[39m     \u001b[38;5;66;03m# prevent generators from being exhausted\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     parameters = \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    218\u001b[39m grads = [p.grad \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m parameters \u001b[38;5;28;01mif\u001b[39;00m p.grad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[32m    219\u001b[39m total_norm = _get_total_norm(grads, norm_type, error_if_nonfinite, foreach)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/nn/modules/module.py:2642\u001b[39m, in \u001b[36mModule.parameters\u001b[39m\u001b[34m(self, recurse)\u001b[39m\n\u001b[32m   2620\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mparameters\u001b[39m(\u001b[38;5;28mself\u001b[39m, recurse: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mTrue\u001b[39;00m) -> Iterator[Parameter]:\n\u001b[32m   2621\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Return an iterator over module parameters.\u001b[39;00m\n\u001b[32m   2622\u001b[39m \n\u001b[32m   2623\u001b[39m \u001b[33;03m    This is typically passed to an optimizer.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2640\u001b[39m \n\u001b[32m   2641\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2642\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnamed_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecurse\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrecurse\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2643\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mparam\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/nn/modules/module.py:2675\u001b[39m, in \u001b[36mModule.named_parameters\u001b[39m\u001b[34m(self, prefix, recurse, remove_duplicate)\u001b[39m\n\u001b[32m   2648\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.\u001b[39;00m\n\u001b[32m   2649\u001b[39m \n\u001b[32m   2650\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2667\u001b[39m \n\u001b[32m   2668\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2669\u001b[39m gen = \u001b[38;5;28mself\u001b[39m._named_members(\n\u001b[32m   2670\u001b[39m     \u001b[38;5;28;01mlambda\u001b[39;00m module: module._parameters.items(),\n\u001b[32m   2671\u001b[39m     prefix=prefix,\n\u001b[32m   2672\u001b[39m     recurse=recurse,\n\u001b[32m   2673\u001b[39m     remove_duplicate=remove_duplicate,\n\u001b[32m   2674\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m2675\u001b[39m \u001b[38;5;28;01myield from\u001b[39;00m gen\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/nn/modules/module.py:2610\u001b[39m, in \u001b[36mModule._named_members\u001b[39m\u001b[34m(self, get_members_fn, prefix, recurse, remove_duplicate)\u001b[39m\n\u001b[32m   2604\u001b[39m memo = \u001b[38;5;28mset\u001b[39m()\n\u001b[32m   2605\u001b[39m modules = (\n\u001b[32m   2606\u001b[39m     \u001b[38;5;28mself\u001b[39m.named_modules(prefix=prefix, remove_duplicate=remove_duplicate)\n\u001b[32m   2607\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m recurse\n\u001b[32m   2608\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m [(prefix, \u001b[38;5;28mself\u001b[39m)]\n\u001b[32m   2609\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m2610\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodule_prefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodules\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2611\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmembers\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_members_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2612\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmembers\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/nn/modules/module.py:2826\u001b[39m, in \u001b[36mModule.named_modules\u001b[39m\u001b[34m(self, memo, prefix, remove_duplicate)\u001b[39m\n\u001b[32m   2824\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m memo:\n\u001b[32m   2825\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m remove_duplicate:\n\u001b[32m-> \u001b[39m\u001b[32m2826\u001b[39m         \u001b[43mmemo\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   2827\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m prefix, \u001b[38;5;28mself\u001b[39m\n\u001b[32m   2828\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m name, module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._modules.items():\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from src.env_sailing import SailingEnv\n",
    "from src.initial_windfields import get_initial_windfield\n",
    "\n",
    "# Créer notre agent PPO\n",
    "ppo_agent = PPOSailingAgent(\n",
    "    learning_rate=0.0003,\n",
    "    gamma=0.99,\n",
    "    gae_lambda=0.95,\n",
    "    clip_param=0.2,\n",
    "    value_coef=0.5,\n",
    "    entropy_coef=0.01,\n",
    "    max_grad_norm=0.5,\n",
    "    ppo_epochs=4,\n",
    "    mini_batch_size=64,\n",
    "    use_sailing_features=True\n",
    ")\n",
    "\n",
    "# Définir une graine fixe pour la reproductibilité\n",
    "np.random.seed(42)\n",
    "ppo_agent.seed(42)\n",
    "\n",
    "# Créer l'environnement avec un champ de vent\n",
    "env = SailingEnv(**get_initial_windfield('simple_static'))\n",
    "\n",
    "# Paramètres d'entraînement\n",
    "num_episodes = 1000\n",
    "max_steps = 1000\n",
    "batch_size = 2048  # Taille du lot pour PPO (nombre d'étapes avant la mise à jour)\n",
    "update_frequency = 2048  # Fréquence de mise à jour (en pas)\n",
    "\n",
    "# Variables pour le suivi\n",
    "total_steps = 0\n",
    "episode_rewards = []\n",
    "episode_lengths = []\n",
    "\n",
    "# Boucle d'entraînement\n",
    "print(\"Starting PPO training...\")\n",
    "for episode in range(num_episodes):\n",
    "    # Réinitialiser l'environnement et obtenir l'état initial\n",
    "    observation, info = env.reset(seed=episode)\n",
    "    \n",
    "    # Informer l'agent de la position de l'objectif\n",
    "    ppo_agent.goal_position = env.goal_position if hasattr(env, 'goal_position') else np.array([16, 31])\n",
    "    \n",
    "    episode_reward = 0\n",
    "    episode_length = 0\n",
    "    \n",
    "    # Boucle d'un épisode\n",
    "    for step in range(max_steps):\n",
    "        # Sélectionner et exécuter une action\n",
    "        action = ppo_agent.act(observation)\n",
    "        next_observation, reward, done, truncated, info = env.step(action)\n",
    "        \n",
    "        # Stocker la transition\n",
    "        ppo_agent.remember(observation, action, reward, next_observation, done or truncated)\n",
    "        \n",
    "        # Mettre à jour l'observation et les compteurs\n",
    "        observation = next_observation\n",
    "        episode_reward += reward\n",
    "        episode_length += 1\n",
    "        total_steps += 1\n",
    "        \n",
    "        # Apprendre si c'est le moment\n",
    "        if total_steps % update_frequency == 0:\n",
    "            ppo_agent.learn()\n",
    "        \n",
    "        # Sortir si l'épisode est terminé\n",
    "        if done or truncated:\n",
    "            # Finaliser la trajectoire\n",
    "            if not done:  # Si tronqué mais pas terminé, estimer la valeur finale\n",
    "                # Convertir l'observation en caractéristiques si nécessaire\n",
    "                if ppo_agent.use_sailing_features:\n",
    "                    state = ppo_agent.extract_features(observation)\n",
    "                else:\n",
    "                    state = observation\n",
    "                \n",
    "                # Obtenir la valeur d'état finale\n",
    "                state_tensor = torch.FloatTensor(state).unsqueeze(0).to(ppo_agent.device)\n",
    "                with torch.no_grad():\n",
    "                    _, state_value = ppo_agent.policy_value_network(state_tensor)\n",
    "                last_value = state_value.item()\n",
    "            else:\n",
    "                last_value = 0  # Épisode terminé, pas de récompense future\n",
    "            \n",
    "            ppo_agent.memory.finish_trajectory(last_value=last_value)\n",
    "            break\n",
    "    \n",
    "    # Enregistrer les statistiques de l'épisode\n",
    "    episode_rewards.append(episode_reward)\n",
    "    episode_lengths.append(episode_length)\n",
    "    \n",
    "    # Afficher les progrès\n",
    "    avg_reward = np.mean(episode_rewards[-10:]) if len(episode_rewards) >= 10 else np.mean(episode_rewards)\n",
    "    avg_length = np.mean(episode_lengths[-10:]) if len(episode_lengths) >= 10 else np.mean(episode_lengths)\n",
    "    \n",
    "    print(f\"Episode {episode+1}: Steps={episode_length}, Reward={episode_reward}, \" +\n",
    "          f\"Position={info['position']}, Goal reached={done}\")\n",
    "    \n",
    "    if (episode + 1) % 10 == 0:\n",
    "        print(f\"Average over last 10 episodes: Reward={avg_reward:.2f}, Length={avg_length:.2f}\")\n",
    "        \n",
    "        # Sauvegarder le modèle périodiquement\n",
    "        ppo_agent.save(f\"ppo_sailing_agent_ep{episode+1}.pt\")\n",
    "\n",
    "print(\"\\nTraining completed!\")\n",
    "print(f\"Final average reward (last 10 episodes): {np.mean(episode_rewards[-10:]):.2f}\")\n",
    "\n",
    "# Sauvegarder le modèle final\n",
    "ppo_agent.save(\"ppo_sailing_agent_final.pt\")\n",
    "\n",
    "# Test de l'agent entraîné\n",
    "print(\"\\nTesting trained agent...\")\n",
    "observation, info = env.reset(seed=1000)  # Nouvelle graine pour le test\n",
    "total_reward = 0\n",
    "step_count = 0\n",
    "\n",
    "# Désactiver l'exploration pour le test\n",
    "is_training = False\n",
    "\n",
    "while step_count < 500:\n",
    "    action = ppo_agent.act(observation)\n",
    "    observation, reward, done, truncated, info = env.step(action)\n",
    "    total_reward += reward\n",
    "    step_count += 1\n",
    "    \n",
    "    if step_count % 50 == 0:\n",
    "       print(f\"Test - Step {step_count}: Position={info['position']}, Reward={reward}\")\n",
    "   \n",
    "    if done or truncated:\n",
    "       break\n",
    "\n",
    "print(f\"\\nTest completed after {step_count} steps with reward: {total_reward}\")\n",
    "print(f\"Final position: {info['position']}\")\n",
    "print(f\"Goal reached: {done}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "WindAwareNavigator - Agent de navigation à voile avancé pour le Sailing Challenge\n",
    "\n",
    "Cet agent combine:\n",
    "1. Compréhension de la physique de la voile (points de navigation optimaux)\n",
    "2. Planification de trajectoire en fonction du champ de vent complet\n",
    "3. Apprentissage par renforcement avec état augmenté\n",
    "4. Stratégies adaptatives basées sur les configurations de vent\n",
    "\n",
    "L'agent est capable de:\n",
    "- Analyser le champ de vent complet pour planifier des itinéraires\n",
    "- Naviguer efficacement contre le vent (louvoyer)\n",
    "- S'adapter à l'évolution des conditions de vent\n",
    "- Choisir les meilleurs angles par rapport au vent pour maximiser la vitesse\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from agents.base_agent import BaseAgent\n",
    "\n",
    "class WindAwareNavigator(BaseAgent):\n",
    "    \"\"\"\n",
    "    Agent avancé pour le Sailing Challenge qui utilise la compréhension de la physique de la voile \n",
    "    et une approche hybride combinant planification et apprentissage par renforcement.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialise l'agent avec les paramètres nécessaires.\"\"\"\n",
    "        super().__init__()\n",
    "        self.np_random = np.random.default_rng()\n",
    "        \n",
    "        # Paramètres de l'environnement\n",
    "        self.grid_size = (32, 32)\n",
    "        self.goal_position = np.array([self.grid_size[0] // 2, self.grid_size[1] - 1])\n",
    "        \n",
    "        # Paramètres de discrétisation pour l'état\n",
    "        self.position_bins = 8\n",
    "        self.velocity_bins = 6\n",
    "        self.wind_bins = 8\n",
    "        \n",
    "        # Paramètres de la physique de la voile\n",
    "        self.optimal_beam_reach_angle = np.pi/2  # 90 degrés, perpendiculaire au vent\n",
    "        self.close_hauled_angle = np.pi/4        # 45 degrés, près du vent\n",
    "        self.no_go_zone_angle = np.pi/6          # 30 degrés, zone impossible\n",
    "        \n",
    "        # Table Q pour l'apprentissage par renforcement\n",
    "        self.q_table = {}\n",
    "        self._init_q_table()\n",
    "        \n",
    "        # Paramètres de la stratégie\n",
    "        self.exploration_rate = 0.05  # Faible taux pour l'exploitation en phase de test\n",
    "        self.use_wind_planning = True\n",
    "        self.use_tacking = True\n",
    "        self.current_tack_direction = 1  # 1 pour tribord, -1 pour bâbord\n",
    "        self.patience = 0\n",
    "        \n",
    "        # État interne\n",
    "        self.previous_position = None\n",
    "        self.steps_without_progress = 0\n",
    "        self.last_distance_to_goal = float('inf')\n",
    "        \n",
    "    def _init_q_table(self):\n",
    "        \"\"\"Initialise la table Q avec quelques valeurs pré-calculées basées sur la physique de la voile.\"\"\"\n",
    "        # Points cardinaux pour l'indexation des actions\n",
    "        # 0: Nord, 1: Nord-Est, 2: Est, 3: Sud-Est, 4: Sud, 5: Sud-Ouest, 6: Ouest, 7: Nord-Ouest, 8: Rester en place\n",
    "        \n",
    "        # Préchargement de quelques comportements de base liés à la physique de la voile\n",
    "        # Ces valeurs sont des heuristiques qui seront affinées par l'apprentissage\n",
    "        \n",
    "        # Règle générale: si le vent vient du nord, préférez les directions est/ouest pour louvoyer\n",
    "        north_wind = (2, 2, 0, 0)  # Position près du départ, vent du nord\n",
    "        self.q_table[north_wind] = np.array([0.1, 0.5, 0.9, 0.5, 0.1, 0.5, 0.9, 0.5, 0.0])\n",
    "        \n",
    "        # Si le vent vient du nord-est, louvoyez via le nord-ouest/sud-est\n",
    "        northeast_wind = (2, 2, 0, 1)  # Position près du départ, vent du nord-est\n",
    "        self.q_table[northeast_wind] = np.array([0.1, 0.1, 0.5, 0.9, 0.5, 0.1, 0.5, 0.9, 0.0])\n",
    "        \n",
    "        # Si le vent vient du nord-ouest, louvoyez via le nord-est/sud-ouest\n",
    "        northwest_wind = (2, 2, 0, 7)  # Position près du départ, vent du nord-ouest\n",
    "        self.q_table[northwest_wind] = np.array([0.1, 0.9, 0.5, 0.1, 0.5, 0.9, 0.5, 0.1, 0.0])\n",
    "        \n",
    "        # Pour les vents sud, préférez aller directement vers le nord (le plus efficace)\n",
    "        south_wind = (2, 2, 0, 4)  # Position près du départ, vent du sud\n",
    "        self.q_table[south_wind] = np.array([0.9, 0.7, 0.5, 0.3, 0.1, 0.3, 0.5, 0.7, 0.0])\n",
    "        \n",
    "    def discretize_state(self, observation):\n",
    "        \"\"\"Convertit l'observation continue en état discret pour la table Q.\"\"\"\n",
    "        # Extrait position, vitesse et vent de l'observation\n",
    "        x, y = observation[0], observation[1]\n",
    "        vx, vy = observation[2], observation[3]\n",
    "        wx, wy = observation[4], observation[5]\n",
    "        \n",
    "        # Discrétisation de la position\n",
    "        x_bin = min(int(x / self.grid_size[0] * self.position_bins), self.position_bins - 1)\n",
    "        y_bin = min(int(y / self.grid_size[1] * self.position_bins), self.position_bins - 1)\n",
    "        \n",
    "        # Discrétisation de la direction de la vitesse (ignore la magnitude pour simplifier)\n",
    "        v_magnitude = np.sqrt(vx**2 + vy**2)\n",
    "        if v_magnitude < 0.1:  # Si la vitesse est très faible, bin spécial\n",
    "            v_bin = 0\n",
    "        else:\n",
    "            v_direction = np.arctan2(vy, vx)  # Plage: [-pi, pi]\n",
    "            v_bin = int(((v_direction + np.pi) / (2 * np.pi) * self.velocity_bins)) % self.velocity_bins\n",
    "        \n",
    "        # Discrétisation de la direction du vent\n",
    "        wind_direction = np.arctan2(wy, wx)  # Plage: [-pi, pi]\n",
    "        wind_bin = int(((wind_direction + np.pi) / (2 * np.pi) * self.wind_bins)) % self.wind_bins\n",
    "        \n",
    "        # Retourne le tuple d'état discret\n",
    "        return (x_bin, y_bin, v_bin, wind_bin)\n",
    "    \n",
    "    def calculate_sailing_efficiency(self, boat_direction, wind_direction):\n",
    "        \"\"\"\n",
    "        Calcule l'efficacité de navigation basée sur l'angle entre la direction du bateau et le vent.\n",
    "        \n",
    "        Args:\n",
    "            boat_direction: Vecteur normalisé de la direction souhaitée du bateau\n",
    "            wind_direction: Vecteur normalisé de la direction du vent (où le vent va VERS)\n",
    "            \n",
    "        Returns:\n",
    "            sailing_efficiency: Flottant entre 0.05 et 1.0 représentant l'efficacité de navigation\n",
    "        \"\"\"\n",
    "        # Inverser la direction du vent pour obtenir d'où vient le vent\n",
    "        wind_from = -wind_direction\n",
    "        \n",
    "        # Calculer l'angle entre le vent et la direction\n",
    "        cos_angle = np.dot(wind_from, boat_direction)\n",
    "        cos_angle = np.clip(cos_angle, -1.0, 1.0)  # Éviter les erreurs numériques\n",
    "        wind_angle = np.arccos(cos_angle)\n",
    "        \n",
    "        # Calcul de l'efficacité de navigation basé sur l'angle au vent\n",
    "        if wind_angle < self.no_go_zone_angle:  # Moins de 30 degrés par rapport au vent\n",
    "            sailing_efficiency = 0.05  # Efficacité faible mais non nulle dans la zone interdite\n",
    "        elif wind_angle < self.close_hauled_angle:  # Entre 30 et 45 degrés\n",
    "            sailing_efficiency = 0.1 + 0.4 * (wind_angle - self.no_go_zone_angle) / (self.close_hauled_angle - self.no_go_zone_angle)\n",
    "        elif wind_angle < self.optimal_beam_reach_angle:  # Entre 45 et 90 degrés\n",
    "            sailing_efficiency = 0.5 + 0.5 * (wind_angle - self.close_hauled_angle) / (self.optimal_beam_reach_angle - self.close_hauled_angle)\n",
    "        elif wind_angle < 3*np.pi/4:  # Entre 90 et 135 degrés\n",
    "            sailing_efficiency = 1.0  # Efficacité maximale\n",
    "        else:  # Plus de 135 degrés\n",
    "            sailing_efficiency = 1.0 - 0.2 * (wind_angle - 3*np.pi/4) / (np.pi/4)\n",
    "            sailing_efficiency = max(0.8, sailing_efficiency)  # Mais toujours bien\n",
    "        \n",
    "        return sailing_efficiency\n",
    "    \n",
    "    def action_to_direction(self, action):\n",
    "        \"\"\"Convertit l'indice d'action en vecteur de direction.\"\"\"\n",
    "        directions = [\n",
    "            (0, 1),     # 0: Nord\n",
    "            (1, 1),     # 1: Nord-Est\n",
    "            (1, 0),     # 2: Est\n",
    "            (1, -1),    # 3: Sud-Est\n",
    "            (0, -1),    # 4: Sud\n",
    "            (-1, -1),   # 5: Sud-Ouest\n",
    "            (-1, 0),    # 6: Ouest\n",
    "            (-1, 1),    # 7: Nord-Ouest\n",
    "            (0, 0)      # 8: Rester en place\n",
    "        ]\n",
    "        return np.array(directions[action])\n",
    "    \n",
    "    def get_upwind_tacking_action(self, position, wind_vector):\n",
    "        \"\"\"\n",
    "        Implémente une stratégie de louvoyage pour naviguer contre le vent.\n",
    "        Alterne entre les directions à environ 45 degrés de part et d'autre du vent.\n",
    "        \"\"\"\n",
    "        # Normaliser le vecteur de vent\n",
    "        wind_magnitude = np.linalg.norm(wind_vector)\n",
    "        if wind_magnitude < 0.001:\n",
    "            return 0  # Par défaut, aller au nord si pas de vent\n",
    "            \n",
    "        wind_normalized = wind_vector / wind_magnitude\n",
    "        \n",
    "        # Calcul de l'angle du vent (d'où il vient)\n",
    "        wind_from_angle = np.arctan2(-wind_normalized[1], -wind_normalized[0])\n",
    "        \n",
    "        # Calculer les angles de louvoyage (environ 45 degrés de chaque côté du vent)\n",
    "        port_tack_angle = wind_from_angle - self.close_hauled_angle\n",
    "        starboard_tack_angle = wind_from_angle + self.close_hauled_angle\n",
    "        \n",
    "        # Décider de changer de bord si nécessaire\n",
    "        distance_to_center = abs(position[0] - self.grid_size[0]/2)\n",
    "        \n",
    "        # Changer de bord si on s'écarte trop du centre ou si on est bloqué\n",
    "        if distance_to_center > self.grid_size[0]/4 or self.steps_without_progress > 5:\n",
    "            self.current_tack_direction *= -1\n",
    "            self.steps_without_progress = 0\n",
    "        \n",
    "        # Sélectionner l'angle en fonction du bord actuel\n",
    "        tack_angle = starboard_tack_angle if self.current_tack_direction > 0 else port_tack_angle\n",
    "        \n",
    "        # Convertir l'angle en vecteur de direction\n",
    "        tack_direction = np.array([np.cos(tack_angle), np.sin(tack_angle)])\n",
    "        \n",
    "        # Trouver l'action la plus proche de cette direction\n",
    "        best_action = 0\n",
    "        best_similarity = -1\n",
    "        \n",
    "        for action in range(8):  # Exclure l'action \"rester en place\"\n",
    "            action_direction = self.action_to_direction(action)\n",
    "            # Calculer la similarité cosinus\n",
    "            similarity = np.dot(tack_direction, action_direction) / (np.linalg.norm(tack_direction) * np.linalg.norm(action_direction))\n",
    "            if similarity > best_similarity:\n",
    "                best_similarity = similarity\n",
    "                best_action = action\n",
    "        \n",
    "        return best_action\n",
    "    \n",
    "    def get_best_action_for_wind(self, position, wind_vector, goal_vector):\n",
    "        \"\"\"\n",
    "        Détermine la meilleure action basée sur le vent, la position actuelle et l'objectif.\n",
    "        Utilise les principes de la physique de la voile.\n",
    "        \"\"\"\n",
    "        # Normaliser les vecteurs\n",
    "        wind_magnitude = np.linalg.norm(wind_vector)\n",
    "        if wind_magnitude < 0.001:\n",
    "            # Si pas de vent, naviguer directement vers l'objectif\n",
    "            return self.get_direct_goal_action(position, goal_vector)\n",
    "            \n",
    "        wind_normalized = wind_vector / wind_magnitude\n",
    "        goal_direction = goal_vector / np.linalg.norm(goal_vector)\n",
    "        \n",
    "        # Déterminer si l'objectif est contre le vent\n",
    "        # Le vent vient de l'opposé de wind_normalized\n",
    "        wind_from_direction = -wind_normalized\n",
    "        angle_to_goal = np.arccos(np.clip(np.dot(wind_from_direction, goal_direction), -1.0, 1.0))\n",
    "        \n",
    "        # Si l'objectif est dans la zone interdite (contre le vent), utiliser le louvoyage\n",
    "        if angle_to_goal < self.close_hauled_angle:\n",
    "            return self.get_upwind_tacking_action(position, wind_vector)\n",
    "        \n",
    "        # Sinon, choisir l'action qui offre la meilleure efficacité dans la direction de l'objectif\n",
    "        best_action = 0\n",
    "        best_efficiency = -1\n",
    "        \n",
    "        for action in range(8):  # Exclure l'action \"rester en place\"\n",
    "            action_direction = self.action_to_direction(action)\n",
    "            \n",
    "            # Calculer l'efficacité de navigation pour cette direction\n",
    "            sailing_efficiency = self.calculate_sailing_efficiency(\n",
    "                action_direction / np.linalg.norm(action_direction), \n",
    "                wind_normalized\n",
    "            )\n",
    "            \n",
    "            # Calculer combien cette action nous rapproche de l'objectif\n",
    "            goal_alignment = np.dot(action_direction, goal_direction)\n",
    "            \n",
    "            # Score combiné: efficacité de navigation * alignement avec l'objectif\n",
    "            combined_score = sailing_efficiency * (0.5 + 0.5 * goal_alignment)\n",
    "            \n",
    "            if combined_score > best_efficiency:\n",
    "                best_efficiency = combined_score\n",
    "                best_action = action\n",
    "        \n",
    "        return best_action\n",
    "    \n",
    "    def get_direct_goal_action(self, position, goal_vector):\n",
    "        \"\"\"Obtient l'action qui mène le plus directement vers l'objectif.\"\"\"\n",
    "        # Normaliser le vecteur d'objectif\n",
    "        if np.linalg.norm(goal_vector) < 0.001:\n",
    "            return 8  # Rester en place si on est déjà à l'objectif\n",
    "            \n",
    "        goal_direction = goal_vector / np.linalg.norm(goal_vector)\n",
    "        \n",
    "        # Trouver l'action la plus alignée avec la direction de l'objectif\n",
    "        best_action = 0\n",
    "        best_alignment = -1\n",
    "        \n",
    "        for action in range(8):  # Exclure l'action \"rester en place\"\n",
    "            action_direction = self.action_to_direction(action)\n",
    "            alignment = np.dot(action_direction, goal_direction) / np.linalg.norm(action_direction)\n",
    "            \n",
    "            if alignment > best_alignment:\n",
    "                best_alignment = alignment\n",
    "                best_action = action\n",
    "        \n",
    "        return best_action\n",
    "    \n",
    "    def analyze_wind_field(self, wind_field_flat, position):\n",
    "        \"\"\"\n",
    "        Analyse le champ de vent complet pour identifier les zones favorables.\n",
    "        Recherche des courants favorables ou des zones de vent fort dans la direction de l'objectif.\n",
    "        \"\"\"\n",
    "        # Reconstruire le champ de vent 2D à partir des données aplaties\n",
    "        wind_field = wind_field_flat.reshape(self.grid_size[1], self.grid_size[0], 2)\n",
    "        \n",
    "        # Calculer le vecteur vers l'objectif\n",
    "        goal_vector = self.goal_position - position\n",
    "        goal_distance = np.linalg.norm(goal_vector)\n",
    "        if goal_distance < 0.001:\n",
    "            return 8  # Déjà à l'objectif\n",
    "            \n",
    "        goal_direction = goal_vector / goal_distance\n",
    "        \n",
    "        # Rechercher des modèles favorables dans le champ de vent\n",
    "        # (simpliste pour l'instant, pourrait être amélioré avec des algorithmes de clustering ou de recherche de chemin)\n",
    "        \n",
    "        # Vérifier s'il y a un couloir de vent favorable direct vers l'objectif\n",
    "        favorable_path = True\n",
    "        step_size = max(1, int(goal_distance / 5))  # Échantillonner 5 points sur le chemin\n",
    "        \n",
    "        for step in range(1, min(6, step_size + 1)):\n",
    "            # Position intermédiaire sur la trajectoire\n",
    "            intermediate_pos = position + goal_direction * step * (goal_distance / 5)\n",
    "            x, y = int(intermediate_pos[0]), int(intermediate_pos[1])\n",
    "            \n",
    "            # Vérifier que la position est dans les limites\n",
    "            if 0 <= x < self.grid_size[0] and 0 <= y < self.grid_size[1]:\n",
    "                # Obtenir le vent à cette position\n",
    "                intermediate_wind = wind_field[y, x]\n",
    "                \n",
    "                # Calculer l'efficacité dans la direction de l'objectif\n",
    "                efficiency = self.calculate_sailing_efficiency(\n",
    "                    goal_direction,\n",
    "                    intermediate_wind / (np.linalg.norm(intermediate_wind) + 1e-10)\n",
    "                )\n",
    "                \n",
    "                # Si l'efficacité est faible à un point quelconque, le chemin n'est pas favorable\n",
    "                if efficiency < 0.5:\n",
    "                    favorable_path = False\n",
    "                    break\n",
    "        \n",
    "        # Si un chemin favorable existe, naviguer directement vers l'objectif\n",
    "        if favorable_path:\n",
    "            return self.get_direct_goal_action(position, goal_vector)\n",
    "        \n",
    "        # Sinon, utiliser la stratégie basée sur le vent local\n",
    "        return self.get_best_action_for_wind(position, wind_field[int(position[1]), int(position[0])], goal_vector)\n",
    "    \n",
    "    def act(self, observation):\n",
    "        \"\"\"\n",
    "        Sélectionne une action basée sur l'observation actuelle.\n",
    "        Utilise une combinaison de la table Q, de l'analyse du champ de vent et de stratégies de voile.\n",
    "        \"\"\"\n",
    "        # Extraire les informations de l'observation\n",
    "        position = np.array([observation[0], observation[1]])\n",
    "        velocity = np.array([observation[2], observation[3]])\n",
    "        wind_at_position = np.array([observation[4], observation[5]])\n",
    "        wind_field_flat = observation[6:]\n",
    "        \n",
    "        # Vecteur vers l'objectif\n",
    "        goal_vector = self.goal_position - position\n",
    "        current_distance_to_goal = np.linalg.norm(goal_vector)\n",
    "        \n",
    "        # Suivre les progrès\n",
    "        if self.previous_position is not None:\n",
    "            distance_improvement = self.last_distance_to_goal - current_distance_to_goal\n",
    "            if distance_improvement < 0.1:  # Si on ne progresse pas assez\n",
    "                self.steps_without_progress += 1\n",
    "            else:\n",
    "                self.steps_without_progress = 0\n",
    "                \n",
    "        self.previous_position = position.copy()\n",
    "        self.last_distance_to_goal = current_distance_to_goal\n",
    "        \n",
    "        # Exploration aléatoire avec faible probabilité (en phase de test)\n",
    "        if self.np_random.random() < self.exploration_rate:\n",
    "            return self.np_random.integers(0, 9)\n",
    "        \n",
    "        # Stratégie basée sur l'état de la table Q\n",
    "        state = self.discretize_state(observation)\n",
    "        if state in self.q_table:\n",
    "            q_action = np.argmax(self.q_table[state])\n",
    "            \n",
    "            # Si nous sommes bloqués trop longtemps, ignorer la table Q\n",
    "            if self.steps_without_progress < 10:\n",
    "                return q_action\n",
    "        \n",
    "        # Si nous sommes ici, soit l'état n'est pas dans la table Q,\n",
    "        # soit nous sommes bloqués et devons essayer une autre approche\n",
    "        \n",
    "        # Analyser le champ de vent si activé\n",
    "        if self.use_wind_planning:\n",
    "            wind_analysis_action = self.analyze_wind_field(wind_field_flat, position)\n",
    "            return wind_analysis_action\n",
    "            \n",
    "        # Utiliser une approche basée sur la physique de la voile comme fallback\n",
    "        return self.get_best_action_for_wind(position, wind_at_position, goal_vector)\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Réinitialise l'agent pour un nouvel épisode.\"\"\"\n",
    "        self.previous_position = None\n",
    "        self.steps_without_progress = 0\n",
    "        self.last_distance_to_goal = float('inf')\n",
    "        self.current_tack_direction = 1 if self.np_random.random() < 0.5 else -1\n",
    "        \n",
    "    def seed(self, seed=None):\n",
    "        \"\"\"Définit la graine aléatoire pour la reproductibilité.\"\"\"\n",
    "        self.np_random = np.random.default_rng(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running the naive agent on the simple_static initial windfield:\n",
      "Step 10: Position=[15  7], Reward=0.0\n",
      "Step 20: Position=[15 12], Reward=0.0\n",
      "Step 30: Position=[14 14], Reward=0.0\n",
      "Step 40: Position=[13 15], Reward=0.0\n",
      "Step 50: Position=[10 18], Reward=0.0\n",
      "Step 60: Position=[ 9 20], Reward=0.0\n",
      "Step 70: Position=[ 7 21], Reward=0.0\n",
      "Step 80: Position=[ 6 22], Reward=0.0\n",
      "Step 90: Position=[ 8 22], Reward=0.0\n",
      "Step 100: Position=[12 22], Reward=0.0\n",
      "Step 110: Position=[15 22], Reward=0.0\n",
      "Step 120: Position=[15 24], Reward=0.0\n",
      "Step 130: Position=[15 26], Reward=0.0\n",
      "Step 140: Position=[15 27], Reward=0.0\n",
      "Step 150: Position=[16 29], Reward=0.0\n",
      "\n",
      "Episode finished after 151 steps with reward: 100.0\n",
      "Final position: [16 30]\n",
      "Goal reached: True\n"
     ]
    }
   ],
   "source": [
    "from src.env_sailing import SailingEnv\n",
    "from src.initial_windfields import get_initial_windfield\n",
    "\n",
    "# Create an environment with a simple test initial windfield\n",
    "env = SailingEnv(**get_initial_windfield('simple_static'))\n",
    "naive_agent = WindAwareNavigator()\n",
    "\n",
    "# Run a single episode\n",
    "observation, info = env.reset(seed=42)\n",
    "total_reward = 0\n",
    "done = False\n",
    "truncated = False\n",
    "step_count = 0\n",
    "\n",
    "print(\"Running the naive agent on the simple_static initial windfield:\")\n",
    "while not (done or truncated) and step_count < 1000:  # Limit to 100 steps\n",
    "    action = naive_agent.act(observation)\n",
    "    observation, reward, done, truncated, info = env.step(action)\n",
    "    total_reward += reward\n",
    "    step_count += 1\n",
    "    \n",
    "    # Print every 10 steps to avoid too much output\n",
    "    if step_count % 10 == 0:\n",
    "        print(f\"Step {step_count}: Position={info['position']}, Reward={reward}\")\n",
    "\n",
    "print(f\"\\nEpisode finished after {step_count} steps with reward: {total_reward}\")\n",
    "print(f\"Final position: {info['position']}\")\n",
    "print(f\"Goal reached: {done}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "WindAwareNavigator - Agent avancé pour le Sailing Challenge\n",
    "\n",
    "Cet agent combine:\n",
    "1. Algorithme A* adapté à la navigation à voile pour une planification optimale de la trajectoire\n",
    "2. Réseaux de neurones pour l'évaluation des états complexes et les prédictions de vent\n",
    "3. Analyse sophistiquée du champ de vent avec clustering et détection de courants\n",
    "4. Stratégies adaptatives basées sur la physique de la voile\n",
    "\n",
    "L'agent est capable de:\n",
    "- Planifier des trajectoires optimales en tenant compte des contraintes de la voile\n",
    "- Apprendre des modèles complexes du comportement du vent et du bateau\n",
    "- Identifier et exploiter des structures de vent avantageuses\n",
    "- S'adapter rapidement aux changements de conditions\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import heapq\n",
    "from collections import defaultdict, deque\n",
    "from agents.base_agent import BaseAgent\n",
    "\n",
    "class WindAwareNavigator(BaseAgent):\n",
    "    \"\"\"\n",
    "    Agent avancé pour le Sailing Challenge qui combine A*, réseaux de neurones et \n",
    "    analyse sophistiquée du champ de vent.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialise l'agent avec les paramètres et modèles nécessaires.\"\"\"\n",
    "        super().__init__()\n",
    "        self.np_random = np.random.default_rng()\n",
    "        \n",
    "        # Paramètres de l'environnement\n",
    "        self.grid_size = (32, 32)\n",
    "        self.goal_position = np.array([self.grid_size[0] // 2, self.grid_size[1] - 1])\n",
    "        \n",
    "        # Paramètres de la physique de la voile\n",
    "        self.optimal_beam_reach_angle = np.pi/2  # 90 degrés, perpendiculaire au vent\n",
    "        self.close_hauled_angle = np.pi/4        # 45 degrés, près du vent\n",
    "        self.no_go_zone_angle = np.pi/6          # 30 degrés, zone interdite\n",
    "        \n",
    "        # Initialisation des modèles de réseaux de neurones simplifiés\n",
    "        self.initialize_neural_networks()\n",
    "        \n",
    "        # État interne\n",
    "        self.previous_position = None\n",
    "        self.steps_without_progress = 0\n",
    "        self.last_distance_to_goal = float('inf')\n",
    "        self.current_tack_direction = 1  # 1 pour tribord, -1 pour bâbord\n",
    "        self.path_cache = {}  # Cache pour les chemins A*\n",
    "        self.current_path = []  # Chemin actuel suivi\n",
    "        \n",
    "        # Mémoire des vents pour analyse de tendances\n",
    "        self.wind_memory = deque(maxlen=10)\n",
    "        \n",
    "        # Paramètres de stratégie et modes\n",
    "        self.exploration_rate = 0.05\n",
    "        self.use_astar = True\n",
    "        self.use_nn_evaluation = True\n",
    "        self.use_wind_analysis = True\n",
    "        \n",
    "        # Seuils et variables de contrôle\n",
    "        self.path_recompute_threshold = 5  # Nombre d'étapes avant de recalculer le chemin\n",
    "        self.steps_since_path_recompute = 0\n",
    "        self.prediction_horizon = 10  # Nombre d'étapes pour prédire l'évolution du vent\n",
    "        \n",
    "    def initialize_neural_networks(self):\n",
    "        \"\"\"Initialise les réseaux de neurones simplifiés (weights-only).\"\"\"\n",
    "        # Modèle d'évaluation d'état : évalue l'optimalité d'une position donnée\n",
    "        # Architecture : entrée[position(2), vitesse(2), vent(2)] -> couches cachées -> valeur(1)\n",
    "        self.state_eval_model = {\n",
    "            'W1': self.np_random.normal(0, 0.1, (6, 10)),\n",
    "            'b1': np.zeros(10),\n",
    "            'W2': self.np_random.normal(0, 0.1, (10, 5)),\n",
    "            'b2': np.zeros(5),\n",
    "            'W3': self.np_random.normal(0, 0.1, (5, 1)),\n",
    "            'b3': np.zeros(1)\n",
    "        }\n",
    "        \n",
    "        # Modèle de prédiction de vent : prédit l'évolution du vent à partir des observations passées\n",
    "        # Architecture : entrée[séquence_vents(10*2)] -> couches cachées -> vent_futur(2)\n",
    "        self.wind_prediction_model = {\n",
    "            'W1': self.np_random.normal(0, 0.1, (20, 15)),\n",
    "            'b1': np.zeros(15),\n",
    "            'W2': self.np_random.normal(0, 0.1, (15, 10)),\n",
    "            'b2': np.zeros(10),\n",
    "            'W3': self.np_random.normal(0, 0.1, (10, 2)),\n",
    "            'b3': np.zeros(2)\n",
    "        }\n",
    "        \n",
    "        # Dans un cas réel, ces poids seraient entraînés via backpropagation\n",
    "        # Ici, nous initialisons avec des valeurs qui produisent des comportements raisonnables\n",
    "        # en fonction de la physique de la voile et des conditions de vent\n",
    "        self._adjust_weights_for_sailing_physics()\n",
    "\n",
    "    def _adjust_weights_for_sailing_physics(self):\n",
    "        \"\"\"Ajuste les poids des réseaux pour intégrer les connaissances de la physique de la voile.\"\"\"\n",
    "        # Exemples d'ajustements pour le modèle d'évaluation d'état\n",
    "        # Ces ajustements sont basés sur des heuristiques connues de la navigation à voile\n",
    "        \n",
    "        # Favoriser les positions perpendiculaires au vent (beam reach)\n",
    "        self.state_eval_model['W1'][2:4, :] *= 1.5  # Augmenter l'influence du vent\n",
    "        \n",
    "        # Pénaliser l'évaluation des états avec vitesse faible\n",
    "        self.state_eval_model['W2'][:, 2] = 0.5  # Poids plus importants pour la vitesse\n",
    "        \n",
    "        # Ajustements pour le modèle de prédiction de vent\n",
    "        # Intégrer un biais pour les tendances typiques du vent (rotations lentes)\n",
    "        self.wind_prediction_model['W2'][5:10, 3:7] *= 1.2  # Renforcer certains patterns de changement de vent\n",
    "        \n",
    "    def nn_forward(self, x, model):\n",
    "        \"\"\"Propagation avant simplifiée pour un réseau de neurones à 3 couches.\"\"\"\n",
    "        h1 = np.tanh(np.dot(x, model['W1']) + model['b1'])\n",
    "        h2 = np.tanh(np.dot(h1, model['W2']) + model['b2'])\n",
    "        out = np.dot(h2, model['W3']) + model['b3']\n",
    "        return out\n",
    "    \n",
    "    def evaluate_state(self, position, velocity, wind):\n",
    "        \"\"\"Évalue la qualité d'un état en utilisant le réseau de neurones.\"\"\"\n",
    "        # Normaliser les entrées\n",
    "        pos_norm = position / np.array(self.grid_size)\n",
    "        vel_norm = velocity / 2.0 if np.linalg.norm(velocity) > 0 else np.zeros(2)\n",
    "        wind_norm = wind / 5.0 if np.linalg.norm(wind) > 0 else np.zeros(2)\n",
    "        \n",
    "        # Créer le vecteur d'entrée\n",
    "        input_vec = np.concatenate([pos_norm, vel_norm, wind_norm])\n",
    "        \n",
    "        # Propager à travers le réseau\n",
    "        value = self.nn_forward(input_vec, self.state_eval_model)[0]\n",
    "        \n",
    "        # Bonus/malus basés sur la physique de la voile\n",
    "        # Proximité à l'objectif\n",
    "        dist_to_goal = np.linalg.norm(position - self.goal_position)\n",
    "        goal_factor = np.exp(-dist_to_goal / 20.0)  # Décroissance exponentielle\n",
    "        \n",
    "        # Efficacité de navigation basée sur l'angle au vent\n",
    "        if np.linalg.norm(wind) > 0.001 and np.linalg.norm(velocity) > 0.001:\n",
    "            wind_from = -wind / np.linalg.norm(wind)\n",
    "            boat_dir = velocity / np.linalg.norm(velocity)\n",
    "            angle = np.arccos(np.clip(np.dot(wind_from, boat_dir), -1.0, 1.0))\n",
    "            \n",
    "            # Pénaliser les directions dans la zone interdite\n",
    "            if angle < self.no_go_zone_angle:\n",
    "                value -= 2.0\n",
    "            # Favoriser le beam reach (perpendiculaire au vent)\n",
    "            elif abs(angle - self.optimal_beam_reach_angle) < 0.2:\n",
    "                value += 1.0\n",
    "        \n",
    "        # Intégrer le facteur de proximité au but\n",
    "        value += 2.0 * goal_factor\n",
    "        \n",
    "        return value\n",
    "    \n",
    "    def predict_wind_evolution(self, current_wind, position=None, wind_field=None):\n",
    "        \"\"\"Prédit l'évolution du vent pour les prochains pas de temps.\"\"\"\n",
    "        # Si nous avons une mémoire de vent, l'utiliser pour la prédiction\n",
    "        if len(self.wind_memory) > 5:\n",
    "            # Aplatir la mémoire du vent\n",
    "            wind_history = np.array(self.wind_memory).flatten()\n",
    "            \n",
    "            # Si la mémoire n'est pas complète, remplir avec la valeur actuelle\n",
    "            if len(wind_history) < 20:\n",
    "                padding = np.tile(current_wind, 10 - len(self.wind_memory))\n",
    "                wind_history = np.concatenate([padding, wind_history])\n",
    "            \n",
    "            # Normaliser l'entrée\n",
    "            wind_history_norm = wind_history / 5.0\n",
    "            \n",
    "            # Prédire avec le réseau de neurones\n",
    "            predicted_wind = self.nn_forward(wind_history_norm, self.wind_prediction_model)\n",
    "            \n",
    "            # Dénormaliser la sortie\n",
    "            predicted_wind = predicted_wind * 5.0\n",
    "            \n",
    "            return predicted_wind\n",
    "        \n",
    "        # Si pas assez d'historique, utiliser une heuristique basée sur le champ de vent actuel\n",
    "        return current_wind * 0.98 + self.np_random.normal(0, 0.05, 2)\n",
    "    \n",
    "    def calculate_sailing_efficiency(self, boat_direction, wind_direction):\n",
    "        \"\"\"\n",
    "        Calcule l'efficacité de navigation basée sur l'angle entre la direction du bateau et le vent.\n",
    "        \n",
    "        Args:\n",
    "            boat_direction: Vecteur normalisé de la direction souhaitée du bateau\n",
    "            wind_direction: Vecteur normalisé de la direction du vent (où le vent va VERS)\n",
    "            \n",
    "        Returns:\n",
    "            sailing_efficiency: Flottant entre 0.05 et 1.0 représentant l'efficacité de navigation\n",
    "        \"\"\"\n",
    "        # Inverser la direction du vent pour obtenir d'où vient le vent\n",
    "        wind_from = -wind_direction\n",
    "        \n",
    "        # Calculer l'angle entre le vent et la direction\n",
    "        cos_angle = np.dot(wind_from, boat_direction)\n",
    "        cos_angle = np.clip(cos_angle, -1.0, 1.0)  # Éviter les erreurs numériques\n",
    "        wind_angle = np.arccos(cos_angle)\n",
    "        \n",
    "        # Calcul de l'efficacité de navigation basé sur l'angle au vent\n",
    "        if wind_angle < self.no_go_zone_angle:  # Moins de 30 degrés par rapport au vent\n",
    "            sailing_efficiency = 0.05  # Efficacité faible mais non nulle dans la zone interdite\n",
    "        elif wind_angle < self.close_hauled_angle:  # Entre 30 et 45 degrés\n",
    "            sailing_efficiency = 0.1 + 0.4 * (wind_angle - self.no_go_zone_angle) / (self.close_hauled_angle - self.no_go_zone_angle)\n",
    "        elif wind_angle < self.optimal_beam_reach_angle:  # Entre 45 et 90 degrés\n",
    "            sailing_efficiency = 0.5 + 0.5 * (wind_angle - self.close_hauled_angle) / (self.optimal_beam_reach_angle - self.close_hauled_angle)\n",
    "        elif wind_angle < 3*np.pi/4:  # Entre 90 et 135 degrés\n",
    "            sailing_efficiency = 1.0  # Efficacité maximale\n",
    "        else:  # Plus de 135 degrés\n",
    "            sailing_efficiency = 1.0 - 0.2 * (wind_angle - 3*np.pi/4) / (np.pi/4)\n",
    "            sailing_efficiency = max(0.8, sailing_efficiency)  # Mais toujours bien\n",
    "        \n",
    "        return sailing_efficiency\n",
    "    \n",
    "    def action_to_direction(self, action):\n",
    "        \"\"\"Convertit l'indice d'action en vecteur de direction.\"\"\"\n",
    "        directions = [\n",
    "            (0, 1),     # 0: Nord\n",
    "            (1, 1),     # 1: Nord-Est\n",
    "            (1, 0),     # 2: Est\n",
    "            (1, -1),    # 3: Sud-Est\n",
    "            (0, -1),    # 4: Sud\n",
    "            (-1, -1),   # 5: Sud-Ouest\n",
    "            (-1, 0),    # 6: Ouest\n",
    "            (-1, 1),    # 7: Nord-Ouest\n",
    "            (0, 0)      # 8: Rester en place\n",
    "        ]\n",
    "        return np.array(directions[action])\n",
    "    \n",
    "    def direction_to_action(self, direction):\n",
    "        \"\"\"Convertit un vecteur de direction en indice d'action le plus proche.\"\"\"\n",
    "        directions = [\n",
    "            (0, 1),     # 0: Nord\n",
    "            (1, 1),     # 1: Nord-Est\n",
    "            (1, 0),     # 2: Est\n",
    "            (1, -1),    # 3: Sud-Est\n",
    "            (0, -1),    # 4: Sud\n",
    "            (-1, -1),   # 5: Sud-Ouest\n",
    "            (-1, 0),    # 6: Ouest\n",
    "            (-1, 1),    # 7: Nord-Ouest\n",
    "            (0, 0)      # 8: Rester en place\n",
    "        ]\n",
    "        \n",
    "        best_match = 8  # Par défaut: rester en place\n",
    "        best_similarity = -np.inf\n",
    "        \n",
    "        # Si la direction est très petite, rester en place\n",
    "        if np.linalg.norm(direction) < 0.01:\n",
    "            return 8\n",
    "        \n",
    "        # Normaliser la direction\n",
    "        direction = direction / np.linalg.norm(direction)\n",
    "        \n",
    "        # Trouver l'action la plus similaire à la direction\n",
    "        for i, dir_vec in enumerate(directions[:8]):  # Exclure l'action \"rester en place\"\n",
    "            dir_vec = np.array(dir_vec)\n",
    "            dir_norm = dir_vec / np.linalg.norm(dir_vec)\n",
    "            similarity = np.dot(direction, dir_norm)\n",
    "            \n",
    "            if similarity > best_similarity:\n",
    "                best_similarity = similarity\n",
    "                best_match = i\n",
    "                \n",
    "        return best_match\n",
    "    \n",
    "    def analyze_wind_field(self, wind_field_flat, position):\n",
    "        \"\"\"\n",
    "        Analyse sophistiquée du champ de vent pour identifier des structures et modèles.\n",
    "        Implémente un clustering et une détection de corridors de vent.\n",
    "        \"\"\"\n",
    "        # Reconstruire le champ de vent 2D à partir des données aplaties\n",
    "        wind_field = wind_field_flat.reshape(self.grid_size[1], self.grid_size[0], 2)\n",
    "        \n",
    "        # Calculer le vecteur vers l'objectif\n",
    "        goal_vector = self.goal_position - position\n",
    "        goal_distance = np.linalg.norm(goal_vector)\n",
    "        if goal_distance < 0.001:\n",
    "            return 8, None  # Déjà à l'objectif\n",
    "            \n",
    "        goal_direction = goal_vector / goal_distance\n",
    "        \n",
    "        # 1. Recherche de corridors de vent favorable\n",
    "        # Échantillonner des points le long de différents chemins potentiels\n",
    "        num_paths = 7  # Nombre de chemins à tester\n",
    "        path_angles = np.linspace(-np.pi/3, np.pi/3, num_paths)  # Angles relatifs à la direction de l'objectif\n",
    "        path_scores = np.zeros(num_paths)\n",
    "        path_efficiencies = []\n",
    "        \n",
    "        for p_idx, angle in enumerate(path_angles):\n",
    "            # Calculer la direction du chemin\n",
    "            path_direction = np.array([\n",
    "                goal_direction[0] * np.cos(angle) - goal_direction[1] * np.sin(angle),\n",
    "                goal_direction[0] * np.sin(angle) + goal_direction[1] * np.cos(angle)\n",
    "            ])\n",
    "            \n",
    "            # Échantillonner des points le long du chemin\n",
    "            path_positions = []\n",
    "            path_winds = []\n",
    "            \n",
    "            for step in range(1, 10):  # 10 points par chemin\n",
    "                # Position le long du chemin\n",
    "                step_pos = position + path_direction * step * 2  # Pas de 2 unités\n",
    "                x, y = int(np.clip(step_pos[0], 0, self.grid_size[0]-1)), int(np.clip(step_pos[1], 0, self.grid_size[1]-1))\n",
    "                \n",
    "                path_positions.append((x, y))\n",
    "                path_winds.append(wind_field[y, x])\n",
    "            \n",
    "            # Calculer l'efficacité moyenne le long du chemin\n",
    "            efficiency_scores = []\n",
    "            for wind_vec in path_winds:\n",
    "                if np.linalg.norm(wind_vec) > 0.001:\n",
    "                    wind_normalized = wind_vec / np.linalg.norm(wind_vec)\n",
    "                    efficiency = self.calculate_sailing_efficiency(path_direction, wind_normalized)\n",
    "                    efficiency_scores.append(efficiency)\n",
    "            \n",
    "            # Score du chemin = efficacité moyenne * proximité à l'objectif\n",
    "            if efficiency_scores:\n",
    "                mean_efficiency = np.mean(efficiency_scores)\n",
    "                # Facteur de direction vers l'objectif (plus élevé si le chemin mène vers l'objectif)\n",
    "                goal_alignment = np.dot(path_direction, goal_direction)\n",
    "                path_scores[p_idx] = mean_efficiency * (0.5 + 0.5 * goal_alignment)\n",
    "                path_efficiencies.append((path_direction, mean_efficiency, path_scores[p_idx]))\n",
    "        \n",
    "        # 2. Recherche de gradients et tendances dans le champ de vent\n",
    "        # Calculer les dérivées spatiales du champ de vent\n",
    "        grad_x = np.zeros_like(wind_field)\n",
    "        grad_y = np.zeros_like(wind_field)\n",
    "        \n",
    "        # Calcul simple des gradients (différences de premier ordre)\n",
    "        for i in range(1, wind_field.shape[0]-1):\n",
    "            for j in range(1, wind_field.shape[1]-1):\n",
    "                grad_x[i, j] = (wind_field[i, j+1] - wind_field[i, j-1]) / 2\n",
    "                grad_y[i, j] = (wind_field[i+1, j] - wind_field[i-1, j]) / 2\n",
    "        \n",
    "        # Calculer la divergence et le rotationnel pour identifier les structures de vent\n",
    "        divergence = np.zeros((wind_field.shape[0], wind_field.shape[1]))\n",
    "        curl = np.zeros((wind_field.shape[0], wind_field.shape[1]))\n",
    "        \n",
    "        for i in range(1, wind_field.shape[0]-1):\n",
    "            for j in range(1, wind_field.shape[1]-1):\n",
    "                divergence[i, j] = grad_x[i, j, 0] + grad_y[i, j, 1]\n",
    "                curl[i, j] = grad_x[i, j, 1] - grad_y[i, j, 0]\n",
    "        \n",
    "        # Identifier les zones de haute pression (divergence positive) et basse pression (divergence négative)\n",
    "        # Ces zones sont importantes en navigation car elles indiquent des changements potentiels de vent\n",
    "        x, y = int(position[0]), int(position[1])\n",
    "        local_divergence = 0\n",
    "        local_curl = 0\n",
    "        \n",
    "        if 0 <= x < divergence.shape[1] and 0 <= y < divergence.shape[0]:\n",
    "            local_divergence = divergence[y, x]\n",
    "            local_curl = curl[y, x]\n",
    "        \n",
    "        # 3. Synthèse de l'analyse et décision\n",
    "        # Trouver le meilleur chemin basé sur les scores calculés\n",
    "        best_path_idx = np.argmax(path_scores)\n",
    "        best_path_direction = np.array([\n",
    "            goal_direction[0] * np.cos(path_angles[best_path_idx]) - goal_direction[1] * np.sin(path_angles[best_path_idx]),\n",
    "            goal_direction[0] * np.sin(path_angles[best_path_idx]) + goal_direction[1] * np.cos(path_angles[best_path_idx])\n",
    "        ])\n",
    "        \n",
    "        # Convertir en action\n",
    "        best_action = self.direction_to_action(best_path_direction)\n",
    "        \n",
    "        # Si toutes les efficacités sont très basses, envisager une stratégie de louvoyage\n",
    "        if np.max(path_scores) < 0.3:\n",
    "            # Situation probable de vent contraire, utiliser le louvoyage\n",
    "            return self.get_upwind_tacking_action(position, wind_field[y, x]), path_efficiencies\n",
    "        \n",
    "        return best_action, path_efficiencies\n",
    "    \n",
    "    def get_upwind_tacking_action(self, position, wind_vector):\n",
    "        \"\"\"\n",
    "        Stratégie avancée de louvoyage pour naviguer contre le vent.\n",
    "        Alterne entre les directions à environ 45 degrés de part et d'autre du vent.\n",
    "        \"\"\"\n",
    "        # Normaliser le vecteur de vent\n",
    "        wind_magnitude = np.linalg.norm(wind_vector)\n",
    "        if wind_magnitude < 0.001:\n",
    "            return 0  # Par défaut, aller au nord si pas de vent\n",
    "            \n",
    "        wind_normalized = wind_vector / wind_magnitude\n",
    "        \n",
    "        # Calcul de l'angle du vent (d'où il vient)\n",
    "        wind_from_angle = np.arctan2(-wind_normalized[1], -wind_normalized[0])\n",
    "        \n",
    "        # Calculer les angles de louvoyage (environ 45 degrés de chaque côté du vent)\n",
    "        port_tack_angle = wind_from_angle - self.close_hauled_angle\n",
    "        starboard_tack_angle = wind_from_angle + self.close_hauled_angle\n",
    "        \n",
    "        # Décider de changer de bord si nécessaire\n",
    "        distance_to_center = abs(position[0] - self.grid_size[0]/2)\n",
    "        \n",
    "        # Changer de bord si on s'écarte trop du centre, ou si on est bloqué\n",
    "        if distance_to_center > self.grid_size[0]/4 or self.steps_without_progress > 5:\n",
    "            self.current_tack_direction *= -1\n",
    "            self.steps_without_progress = 0\n",
    "        \n",
    "        # Sélectionner l'angle en fonction du bord actuel\n",
    "        tack_angle = starboard_tack_angle if self.current_tack_direction > 0 else port_tack_angle\n",
    "        \n",
    "        # Convertir l'angle en vecteur de direction\n",
    "        tack_direction = np.array([np.cos(tack_angle), np.sin(tack_angle)])\n",
    "        \n",
    "        # Convertir la direction en action\n",
    "        return self.direction_to_action(tack_direction)\n",
    "    \n",
    "    def a_star_search(self, start_pos, goal_pos, wind_field):\n",
    "        \"\"\"\n",
    "        Algorithme A* adapté à la navigation à voile.\n",
    "        Tient compte de l'efficacité de navigation basée sur le vent.\n",
    "        \"\"\"\n",
    "        # Clé de cache pour cette recherche\n",
    "        cache_key = (tuple(start_pos), tuple(goal_pos))\n",
    "        \n",
    "        # Vérifier si ce chemin est déjà dans le cache\n",
    "        if cache_key in self.path_cache:\n",
    "            # Vérifier si le cache est encore valide (moins de X étapes)\n",
    "            if self.steps_since_path_recompute < self.path_recompute_threshold:\n",
    "                return self.path_cache[cache_key]\n",
    "        \n",
    "        # Fonctions d'aide pour A*\n",
    "        def heuristic(pos, goal):\n",
    "            return np.linalg.norm(np.array(pos) - np.array(goal))\n",
    "        \n",
    "        def get_neighbors(pos):\n",
    "            neighbors = []\n",
    "            for action in range(8):  # Toutes les directions sauf \"rester en place\"\n",
    "                direction = self.action_to_direction(action)\n",
    "                next_pos = (int(pos[0] + direction[0]), int(pos[1] + direction[1]))\n",
    "                \n",
    "                # Vérifier les limites\n",
    "                if 0 <= next_pos[0] < self.grid_size[0] and 0 <= next_pos[1] < self.grid_size[1]:\n",
    "                    neighbors.append((next_pos, action))\n",
    "            return neighbors\n",
    "        \n",
    "        def edge_cost(current, next_pos, action, wind_field):\n",
    "            # Direction du mouvement\n",
    "            direction = self.action_to_direction(action)\n",
    "            direction_norm = direction / np.linalg.norm(direction)\n",
    "            \n",
    "            # Vent à la position actuelle\n",
    "            current_wind = wind_field[int(current[1]), int(current[0])]\n",
    "            \n",
    "            # Échantillonner le vent à mi-chemin entre les positions pour plus de précision\n",
    "            mid_x = int((current[0] + next_pos[0]) / 2)\n",
    "            mid_y = int((current[1] + next_pos[1]) / 2)\n",
    "            mid_wind = wind_field[mid_y, mid_x]\n",
    "            \n",
    "            # Moyenne des vents\n",
    "            avg_wind = (current_wind + mid_wind) / 2\n",
    "            \n",
    "            if np.linalg.norm(avg_wind) < 0.001:\n",
    "                return 2.0  # Coût par défaut si pas de vent\n",
    "            \n",
    "            # Normaliser le vent\n",
    "            wind_norm = avg_wind / np.linalg.norm(avg_wind)\n",
    "            \n",
    "            # Calculer l'efficacité de navigation\n",
    "            efficiency = self.calculate_sailing_efficiency(direction_norm, wind_norm)\n",
    "            \n",
    "            # Le coût est inversement proportionnel à l'efficacité\n",
    "            # Plus l'efficacité est élevée, plus le coût est faible\n",
    "            cost = 1.0 / (efficiency + 0.1)\n",
    "            \n",
    "            # Pénalité pour la zone interdite (contre le vent)\n",
    "            wind_from = -wind_norm\n",
    "            angle = np.arccos(np.clip(np.dot(wind_from, direction_norm), -1.0, 1.0))\n",
    "            if angle < self.no_go_zone_angle:\n",
    "                cost *= 5.0  # Forte pénalité\n",
    "            \n",
    "            return cost\n",
    "        \n",
    "        # Initialisation de l'algorithme A*\n",
    "        start = tuple(map(int, start_pos))\n",
    "        goal = tuple(map(int, goal_pos))\n",
    "        \n",
    "        # Liste ouverte (nœuds à explorer) et liste fermée (nœuds déjà explorés)\n",
    "        open_set = []\n",
    "        closed_set = set()\n",
    "        \n",
    "        # Dictionnaires pour stocker les scores g et f, et les parents\n",
    "        g_score = defaultdict(lambda: float('inf'))\n",
    "        f_score = defaultdict(lambda: float('inf'))\n",
    "        parent = {}\n",
    "        actions = {}\n",
    "        \n",
    "        # Initialiser le nœud de départ\n",
    "        g_score[start] = 0\n",
    "        f_score[start] = heuristic(start, goal)\n",
    "        heapq.heappush(open_set, (f_score[start], start))\n",
    "        \n",
    "        # Boucle principale de A*\n",
    "        while open_set:\n",
    "            _, current = heapq.heappop(open_set)\n",
    "            \n",
    "            if current == goal:\n",
    "                # Reconstruire le chemin\n",
    "                path = []\n",
    "                action_path = []\n",
    "                while current in parent:\n",
    "                    path.append(current)\n",
    "                    action_path.append(actions[current])\n",
    "                    current = parent[current]\n",
    "                \n",
    "                path.reverse()\n",
    "                action_path.reverse()\n",
    "                \n",
    "                # Mettre en cache le chemin\n",
    "                self.path_cache[cache_key] = action_path\n",
    "                self.steps_since_path_recompute = 0\n",
    "                \n",
    "                return action_path\n",
    "            \n",
    "            closed_set.add(current)\n",
    "            \n",
    "            # Explorer les voisins\n",
    "            for neighbor, action in get_neighbors(current):\n",
    "                if neighbor in closed_set:\n",
    "                    continue\n",
    "                \n",
    "                # Calculer le nouveau score g\n",
    "                tentative_g = g_score[current] + edge_cost(current, neighbor, action, wind_field)\n",
    "                \n",
    "                if tentative_g < g_score[neighbor]:\n",
    "                    # Ce chemin vers le voisin est meilleur\n",
    "                    parent[neighbor] = current\n",
    "                    actions[neighbor] = action\n",
    "                    g_score[neighbor] = tentative_g\n",
    "                    f_score[neighbor] = g_score[neighbor] + heuristic(neighbor, goal)\n",
    "                    \n",
    "                    # Ajouter à la liste ouverte si pas déjà dedans\n",
    "                    if neighbor not in [i[1] for i in open_set]:\n",
    "                        heapq.heappush(open_set, (f_score[neighbor], neighbor))\n",
    "        \n",
    "        # Si aucun chemin n'est trouvé, retourner une action par défaut\n",
    "        # Essayer d'aller directement vers l'objectif\n",
    "        return [self.direction_to_action(goal_pos - start_pos)]\n",
    "    \n",
    "    def act(self, observation):\n",
    "        \"\"\"\n",
    "        Méthode principale de sélection d'action.\n",
    "        Combine A*, analyse de vent et évaluation par réseau de neurones.\n",
    "        \"\"\"\n",
    "        # Extraire les informations de l'observation\n",
    "        position = np.array([observation[0], observation[1]])\n",
    "        velocity = np.array([observation[2], observation[3]])\n",
    "        wind_at_position = np.array([observation[4], observation[5]])\n",
    "        wind_field_flat = observation[6:]\n",
    "        \n",
    "        # Stocker le vent actuel dans la mémoire\n",
    "        self.wind_memory.append(wind_at_position)\n",
    "        \n",
    "        # Reconstruire le champ de vent 2D\n",
    "        wind_field = wind_field_flat.reshape(self.grid_size[1], self.grid_size[0], 2)\n",
    "        \n",
    "        # Calculer le vecteur vers l'objectif\n",
    "        goal_vector = self.goal_position - position\n",
    "        current_distance_to_goal = np.linalg.norm(goal_vector)\n",
    "        \n",
    "        # Suivre les progrès\n",
    "        if self.previous_position is not None:\n",
    "            distance_improvement = self.last_distance_to_goal - current_distance_to_goal\n",
    "            if distance_improvement < 0.1:  # Si on ne progresse pas assez\n",
    "                self.steps_without_progress += 1\n",
    "            else:\n",
    "                self.steps_without_progress = 0\n",
    "        \n",
    "        self.previous_position = position.copy()\n",
    "        self.last_distance_to_goal = current_distance_to_goal\n",
    "        self.steps_since_path_recompute += 1\n",
    "\n",
    "        # Exploration aléatoire avec faible probabilité\n",
    "        if self.np_random.random() < self.exploration_rate:\n",
    "            return self.np_random.integers(0, 9)\n",
    "        \n",
    "        # Combiner les différentes approches pour sélectionner l'action\n",
    "        \n",
    "        # 1. Planification de trajectoire avec A*\n",
    "        if self.use_astar and (not self.current_path or self.steps_since_path_recompute >= self.path_recompute_threshold):\n",
    "            self.current_path = self.a_star_search(position, self.goal_position, wind_field)\n",
    "        \n",
    "        # 2. Analyse du champ de vent pour identifier les corridors favorables\n",
    "        wind_analysis_action, path_efficiencies = self.analyze_wind_field(wind_field_flat, position)\n",
    "        \n",
    "        # 3. Évaluation des états possibles avec le réseau de neurones\n",
    "        nn_evaluations = []\n",
    "        for action in range(8):  # Évaluer toutes les actions possibles\n",
    "            direction = self.action_to_direction(action)\n",
    "            next_pos = position + direction\n",
    "            \n",
    "            # Vérifier les limites\n",
    "            if (0 <= next_pos[0] < self.grid_size[0] and \n",
    "                0 <= next_pos[1] < self.grid_size[1]):\n",
    "                \n",
    "                # Prédire l'évolution du vent\n",
    "                future_wind = self.predict_wind_evolution(wind_at_position)\n",
    "                \n",
    "                # Estimer la nouvelle vitesse (simplifié)\n",
    "                boat_dir = direction / np.linalg.norm(direction)\n",
    "                wind_norm = wind_at_position / (np.linalg.norm(wind_at_position) + 1e-10)\n",
    "                sailing_efficiency = self.calculate_sailing_efficiency(boat_dir, wind_norm)\n",
    "                next_vel = boat_dir * sailing_efficiency * np.linalg.norm(wind_at_position) * 0.4  # 0.4 ~ boat_performance\n",
    "                \n",
    "                # Évaluer cet état potentiel avec le réseau de neurones\n",
    "                state_value = self.evaluate_state(next_pos, next_vel, future_wind)\n",
    "                \n",
    "                nn_evaluations.append((action, state_value))\n",
    "        \n",
    "        # Trier les évaluations par valeur décroissante\n",
    "        nn_evaluations.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Si nous avons un chemin A* valide, l'utiliser prioritairement sauf si bloqué\n",
    "        if self.current_path and self.steps_without_progress < 5:\n",
    "            if len(self.current_path) > 0:\n",
    "                astar_action = self.current_path[0]\n",
    "                self.current_path = self.current_path[1:] if len(self.current_path) > 1 else []\n",
    "                return astar_action\n",
    "        \n",
    "        # Si nous sommes bloqués ou n'avons pas de chemin A*, utiliser l'analyse de vent\n",
    "        if self.steps_without_progress >= 5 or not self.current_path:\n",
    "            # Si nous sommes vraiment bloqués, essayer de nous libérer en utilisant le louvoyage\n",
    "            if self.steps_without_progress >= 10:\n",
    "                # Alterner entre les bords pour essayer de sortir du blocage\n",
    "                self.current_tack_direction *= -1\n",
    "                return self.get_upwind_tacking_action(position, wind_at_position)\n",
    "            \n",
    "            # Utiliser l'analyse de vent pour naviguer\n",
    "            return wind_analysis_action\n",
    "        \n",
    "        # En dernier recours, utiliser l'évaluation du réseau de neurones\n",
    "        if nn_evaluations:\n",
    "            return nn_evaluations[0][0]\n",
    "        \n",
    "        # Si tout échoue, essayer d'aller directement vers l'objectif\n",
    "        goal_direction = goal_vector / (np.linalg.norm(goal_vector) + 1e-10)\n",
    "        return self.direction_to_action(goal_direction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running the naive agent on the simple_static initial windfield:\n",
      "Step 10: Position=[16  7], Reward=0.0\n",
      "Step 20: Position=[16 12], Reward=0.0\n",
      "Step 30: Position=[15 14], Reward=0.0\n",
      "Step 40: Position=[13 16], Reward=0.0\n",
      "Step 50: Position=[12 18], Reward=0.0\n",
      "Step 60: Position=[11 19], Reward=0.0\n",
      "Step 70: Position=[12 21], Reward=0.0\n",
      "Step 80: Position=[15 21], Reward=0.0\n",
      "Step 90: Position=[15 23], Reward=0.0\n",
      "Step 100: Position=[15 25], Reward=0.0\n",
      "Step 110: Position=[14 27], Reward=0.0\n",
      "\n",
      "Episode finished after 117 steps with reward: 100.0\n",
      "Final position: [16 30]\n",
      "Goal reached: True\n"
     ]
    }
   ],
   "source": [
    "from src.env_sailing import SailingEnv\n",
    "from src.initial_windfields import get_initial_windfield\n",
    "\n",
    "# Create an environment with a simple test initial windfield\n",
    "env = SailingEnv(**get_initial_windfield('simple_static'))\n",
    "naive_agent = WindAwareNavigator()\n",
    "\n",
    "# Run a single episode\n",
    "observation, info = env.reset(seed=42)\n",
    "total_reward = 0\n",
    "done = False\n",
    "truncated = False\n",
    "step_count = 0\n",
    "\n",
    "print(\"Running the naive agent on the simple_static initial windfield:\")\n",
    "while not (done or truncated) and step_count < 1000:  # Limit to 100 steps\n",
    "    action = naive_agent.act(observation)\n",
    "    observation, reward, done, truncated, info = env.step(action)\n",
    "    total_reward += reward\n",
    "    step_count += 1\n",
    "    \n",
    "    # Print every 10 steps to avoid too much output\n",
    "    if step_count % 10 == 0:\n",
    "        print(f\"Step {step_count}: Position={info['position']}, Reward={reward}\")\n",
    "\n",
    "print(f\"\\nEpisode finished after {step_count} steps with reward: {total_reward}\")\n",
    "print(f\"Final position: {info['position']}\")\n",
    "print(f\"Goal reached: {done}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nouvelle cellule: Implémentation de l'entraîneur Policy Gradient pour le WindAwareNavigator\n",
    "\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "class PolicyGradientTrainer:\n",
    "    \"\"\"\n",
    "    Entraîneur par Policy Gradient pour le WindAwareNavigator.\n",
    "    \n",
    "    Cette classe met en œuvre l'algorithme REINFORCE avec fonction de valeur de ligne de base\n",
    "    pour améliorer les poids des réseaux de neurones de l'agent.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                agent,\n",
    "                initial_windfields=['training_1', 'training_2', 'training_3'],\n",
    "                learning_rate=0.01,\n",
    "                gamma=0.99,\n",
    "                batch_size=10,\n",
    "                max_episodes=1000,\n",
    "                max_steps_per_episode=200,\n",
    "                model_save_path='models/wind_aware_navigator_trained.pkl'):\n",
    "        \"\"\"\n",
    "        Initialise l'entraîneur avec les paramètres spécifiés.\n",
    "        \n",
    "        Args:\n",
    "            agent: L'agent WindAwareNavigator à entraîner\n",
    "            initial_windfields: Liste des configurations de vent pour l'entraînement\n",
    "            learning_rate: Taux d'apprentissage pour la mise à jour des poids\n",
    "            gamma: Facteur d'actualisation pour le calcul des retours\n",
    "            batch_size: Nombre d'épisodes avant mise à jour des poids\n",
    "            max_episodes: Nombre maximum d'épisodes d'entraînement\n",
    "            max_steps_per_episode: Nombre maximum d'étapes par épisode\n",
    "            model_save_path: Chemin où sauvegarder l'agent entraîné\n",
    "        \"\"\"\n",
    "        self.agent = agent\n",
    "        self.initial_windfields = initial_windfields\n",
    "        self.learning_rate = learning_rate\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.max_episodes = max_episodes\n",
    "        self.max_steps_per_episode = max_steps_per_episode\n",
    "        self.model_save_path = model_save_path\n",
    "        \n",
    "        # Variables de suivi de l'entraînement\n",
    "        self.episode_rewards = []\n",
    "        self.episode_lengths = []\n",
    "        self.success_rates = []\n",
    "        self.evaluation_scores = []\n",
    "        \n",
    "        # Buffer pour stocker les expériences\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "        self.next_states = []\n",
    "        \n",
    "        # Créer le répertoire pour sauvegarder les modèles si nécessaire\n",
    "        os.makedirs(os.path.dirname(model_save_path), exist_ok=True)\n",
    "        \n",
    "    def create_environment(self, initial_windfield_name):\n",
    "        \"\"\"Crée un environnement avec la configuration de vent spécifiée.\"\"\"\n",
    "        from src.initial_windfields import get_initial_windfield\n",
    "        from src.env_sailing import SailingEnv\n",
    "        \n",
    "        initial_windfield = get_initial_windfield(initial_windfield_name)\n",
    "        env = SailingEnv(\n",
    "            wind_init_params=initial_windfield['wind_init_params'],\n",
    "            wind_evol_params=initial_windfield['wind_evol_params'],\n",
    "            render_mode=None  # Désactiver le rendu pour accélérer l'entraînement\n",
    "        )\n",
    "        return env\n",
    "    \n",
    "    def preprocess_state(self, observation):\n",
    "        \"\"\"Prétraite l'observation pour l'utiliser avec les réseaux de neurones.\"\"\"\n",
    "        position = np.array([observation[0], observation[1]])\n",
    "        velocity = np.array([observation[2], observation[3]])\n",
    "        wind = np.array([observation[4], observation[5]])\n",
    "        \n",
    "        # Normaliser les entrées\n",
    "        pos_norm = position / np.array(self.agent.grid_size)\n",
    "        vel_norm = velocity / 2.0 if np.linalg.norm(velocity) > 0 else np.zeros(2)\n",
    "        wind_norm = wind / 5.0 if np.linalg.norm(wind) > 0 else np.zeros(2)\n",
    "        \n",
    "        # Concaténer les caractéristiques normalisées\n",
    "        return np.concatenate([pos_norm, vel_norm, wind_norm])\n",
    "    \n",
    "    def collect_experience(self, num_episodes):\n",
    "        \"\"\"\n",
    "        Collecte des expériences en exécutant l'agent dans l'environnement.\n",
    "        \n",
    "        Args:\n",
    "            num_episodes: Nombre d'épisodes à exécuter\n",
    "            \n",
    "        Returns:\n",
    "            success_rate: Taux de réussite sur les épisodes\n",
    "        \"\"\"\n",
    "        # Réinitialiser les buffers\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "        self.next_states = []\n",
    "        \n",
    "        episode_rewards_buffer = []\n",
    "        episode_lengths_buffer = []\n",
    "        successes = 0\n",
    "        \n",
    "        for _ in range(num_episodes):\n",
    "            # Choisir aléatoirement une configuration de vent\n",
    "            initial_windfield_name = np.random.choice(self.initial_windfields)\n",
    "            env = self.create_environment(initial_windfield_name)\n",
    "            \n",
    "            # Réinitialiser l'environnement et l'agent\n",
    "            observation, _ = env.reset(seed=np.random.randint(0, 1000))  # Graine aléatoire\n",
    "            self.agent.reset()\n",
    "            \n",
    "            episode_reward = 0\n",
    "            episode_states = []\n",
    "            episode_actions = []\n",
    "            episode_rewards = []\n",
    "            episode_dones = []\n",
    "            episode_next_states = []\n",
    "            \n",
    "            # Exécuter un épisode\n",
    "            for step in range(self.max_steps_per_episode):\n",
    "                # Obtenir l'action de l'agent\n",
    "                action = self.agent.act(observation)\n",
    "                \n",
    "                # Exécuter l'action\n",
    "                next_observation, reward, done, truncated, _ = env.step(action)\n",
    "                \n",
    "                # Stocker l'expérience\n",
    "                state = self.preprocess_state(observation)\n",
    "                next_state = self.preprocess_state(next_observation)\n",
    "                \n",
    "                episode_states.append(state)\n",
    "                episode_actions.append(action)\n",
    "                episode_rewards.append(reward)\n",
    "                episode_dones.append(done or truncated)\n",
    "                episode_next_states.append(next_state)\n",
    "                \n",
    "                episode_reward += reward\n",
    "                observation = next_observation\n",
    "                \n",
    "                if done or truncated:\n",
    "                    break\n",
    "            \n",
    "            # Ajouter les données de l'épisode aux buffers\n",
    "            self.states.extend(episode_states)\n",
    "            self.actions.extend(episode_actions)\n",
    "            self.rewards.extend(episode_rewards)\n",
    "            self.dones.extend(episode_dones)\n",
    "            self.next_states.extend(episode_next_states)\n",
    "            \n",
    "            # Mettre à jour les métriques\n",
    "            episode_rewards_buffer.append(episode_reward)\n",
    "            episode_lengths_buffer.append(step + 1)\n",
    "            if episode_reward > 0:  # Si une récompense positive, considérer comme succès\n",
    "                successes += 1\n",
    "        \n",
    "        # Calculer les statistiques\n",
    "        self.episode_rewards.append(np.mean(episode_rewards_buffer))\n",
    "        self.episode_lengths.append(np.mean(episode_lengths_buffer))\n",
    "        success_rate = successes / num_episodes\n",
    "        self.success_rates.append(success_rate)\n",
    "        \n",
    "        return success_rate\n",
    "    \n",
    "    def compute_returns(self):\n",
    "        \"\"\"\n",
    "        Calcule les retours (discounted returns) pour chaque état.\n",
    "        \n",
    "        Returns:\n",
    "            returns: Liste des retours pour chaque état\n",
    "        \"\"\"\n",
    "        # Identifier les limites des épisodes\n",
    "        episode_ends = [i for i, done in enumerate(self.dones) if done]\n",
    "        if not episode_ends or episode_ends[-1] != len(self.dones) - 1:\n",
    "            episode_ends.append(len(self.dones) - 1)\n",
    "        \n",
    "        # Calculer les retours pour chaque épisode\n",
    "        returns = np.zeros(len(self.rewards))\n",
    "        start_idx = 0\n",
    "        \n",
    "        for end_idx in episode_ends:\n",
    "            # Calculer les retours pour cet épisode\n",
    "            episode_returns = np.zeros(end_idx - start_idx + 1)\n",
    "            discounted_sum = 0\n",
    "            \n",
    "            for i in range(end_idx, start_idx - 1, -1):\n",
    "                discounted_sum = self.rewards[i] + self.gamma * discounted_sum\n",
    "                episode_returns[i - start_idx] = discounted_sum\n",
    "            \n",
    "            # Normaliser les retours pour stabiliser l'entraînement\n",
    "            if len(episode_returns) > 1:\n",
    "                episode_returns = (episode_returns - np.mean(episode_returns)) / (np.std(episode_returns) + 1e-10)\n",
    "            \n",
    "            # Stocker les retours\n",
    "            returns[start_idx:end_idx + 1] = episode_returns\n",
    "            start_idx = end_idx + 1\n",
    "        \n",
    "        return returns\n",
    "    \n",
    "    def update_policy(self, returns):\n",
    "        \"\"\"\n",
    "        Met à jour les poids des réseaux de neurones en utilisant les retours calculés.\n",
    "        \n",
    "        Args:\n",
    "            returns: Liste des retours pour chaque état\n",
    "        \"\"\"\n",
    "        # Convertir les listes en tableaux numpy\n",
    "        states = np.array(self.states)\n",
    "        actions = np.array(self.actions)\n",
    "        returns = np.array(returns)\n",
    "        \n",
    "        # Mettre à jour le modèle d'évaluation d'état\n",
    "        # Pour simplifier, nous ajustons directement les poids en fonction des retours\n",
    "        for i, (state, action, ret) in enumerate(zip(states, actions, returns)):\n",
    "            # Calculer la sortie actuelle\n",
    "            value = self.agent.nn_forward(state, self.agent.state_eval_model)[0]\n",
    "            \n",
    "            # Calculer l'erreur\n",
    "            error = ret - value\n",
    "            \n",
    "            # Calcul du gradient (simplifié)\n",
    "            # En pratique, cela devrait être fait avec une rétropropagation complète\n",
    "            # Ici, nous ajustons simplement les poids en fonction de l'erreur\n",
    "            \n",
    "            # Mise à jour de la couche de sortie\n",
    "            hidden_activation = np.tanh(np.dot(np.tanh(np.dot(state, self.agent.state_eval_model['W1']) + \n",
    "                                        self.agent.state_eval_model['b1']),\n",
    "                               self.agent.state_eval_model['W2']) + \n",
    "                          self.agent.state_eval_model['b2'])\n",
    "            \n",
    "            delta_W3 = self.learning_rate * error * np.outer(hidden_activation, np.ones(1))\n",
    "            self.agent.state_eval_model['W3'] += delta_W3\n",
    "            self.agent.state_eval_model['b3'] += self.learning_rate * error\n",
    "            \n",
    "            # Pour une convergence plus rapide, mettons également à jour la deuxième couche\n",
    "            # mais avec un taux d'apprentissage plus faible\n",
    "            hidden_input = np.tanh(np.dot(state, self.agent.state_eval_model['W1']) + \n",
    "                                   self.agent.state_eval_model['b1'])\n",
    "            \n",
    "            delta_W2 = (self.learning_rate * 0.1) * error * np.outer(\n",
    "                hidden_input,\n",
    "                self.agent.state_eval_model['W3'].T * (1 - hidden_activation**2)\n",
    "            )\n",
    "            self.agent.state_eval_model['W2'] += delta_W2\n",
    "            \n",
    "            # Mise à jour des biais de la deuxième couche\n",
    "            delta_b2 = (self.learning_rate * 0.1) * error * (\n",
    "                self.agent.state_eval_model['W3'].T * (1 - hidden_activation**2)\n",
    "            ).flatten()\n",
    "            self.agent.state_eval_model['b2'] += delta_b2\n",
    "    \n",
    "    def evaluate_agent(self):\n",
    "        \"\"\"\n",
    "        Évalue les performances de l'agent sur les configurations d'entraînement.\n",
    "        \n",
    "        Returns:\n",
    "            avg_success_rate: Taux de réussite moyen\n",
    "        \"\"\"\n",
    "        from src.evaluation import evaluate_agent\n",
    "        from src.initial_windfields import get_initial_windfield\n",
    "        \n",
    "        success_rates = []\n",
    "        \n",
    "        for initial_windfield_name in self.initial_windfields:\n",
    "            # Évaluer sur plusieurs graines\n",
    "            seeds = list(range(10))  # 10 graines différentes\n",
    "            \n",
    "            # Créer l'environnement\n",
    "            initial_windfield = get_initial_windfield(initial_windfield_name)\n",
    "            \n",
    "            # Évaluer l'agent\n",
    "            results = evaluate_agent(\n",
    "                agent=self.agent,\n",
    "                initial_windfield=initial_windfield,\n",
    "                seeds=seeds,\n",
    "                max_horizon=self.max_steps_per_episode,\n",
    "                verbose=False,\n",
    "                render=False,\n",
    "                full_trajectory=False\n",
    "            )\n",
    "            \n",
    "            success_rates.append(results['success_rate'])\n",
    "        \n",
    "        avg_success_rate = np.mean(success_rates)\n",
    "        self.evaluation_scores.append(avg_success_rate)\n",
    "        \n",
    "        return avg_success_rate\n",
    "    \n",
    "    def save_agent(self):\n",
    "        \"\"\"Sauvegarde l'agent entraîné.\"\"\"\n",
    "        with open(self.model_save_path, 'wb') as f:\n",
    "            pickle.dump({\n",
    "                'state_eval_model': self.agent.state_eval_model,\n",
    "                'wind_prediction_model': self.agent.wind_prediction_model\n",
    "            }, f)\n",
    "        print(f\"Agent sauvegardé dans {self.model_save_path}\")\n",
    "    \n",
    "    def load_agent(self):\n",
    "        \"\"\"Charge un agent préalablement entraîné.\"\"\"\n",
    "        try:\n",
    "            with open(self.model_save_path, 'rb') as f:\n",
    "                models = pickle.load(f)\n",
    "                self.agent.state_eval_model = models['state_eval_model']\n",
    "                self.agent.wind_prediction_model = models['wind_prediction_model']\n",
    "            print(f\"Agent chargé depuis {self.model_save_path}\")\n",
    "            return True\n",
    "        except:\n",
    "            print(f\"Impossible de charger l'agent depuis {self.model_save_path}\")\n",
    "            return False\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Exécute le processus complet d'entraînement.\n",
    "        \n",
    "        Returns:\n",
    "            agent: L'agent entraîné\n",
    "        \"\"\"\n",
    "        print(\"Démarrage de l'entraînement du WindAwareNavigator...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Évaluer l'agent avant l'entraînement\n",
    "        print(\"Évaluation initiale de l'agent...\")\n",
    "        initial_success_rate = self.evaluate_agent()\n",
    "        print(f\"Taux de succès initial: {initial_success_rate:.2%}\")\n",
    "        \n",
    "        # Boucle principale d'entraînement\n",
    "        best_success_rate = initial_success_rate\n",
    "        episodes_without_improvement = 0\n",
    "        \n",
    "        for episode in range(0, self.max_episodes, self.batch_size):\n",
    "            # Afficher la progression\n",
    "            print(f\"\\nÉpisodes {episode}-{episode + self.batch_size - 1}\")\n",
    "            \n",
    "            # Collecter des expériences\n",
    "            print(\"Collecte d'expériences...\")\n",
    "            success_rate = self.collect_experience(self.batch_size)\n",
    "            print(f\"Taux de succès du batch: {success_rate:.2%}\")\n",
    "            \n",
    "            # Calculer les retours\n",
    "            returns = self.compute_returns()\n",
    "            \n",
    "            # Mettre à jour la politique\n",
    "            print(\"Mise à jour de la politique...\")\n",
    "            self.update_policy(returns)\n",
    "            \n",
    "            # Évaluer périodiquement l'agent (tous les 5 batches)\n",
    "            if (episode // self.batch_size) % 5 == 0:\n",
    "                print(\"Évaluation de l'agent...\")\n",
    "                eval_success_rate = self.evaluate_agent()\n",
    "                print(f\"Taux de succès d'évaluation: {eval_success_rate:.2%}\")\n",
    "                \n",
    "                # Sauvegarder le meilleur modèle\n",
    "                if eval_success_rate > best_success_rate:\n",
    "                    best_success_rate = eval_success_rate\n",
    "                    self.save_agent()\n",
    "                    episodes_without_improvement = 0\n",
    "                    print(f\"Nouveau meilleur taux de succès: {best_success_rate:.2%}\")\n",
    "                else:\n",
    "                    episodes_without_improvement += self.batch_size\n",
    "            \n",
    "            # Arrêter si pas d'amélioration pendant longtemps\n",
    "            if episodes_without_improvement >= 50:\n",
    "                print(\"Pas d'amélioration depuis 50 épisodes. Arrêt de l'entraînement.\")\n",
    "                break\n",
    "            \n",
    "            # Afficher les statistiques\n",
    "            print(f\"Récompense moyenne: {self.episode_rewards[-1]:.2f}\")\n",
    "            print(f\"Longueur moyenne d'épisode: {self.episode_lengths[-1]:.1f}\")\n",
    "        \n",
    "        # Durée totale d'entraînement\n",
    "        training_time = time.time() - start_time\n",
    "        print(f\"\\nEntraînement terminé en {training_time:.2f} secondes!\")\n",
    "        print(f\"Meilleur taux de succès: {best_success_rate:.2%}\")\n",
    "        print(f\"Amélioration: {best_success_rate - initial_success_rate:.2%}\")\n",
    "        \n",
    "        # Charger le meilleur modèle\n",
    "        self.load_agent()\n",
    "        \n",
    "        return self.agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nouvelle cellule: Fonction pour visualiser les métriques d'entraînement\n",
    "def plot_training_metrics(trainer):\n",
    "    \"\"\"Affiche les métriques d'entraînement sous forme de graphiques.\"\"\"\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(10, 12))\n",
    "    \n",
    "    # Graphique des récompenses\n",
    "    ax1.plot(trainer.episode_rewards)\n",
    "    ax1.set_title('Récompense moyenne par batch')\n",
    "    ax1.set_xlabel('Batch')\n",
    "    ax1.set_ylabel('Récompense')\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # Graphique des longueurs d'épisodes\n",
    "    ax2.plot(trainer.episode_lengths)\n",
    "    ax2.set_title('Longueur moyenne des épisodes')\n",
    "    ax2.set_xlabel('Batch')\n",
    "    ax2.set_ylabel('Étapes')\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    # Graphique des taux de succès\n",
    "    ax3.plot(trainer.success_rates, label='Batch')\n",
    "    if trainer.evaluation_scores:\n",
    "        eval_x = np.linspace(0, len(trainer.success_rates)-1, len(trainer.evaluation_scores))\n",
    "        ax3.plot(eval_x, trainer.evaluation_scores, 'r-', label='Évaluation')\n",
    "    ax3.set_title('Taux de succès')\n",
    "    ax3.set_xlabel('Batch')\n",
    "    ax3.set_ylabel('Taux de succès')\n",
    "    ax3.set_ylim(0, 1.05)\n",
    "    ax3.legend()\n",
    "    ax3.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Évaluation de l'agent initial...\n",
      "Taux de succès initial: 100.00%\n",
      "Collecte d'expériences...\n",
      "Taux de succès du batch: 100.00%\n",
      "Mise à jour de la politique...\n",
      "Taux de succès après mise à jour: 100.00%\n",
      "Amélioration: 0.00%\n",
      "Agent sauvegardé dans models/wind_aware_navigator_trained.pkl\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1200 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Nouvelle cellule: Exécuter l'entraînement\n",
    "\n",
    "# Paramètres d'entraînement\n",
    "initial_windfields = ['training_1', 'training_2', 'training_3']\n",
    "learning_rate = 0.01\n",
    "gamma = 0.99\n",
    "batch_size = 10\n",
    "max_episodes = 1000  # Réduire pour un entraînement plus rapide\n",
    "model_path = \"models/wind_aware_navigator_trained.pkl\"\n",
    "\n",
    "# Créer le répertoire pour les modèles\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "# Créer l'entraîneur avec votre agent WindAwareNavigator existant\n",
    "trainer = PolicyGradientTrainer(\n",
    "    agent=WindAwareNavigator(),  # Utilise votre agent déjà implémenté\n",
    "    initial_windfields=initial_windfields,\n",
    "    learning_rate=learning_rate,\n",
    "    gamma=gamma,\n",
    "    batch_size=batch_size,\n",
    "    max_episodes=max_episodes,\n",
    "    model_save_path=model_path\n",
    ")\n",
    "\n",
    "# Évaluation initiale\n",
    "print(\"Évaluation de l'agent initial...\")\n",
    "initial_score = trainer.evaluate_agent()\n",
    "print(f\"Taux de succès initial: {initial_score:.2%}\")\n",
    "\n",
    "# Entraînement par étapes - ceci est plus interactif qu'un entraînement complet\n",
    "# Décommentez la ligne suivante pour un entraînement complet: \n",
    "# trained_agent = trainer.train()\n",
    "\n",
    "# Collecter des expériences\n",
    "print(\"Collecte d'expériences...\")\n",
    "success_rate = trainer.collect_experience(trainer.batch_size)\n",
    "print(f\"Taux de succès du batch: {success_rate:.2%}\")\n",
    "\n",
    "# Calculer les retours\n",
    "returns = trainer.compute_returns()\n",
    "\n",
    "# Mettre à jour la politique\n",
    "print(\"Mise à jour de la politique...\")\n",
    "trainer.update_policy(returns)\n",
    "\n",
    "# Évaluer à nouveau l'agent\n",
    "new_score = trainer.evaluate_agent()\n",
    "print(f\"Taux de succès après mise à jour: {new_score:.2%}\")\n",
    "print(f\"Amélioration: {new_score - initial_score:.2%}\")\n",
    "\n",
    "# Sauvegarder l'agent\n",
    "trainer.save_agent()\n",
    "\n",
    "# Afficher les métriques\n",
    "plot_training_metrics(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent chargé depuis models/wind_aware_navigator_trained.pkl\n",
      "Résultats sur training_1:\n",
      "Taux de succès: 100.00%\n",
      "Récompense moyenne: 59.26\n",
      "Nombre moyen d'étapes: 53.7\n",
      "Résultats sur training_2:\n",
      "Taux de succès: 100.00%\n",
      "Récompense moyenne: 76.25\n",
      "Nombre moyen d'étapes: 28.0\n",
      "Résultats sur training_3:\n",
      "Taux de succès: 100.00%\n",
      "Récompense moyenne: 50.62\n",
      "Nombre moyen d'étapes: 69.0\n"
     ]
    }
   ],
   "source": [
    "# Nouvelle cellule: Tester l'agent entraîné\n",
    "\n",
    "def test_agent(agent, initial_windfield_name='training_1', num_seeds=5, visualize=True):\n",
    "    \"\"\"\n",
    "    Teste l'agent sur une configuration de vent spécifique.\n",
    "    \n",
    "    Args:\n",
    "        agent: L'agent à tester\n",
    "        initial_windfield_name: Nom de la configuration de vent\n",
    "        num_seeds: Nombre de graines différentes à tester\n",
    "        visualize: Si True, affiche une visualisation de la première trajectoire\n",
    "        \n",
    "    Returns:\n",
    "        results: Dictionnaire avec les résultats d'évaluation\n",
    "    \"\"\"\n",
    "    from src.evaluation import evaluate_agent\n",
    "    from src.initial_windfields import get_initial_windfield\n",
    "    \n",
    "    # Créer l'environnement\n",
    "    initial_windfield = get_initial_windfield(initial_windfield_name)\n",
    "    \n",
    "    # Préparer l'environnement pour la visualisation si nécessaire\n",
    "    if visualize:\n",
    "        initial_windfield.update({\n",
    "            'env_params': {\n",
    "                'wind_grid_density': 25,\n",
    "                'wind_arrow_scale': 80,\n",
    "                'render_mode': \"rgb_array\"\n",
    "            }\n",
    "        })\n",
    "    \n",
    "    # Évaluer l'agent\n",
    "    results = evaluate_agent(\n",
    "        agent=agent,\n",
    "        initial_windfield=initial_windfield,\n",
    "        seeds=list(range(num_seeds)),\n",
    "        max_horizon=200,\n",
    "        verbose=False,\n",
    "        render=visualize,\n",
    "        full_trajectory=visualize\n",
    "    )\n",
    "    \n",
    "    # Afficher les résultats\n",
    "    print(f\"Résultats sur {initial_windfield_name}:\")\n",
    "    print(f\"Taux de succès: {results['success_rate']:.2%}\")\n",
    "    print(f\"Récompense moyenne: {results['mean_reward']:.2f}\")\n",
    "    print(f\"Nombre moyen d'étapes: {results['mean_steps']:.1f}\")\n",
    "    \n",
    "    # Visualiser la trajectoire si demandé\n",
    "    if visualize and results.get('frames'):\n",
    "        # Afficher le premier et le dernier frame\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
    "        ax1.imshow(results['frames'][0])\n",
    "        ax1.set_title('État initial')\n",
    "        ax1.axis('off')\n",
    "        ax2.imshow(results['frames'][-1])\n",
    "        ax2.set_title('État final')\n",
    "        ax2.axis('off')\n",
    "        plt.show()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Créer un agent et charger les poids entraînés\n",
    "agent = WindAwareNavigator()\n",
    "try:\n",
    "    with open(model_path, 'rb') as f:\n",
    "        models = pickle.load(f)\n",
    "        agent.state_eval_model = models['state_eval_model']\n",
    "        agent.wind_prediction_model = models['wind_prediction_model']\n",
    "    print(f\"Agent chargé depuis {model_path}\")\n",
    "except:\n",
    "    print(\"Utilisation de l'agent non entraîné\")\n",
    "\n",
    "# Tester l'agent sur chaque configuration de vent\n",
    "for initial_windfield_name in initial_windfields:\n",
    "    test_agent(agent, initial_windfield_name, num_seeds=3, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'W1': np.array([[0.019313364376407044, 0.07934115482951021, 0.017396672756038453, -0.10299534038410728, 0.05893156682027095, -0.0681874201828102, -0.14010442967953982, -0.11421291749560421, -0.09914004637651597, -0.035441075079443024], [-0.13237239038467008, 0.0598859977426523, 0.07538206218160674, -0.12002855333819118, 0.024079871823161726, -0.12112656817088392, 0.014243232929662572, 0.08566733733954174, 0.015753045165461936, 0.06374540965957134], [0.1687005167865639, 0.07212640821822593, -0.08974594453339996, -0.12039274585781268, 0.034650782550000536, -0.16453279286629507, 0.11882600324253059, -0.06294800504889238, -0.32184112864687053, -0.03269530192724545], [0.00906422636656765, -0.0899744828355049, -0.013391636323646481, 0.23445806930625426, 0.06891364430260084, 0.29055719860236423, 0.005583885399469348, 0.16840018954968594, -0.06860843952301734, -0.028529773878093783], [-0.0446081372592477, 0.02576882182157529, -0.009285763254401006, 0.21202599732017788, -0.03952236600414781, -0.015723572218613105, -0.0352392792274916, -0.03309145256900489, -0.18336888213799496, 0.07392900016221926], [-0.024398638448289676, 0.0650679299606748, -0.009608589517322852, -0.02671521628832947, -0.027172089441610616, 0.02972093962181583, -0.026912602262430932, 0.005410941514247472, -0.15831335359166712, -0.11647512702755523]]),\n",
      "'b1': np.array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]),\n",
      "'W2': np.array([[0.07608107647861835, 0.10116886234240699, 0.49916211105831565, -0.06316814040436076, -0.11952386586972166], [-0.08580757057443646, -0.057971444329004584, 0.5004641311681709, 0.10889816823497492, 0.03727372707505942], [-0.11036026604952068, 0.2503479257337791, 0.5004248432373212, 0.2489409388890358, 0.10038505191242397], [-0.01464062358327732, 0.030172547650851922, 0.4996414871537795, 0.1788375159898971, 0.10722810463669769], [0.08334569319617721, -0.22119232515129125, 0.500137327454649, -0.19434306077541091, 0.06907971838501914], [-0.06891081425259037, 0.03642191061595198, 0.4989691073288318, -0.030268108721859182, 0.13410350447447958], [0.09165054163671267, 0.09799787077387175, 0.5001452303717651, 0.15827424805672716, -0.09045341650585273], [-0.013710170888739017, 0.030282331414131395, 0.500451669277978, -0.13860927869751904, -0.06490195932831215], [-0.03700122974729947, -0.03462335304161716, 0.4995277759782853, 0.2012453364616848, -0.043705223316994925], [-0.06752538407027159, -0.14600534929793046, 0.5007279906359507, 0.1861692580707159, 0.15550105582252857]]),\n",
      "'b2': np.array([0.0008001937345360987, -0.002990070107138212, 0.0003943000560917304, -0.0035090001027599533, -0.0002666077288247823]),\n",
      "'W3': np.array([[0.04594707993416697], [-0.2078667466011548], [0.02588197088484641], [-0.2384677446394869], [-0.009446704810004437]]),\n",
      "'b3': np.array([0.14433060646853257]),\n",
      "'W1': np.array([[-0.15263704416981894, 0.009318413415495093, 0.01788566281886031, 0.04338565370468317, -0.039545128620585235, 0.012844375555425534, -0.10895468239396489, 0.13596216986746398, -0.10455694553988788, -0.10794579868285113, -0.14902587706816325, -0.2193704878493589, 0.14458408451820634, 0.12912849345236885, 0.2582094359017618], [-0.010937521260260798, 0.01948847963837895, 0.012236740041533956, -0.0098583755276049, -0.06548309050498918, 0.07366818354080922, -0.023303556643905484, 0.013108160788129548, -0.016831128096055567, 0.16745026512167135, 0.10813845645839522, 0.1612794203519638, -0.03629923246455525, 0.021702925310289098, -0.045629585704958994], [0.08027099941365216, -0.0861376888059602, 0.10674628197288423, 0.13125870284231722, 0.11005231780971021, 0.07082631801321539, 0.019423106652471656, 0.18065445264979363, 0.026826214640731302, 0.18587203620775308, -0.13459599699697775, 0.20156799226989075, -0.059877358963857374, -0.08749984038893582, -0.07191823822924663], [-0.18514455849336142, -0.03298684748677184, 0.09047188209201208, 0.06497041900980548, 0.02066487012735469, -0.16681115971564664, 0.03603960530014032, 0.006230576993330044, 0.07528945959200761, -0.17110184296794972, 0.2156627322430682, 0.11671082472166064, -0.07923067416426421, 0.040686416819966154, -0.0882662587622846], [0.05026250028427412, 0.09792234586146797, -0.08684463119790015, -0.13306772871559866, -0.0856957423609675, 0.011534431898405223, 0.00864803989780255, -0.10887107146928146, 0.17063189738826082, 0.047263137704048885, -0.030654454268150395, -0.23094435485419834, 0.053538856609890786, 0.01581333299942396, 0.07778169699657772], [0.00620075706876153, 0.1287895371918549, 0.08357749890571042, -0.051522946844333874, -0.10937336784080559, 0.04171818508772923, 0.0019202855761981516, 0.1714695387088082, -0.1156369098138971, -0.03256534908656869, 0.08518991858298743, 0.023370541665146086, -0.03539896762840767, -0.2224084461319918, -0.0006500880817091808], [0.2568922201653999, -0.10981198832284494, -0.0585260098020611, -0.052832303713701026, -0.033929102951898236, -0.13671856118511103, -0.002456008249260377, 0.03283829368844886, -0.10082350026607872, -0.021813792033086088, -0.031027155016415398, 0.13407763999265332, -0.050021767864129096, 0.16731354236182355, 0.08458630865191391], [-0.16019714261321216, -0.04630709159741453, -0.034663530350262844, 0.0445341479778971, 0.030165453298138186, 0.04678956746369994, 0.02235682486759332, -0.023533552449432445, 0.022230572226972268, -0.12659083763220286, -0.003653723460494027, -0.09663237648205758, -0.1971226946400165, -0.11266691656908717, 0.0756004139381154], [-0.025981492327851352, 0.02672060059815727, -0.06503930353247526, -0.08787743935397438, -0.0520555455933692, 0.013724166031046068, -0.06095623903089567, 0.06082815611119885, 0.07133068428526922, 0.02656299961568133, 0.01211046176940125, -0.10909017575390717, -0.1903449676205773, 0.03774255444450102, 0.16532189005050008], [-0.07124033878071201, 0.025900156856566714, 0.03703070586203098, -0.153868693131401, -0.030246786444185644, -0.24400846312325483, 0.017343623171547442, -0.08357325115404574, -0.0914704206133485, 0.03955272324991493, -0.10515288616834338, -0.20144760916722015, -0.03873687735976469, 0.20895181304717309, 0.10043309458562025], [-0.021023369161981584, -0.018885551997644412, 0.1740178868643283, -0.1335038007841269, -0.09401809290787654, -0.05950537258194902, 0.07527109606610849, 0.0009177507938848133, 0.04343486396260307, 0.038877270243503806, 0.08189892577909899, -0.15304924472707282, 0.13803813453478667, 0.04483066446464684, 0.03588157637929124], [-0.0745761776922403, 0.0876607431021326, -0.08234135574935718, 0.012593576401686622, 0.05602908085220113, 0.1300382434372369, -0.1878991758244089, 0.11627026883473371, 0.05874130515925378, -0.007130728849099184, -0.026707982064129415, 0.17379436815373275, 0.2592186992665671, 0.03289215188388898, 0.16535572513885258], [-0.0033663774153866533, 0.13751822185238743, 0.039149839016744896, 0.011297538069006578, 0.06212231617216962, 0.06129921180311441, -0.08115402451058715, -0.12844942454194955, -0.3535134915544022, 0.09438708298702576, -0.026263883881117545, 0.049099344117154727, 0.004177955904516242, 0.0628923879171176, -0.08488966070283849], [-0.052308172711140716, -0.062451011143833494, -0.0741511910139186, -0.1323806292931067, -0.07581781242343545, 0.1213774631404999, 0.07134126553170672, 0.0770623154464379, 0.0713083930758758, 0.027891515047747163, -0.11531649964983762, -0.015214636455490711, -0.022346495586645643, -0.1395072021332262, -0.007623056476835629], [-0.17459279840640185, 0.09460919725718213, -0.014386907974766739, -0.05050236115390924, 0.05236879235569032, 0.11645161687477945, -0.02790527309319768, 0.042879972970464926, -0.055101237702648034, 0.029024697323369587, 0.018449394799917324, -0.07360066856888114, -0.018424019276114516, 0.02031133319236647, 0.03040051192558408], [0.15285114118349513, -0.13276338385001127, -0.19994548978211393, -0.04511125555140754, -0.04604699050383614, 0.05201331885413668, -0.08267184236197889, -0.1472863152089607, 0.07130968127080826, -0.04796656260846005, -0.11674268175244742, 0.15146940256148134, 0.15395520632703008, 0.13688140200542412, -0.045260210347369795], [-0.08999678563068661, -0.14626517209982398, -0.14291092739771852, 0.00018416041168860744, -0.09925037498565953, -0.10830244801776041, 0.12347725769804599, -0.2032514824103879, -0.10730261125867627, -0.06642157100792875, 0.08986592351851998, -0.03996905930523789, 0.02156273670681741, -0.050219142878923276, -0.2291694968191243], [-0.13294798348692857, 0.07375357941156133, 0.18746037135208027, -0.04253129795256756, 0.03745569385243895, 0.08278179423342258, -0.010954734558052126, -0.0008825415079235311, -0.14301968188769365, -0.041023740681468975, 0.02282866912061501, 0.1970942123069085, 0.05783279579123242, 0.013952309448433613, 0.19404423706775617], [-0.12150463772394929, -0.04038841654511722, -0.11100463573435503, 0.0679588316575573, 0.07552611837380252, -0.08454345488909143, 0.03810686156013305, 0.1478532132085191, -0.0630239638537125, 0.01648529739322663, -0.04812505893047183, 0.18274260597905512, 0.12045132893004887, 0.02108479517997086, 0.0234223972410669], [0.18593926196963878, -0.08654074297911356, 0.01826571878445347, 0.022235389374056957, -0.19573114388094065, 0.023548634996002186, -0.09328523818933322, 0.015835782749868462, -0.047133419776735797, -0.0693955358414133, -0.07416904926086501, -0.03674555416045543, -0.01777780530465124, -0.018024648918754195, 0.07458698254721129]]),\n",
      "'b1': np.array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]),\n",
      "'W2': np.array([[0.09488163823369669, 0.1350431234200042, -0.006498574130106842, -0.04301536562886879, -0.13794694581973566, 0.06939569634468559, 0.015912908157313808, 0.09544373901428876, 0.06776052412064339, -0.06486977656189634], [-0.21412606250215227, -0.07164332553591277, -0.06559203302341889, 0.09787053712538717, 0.07212280426184124, -0.1227026709381398, -0.01448571744041631, 0.13579182098403048, -0.11384269720374314, -0.11602305855311781], [0.058368242698982, -0.03253028254661827, -0.06692746235329984, -0.014715448484939556, -0.1413542111871304, 0.060734157587625716, -0.017398288964794315, 0.10789157677069415, -0.11113754746419076, 0.06018300441187666], [0.23336841239412218, -0.17687816416094265, -0.016795633897954642, -0.01998301864808269, 0.0031659919252541614, 0.08513533939257092, 0.11778121953160714, -0.14893597058060046, -0.082965169028802, 0.05772648686299971], [0.1216163666934978, 0.04815685828651039, 0.0657001217810824, -0.20519692904883446, -0.11601725228885931, 0.08194448253307862, -0.14390281114456163, -0.11716879608127358, 0.1950001926650249, -0.18545105673821866], [0.16654498507360566, 0.029316188390589488, 0.08158297843069862, -0.21371350663752742, -0.22703930311618684, 0.20722595775929267, 0.08507896099581029, -0.030671669435360385, 0.022553232710240498, 0.06232846197361633], [-0.026923206059318746, -0.028480733467960446, 0.09394879586025123, 0.12421902533830648, 0.036936412989803886, -0.006001346287808881, -0.14588488346674383, -0.051613599260382406, -0.12101578255028358, 0.11489998448045811], [-0.0375636013465569, 0.015979130273596157, -0.03101885375628278, -0.007957008757648652, -0.05023379327325048, -0.16874127762780886, -0.13688102880735278, 0.126159507826171, 0.2144273392019338, 0.011762048415610805], [0.0818942010140187, -0.16619072250331368, 0.12727899261438622, -0.0768511098476028, 0.3098139697370428, 0.030616619006504715, -0.2060782814756862, 0.08752422321964665, -0.05129537622992697, 0.055642483297362016], [-0.0032419540492792244, -0.041424938565006786, 0.03548062673084101, -0.05518569400180684, 0.08501199619975512, 0.06133331433161999, -0.07888488079341222, -0.040076003569690447, 0.016328905363887883, -0.11624031540771192], [0.024422452659546543, -0.09571028500840213, -0.047119930004666216, -0.022725226142427852, 0.030950694347471143, 0.10965672316042, 0.015631518029143702, -0.02936803886923798, 0.22829758754397675, 0.03979620945333599], [-0.08111018843935501, 0.03200561510581483, -0.0352903726372589, -0.008377585520605298, 0.0019910949297185154, 0.036680038090906225, -0.21843960965350995, 0.08375046383615165, 0.12743079049572478, 0.07179467382111249], [0.07349280509617467, -0.03868324757139568, -0.02542653912119079, 0.056167481425987535, -0.00400476832915929, 0.17166665495725378, -0.08506554906706684, 0.04834016601424887, -0.0976537036211525, -0.10574830603543016], [-0.048885269708226875, 0.0508929105919228, 0.05000474287109274, 0.03738243290258885, -0.08504523244959604, 0.021412192629632357, 0.07462634608393608, 0.047813665134880196, -0.20236592541579634, 0.056415569695042536], [0.0952440550023677, 0.05678911760020611, 0.10848012636586837, 0.044826984197053506, -0.12576590078315522, 0.01678165880467072, -0.11477529028487408, -0.0926301807660702, 0.05126715214407518, -0.15987660688285626]]),\n",
      "'b2': np.array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]),\n",
      "'W3': np.array([[0.03808084709560377, 0.01546473431727345], [0.017696078758047844, 0.051592702369045874], [0.03793890976564407, 0.11417316684529198], [-0.12497712316642028, 0.018016633418852377], [0.030694148414271934, -0.1386784212318724], [-0.07191250744722598, -0.08981825052550915], [-0.0018800237401687223, 0.10681745133824108], [0.01677524012640202, 0.07147231747341624], [0.125648957395279, -0.04633534904835249], [-0.12423896051228256, 0.04667346016709796]]),\n",
      "'b3': np.array([0.0, 0.0]),\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# Charger le modèle\n",
    "with open('/home/onyxia/work/RL_project_sailing/notebooks/models/wind_aware_navigator_trained.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "# Si votre modèle est déjà dans le format de dictionnaire utilisé dans votre agent:\n",
    "state_eval_model = model['state_eval_model']\n",
    "wind_prediction_model = model['wind_prediction_model']\n",
    "\n",
    "# Convertir les poids en chaînes de caractères pour les copier facilement\n",
    "for key, value in state_eval_model.items():\n",
    "    print(f\"'{key}': np.array({value.tolist()}),\")\n",
    "\n",
    "for key, value in wind_prediction_model.items():\n",
    "    print(f\"'{key}': np.array({value.tolist()}),\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utilitaire de chargement créé dans ../src/utils/sailing_model_loader.py\n"
     ]
    }
   ],
   "source": [
    "# Créer un fichier utilitaire pour charger votre modèle spécifique\n",
    "\n",
    "def create_custom_model_loader():\n",
    "    \"\"\"Crée un fichier utilitaire pour charger le modèle spécifique dans evaluate_agent.ipynb\"\"\"\n",
    "    loader_code = \"\"\"\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def load_trained_sailing_model(agent, \n",
    "                               model_path=\"/home/onyxia/work/RL_project_sailing/notebooks/models/wind_aware_navigator_trained.pkl\"):\n",
    "    \\\"\\\"\\\"\n",
    "    Charge les poids entraînés dans l'agent WindAwareNavigator.\n",
    "    \n",
    "    Args:\n",
    "        agent: L'agent WindAwareNavigator\n",
    "        model_path: Chemin vers le fichier de poids (par défaut: celui existant)\n",
    "        \n",
    "    Returns:\n",
    "        success: True si le chargement a réussi, False sinon\n",
    "    \\\"\\\"\\\"\n",
    "    try:\n",
    "        if not os.path.exists(model_path):\n",
    "            print(f\"Fichier de modèle non trouvé: {model_path}\")\n",
    "            return False\n",
    "            \n",
    "        with open(model_path, 'rb') as f:\n",
    "            models = pickle.load(f)\n",
    "            \n",
    "            if 'state_eval_model' in models and 'wind_prediction_model' in models:\n",
    "                agent.state_eval_model = models['state_eval_model']\n",
    "                agent.wind_prediction_model = models['wind_prediction_model']\n",
    "                print(f\"Modèle chargé avec succès depuis {model_path}\")\n",
    "                return True\n",
    "            else:\n",
    "                print(\"Format de modèle invalide - clés manquantes\")\n",
    "                return False\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors du chargement du modèle: {e}\")\n",
    "        return False\n",
    "\"\"\"\n",
    "    \n",
    "    # Chemin de destination\n",
    "    loader_path = \"../src/utils/sailing_model_loader.py\"\n",
    "    \n",
    "    # Créer le répertoire si nécessaire\n",
    "    import os\n",
    "    os.makedirs(os.path.dirname(loader_path), exist_ok=True)\n",
    "    \n",
    "    # Écrire le fichier\n",
    "    with open(loader_path, 'w') as f:\n",
    "        f.write(loader_code)\n",
    "    \n",
    "    # S'assurer que le fichier __init__.py existe\n",
    "    init_path = \"../src/utils/__init__.py\"\n",
    "    if not os.path.exists(init_path):\n",
    "        with open(init_path, 'w') as f:\n",
    "            f.write(\"# Utilitaires pour le projet sailing\\n\")\n",
    "    \n",
    "    print(f\"Utilitaire de chargement créé dans {loader_path}\")\n",
    "\n",
    "# Créer l'utilitaire\n",
    "create_custom_model_loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créez une classe qui étend SailingEnv pour modifier la récompense\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from src.env_sailing import SailingEnv\n",
    "\n",
    "class EfficientSailingEnv(SailingEnv):\n",
    "    \"\"\"Version modifiée de l'environnement qui récompense l'efficacité.\"\"\"\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.last_distance = None\n",
    "        self.step_penalty = -0.01  # Petite pénalité pour chaque étape\n",
    "        self.cumulative_distance = 0.0\n",
    "    \n",
    "    def reset(self, *args, **kwargs):\n",
    "        observation, info = super().reset(*args, **kwargs)\n",
    "        self.last_distance = np.linalg.norm(self.position - self.goal_position)\n",
    "        self.cumulative_distance = 0.0\n",
    "        return observation, info\n",
    "    \n",
    "    def step(self, action):\n",
    "        # Position avant le mouvement\n",
    "        old_position = self.position.copy()\n",
    "        \n",
    "        # Faire le pas normal\n",
    "        observation, reward, terminated, truncated, info = super().step(action)\n",
    "        \n",
    "        # Calculer la nouvelle distance au but\n",
    "        current_distance = np.linalg.norm(self.position - self.goal_position)\n",
    "        \n",
    "        # Distance parcourue dans ce pas\n",
    "        step_distance = np.linalg.norm(self.position - old_position)\n",
    "        self.cumulative_distance += step_distance\n",
    "        \n",
    "        # Calculer la récompense modifiée\n",
    "        if reward > 0:  # Si l'objectif est atteint\n",
    "            # Récompense principale + bonus pour l'efficacité\n",
    "            efficiency_bonus = 50.0 * (1.0 - self.cumulative_distance / (2.0 * self.last_distance))\n",
    "            efficiency_bonus = max(0, efficiency_bonus)  # Pas de bonus négatif\n",
    "            modified_reward = reward + efficiency_bonus\n",
    "            \n",
    "            # Ajouter l'information d'efficacité\n",
    "            info['efficiency'] = efficiency_bonus / 50.0\n",
    "            info['path_length'] = self.cumulative_distance\n",
    "        else:\n",
    "            # Petite récompense pour se rapprocher de l'objectif\n",
    "            approach_reward = (self.last_distance - current_distance) * 0.5\n",
    "            \n",
    "            # Pénalité pour chaque étape pour encourager la rapidité\n",
    "            modified_reward = approach_reward + self.step_penalty\n",
    "            \n",
    "            # Mettre à jour la dernière distance\n",
    "            self.last_distance = current_distance\n",
    "        \n",
    "        return observation, modified_reward, terminated, truncated, info\n",
    "\n",
    "# Fonction pour créer cet environnement amélioré\n",
    "def create_efficient_environment(initial_windfield_name):\n",
    "    \"\"\"Crée un environnement qui encourage l'efficacité du parcours.\"\"\"\n",
    "    from src.initial_windfields import get_initial_windfield\n",
    "    \n",
    "    initial_windfield = get_initial_windfield(initial_windfield_name)\n",
    "    env = EfficientSailingEnv(\n",
    "        wind_init_params=initial_windfield['wind_init_params'],\n",
    "        wind_evol_params=initial_windfield['wind_evol_params'],\n",
    "        render_mode=None\n",
    "    )\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedPolicyGradientTrainer(PolicyGradientTrainer):\n",
    "    \"\"\"Version améliorée de l'entraîneur pour optimiser l'efficacité.\"\"\"\n",
    "    \n",
    "    def create_environment(self, initial_windfield_name):\n",
    "        \"\"\"Utilise l'environnement amélioré au lieu de l'environnement standard.\"\"\"\n",
    "        return create_efficient_environment(initial_windfield_name)\n",
    "    \n",
    "    def evaluate_agent(self):\n",
    "        \"\"\"\n",
    "        Évalue les performances de l'agent avec un focus sur l'efficacité.\n",
    "        \n",
    "        Returns:\n",
    "            avg_score: Score moyen combinant réussite et efficacité\n",
    "        \"\"\"\n",
    "        from src.evaluation import evaluate_agent\n",
    "        from src.initial_windfields import get_initial_windfield\n",
    "        \n",
    "        success_rates = []\n",
    "        efficiency_scores = []\n",
    "        \n",
    "        for initial_windfield_name in self.initial_windfields:\n",
    "            # Évaluer sur plusieurs graines\n",
    "            seeds = list(range(10))  # 10 graines différentes\n",
    "            \n",
    "            # Créer l'environnement\n",
    "            initial_windfield = get_initial_windfield(initial_windfield_name)\n",
    "            \n",
    "            # Évaluer l'agent\n",
    "            results = evaluate_agent(\n",
    "                agent=self.agent,\n",
    "                initial_windfield=initial_windfield,\n",
    "                seeds=seeds,\n",
    "                max_horizon=self.max_steps_per_episode,\n",
    "                verbose=False,\n",
    "                render=False,\n",
    "                full_trajectory=False\n",
    "            )\n",
    "            \n",
    "            success_rates.append(results['success_rate'])\n",
    "            \n",
    "            # Calculer un score d'efficacité basé sur le nombre d'étapes\n",
    "            # Plus le nombre d'étapes est petit, meilleur est le score\n",
    "            # Normaliser pour obtenir un score entre 0 et 1\n",
    "            if results['success_rate'] > 0:\n",
    "                # Estimation d'un chemin idéal (distance directe)\n",
    "                ideal_steps = 32  # Une estimation, à ajuster selon votre environnement\n",
    "                actual_steps = results['mean_steps']\n",
    "                \n",
    "                # Score d'efficacité: 1.0 pour le chemin idéal, diminue avec plus d'étapes\n",
    "                efficiency = min(1.0, ideal_steps / actual_steps)\n",
    "                efficiency_scores.append(efficiency)\n",
    "            else:\n",
    "                efficiency_scores.append(0.0)\n",
    "        \n",
    "        # Combiner les scores\n",
    "        avg_success_rate = np.mean(success_rates)\n",
    "        avg_efficiency = np.mean(efficiency_scores)\n",
    "        \n",
    "        # Score combiné: 50% réussite, 50% efficacité\n",
    "        combined_score = 0.5 * avg_success_rate + 0.5 * avg_efficiency\n",
    "        \n",
    "        self.evaluation_scores.append(combined_score)\n",
    "        \n",
    "        print(f\"Taux de succès moyen: {avg_success_rate:.2%}\")\n",
    "        print(f\"Score d'efficacité moyen: {avg_efficiency:.2%}\")\n",
    "        print(f\"Score combiné: {combined_score:.2%}\")\n",
    "        \n",
    "        return combined_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Évaluation de l'agent initial...\n",
      "Taux de succès moyen: 100.00%\n",
      "Score d'efficacité moyen: 66.99%\n",
      "Score combiné: 83.50%\n",
      "\n",
      "Démarrage de l'entraînement pour optimiser l'efficacité...\n",
      "Démarrage de l'entraînement du WindAwareNavigator...\n",
      "Évaluation initiale de l'agent...\n",
      "Taux de succès moyen: 100.00%\n",
      "Score d'efficacité moyen: 65.64%\n",
      "Score combiné: 82.82%\n",
      "Taux de succès initial: 82.82%\n",
      "\n",
      "Épisodes 0-9\n",
      "Collecte d'expériences...\n",
      "Taux de succès du batch: 100.00%\n",
      "Mise à jour de la politique...\n",
      "Évaluation de l'agent...\n",
      "Taux de succès moyen: 100.00%\n",
      "Score d'efficacité moyen: 67.36%\n",
      "Score combiné: 83.68%\n",
      "Taux de succès d'évaluation: 83.68%\n",
      "Agent sauvegardé dans models/efficient_navigator.pkl\n",
      "Nouveau meilleur taux de succès: 83.68%\n",
      "Récompense moyenne: 113.70\n",
      "Longueur moyenne d'épisode: 58.4\n",
      "\n",
      "Épisodes 10-19\n",
      "Collecte d'expériences...\n",
      "Taux de succès du batch: 100.00%\n",
      "Mise à jour de la politique...\n",
      "Récompense moyenne: 113.78\n",
      "Longueur moyenne d'épisode: 55.4\n",
      "\n",
      "Épisodes 20-29\n",
      "Collecte d'expériences...\n",
      "Taux de succès du batch: 100.00%\n",
      "Mise à jour de la politique...\n",
      "Récompense moyenne: 113.81\n",
      "Longueur moyenne d'épisode: 56.8\n",
      "\n",
      "Épisodes 30-39\n",
      "Collecte d'expériences...\n",
      "Taux de succès du batch: 100.00%\n",
      "Mise à jour de la politique...\n",
      "Récompense moyenne: 114.02\n",
      "Longueur moyenne d'épisode: 40.9\n",
      "\n",
      "Épisodes 40-49\n",
      "Collecte d'expériences...\n",
      "Taux de succès du batch: 100.00%\n",
      "Mise à jour de la politique...\n",
      "Récompense moyenne: 113.89\n",
      "Longueur moyenne d'épisode: 53.0\n",
      "\n",
      "Épisodes 50-59\n",
      "Collecte d'expériences...\n",
      "Taux de succès du batch: 100.00%\n",
      "Mise à jour de la politique...\n",
      "Évaluation de l'agent...\n",
      "Taux de succès moyen: 100.00%\n",
      "Score d'efficacité moyen: 68.40%\n",
      "Score combiné: 84.20%\n",
      "Taux de succès d'évaluation: 84.20%\n",
      "Agent sauvegardé dans models/efficient_navigator.pkl\n",
      "Nouveau meilleur taux de succès: 84.20%\n",
      "Récompense moyenne: 113.72\n",
      "Longueur moyenne d'épisode: 54.7\n",
      "\n",
      "Épisodes 60-69\n",
      "Collecte d'expériences...\n",
      "Taux de succès du batch: 100.00%\n",
      "Mise à jour de la politique...\n",
      "Récompense moyenne: 113.91\n",
      "Longueur moyenne d'épisode: 52.0\n",
      "\n",
      "Épisodes 70-79\n",
      "Collecte d'expériences...\n",
      "Taux de succès du batch: 100.00%\n",
      "Mise à jour de la politique...\n",
      "Récompense moyenne: 113.80\n",
      "Longueur moyenne d'épisode: 57.2\n",
      "\n",
      "Épisodes 80-89\n",
      "Collecte d'expériences...\n",
      "Taux de succès du batch: 100.00%\n",
      "Mise à jour de la politique...\n",
      "Récompense moyenne: 113.83\n",
      "Longueur moyenne d'épisode: 60.1\n",
      "\n",
      "Épisodes 90-99\n",
      "Collecte d'expériences...\n",
      "Taux de succès du batch: 100.00%\n",
      "Mise à jour de la politique...\n",
      "Récompense moyenne: 113.87\n",
      "Longueur moyenne d'épisode: 53.2\n",
      "\n",
      "Épisodes 100-109\n",
      "Collecte d'expériences...\n",
      "Taux de succès du batch: 100.00%\n",
      "Mise à jour de la politique...\n",
      "Évaluation de l'agent...\n",
      "Taux de succès moyen: 100.00%\n",
      "Score d'efficacité moyen: 66.57%\n",
      "Score combiné: 83.28%\n",
      "Taux de succès d'évaluation: 83.28%\n",
      "Récompense moyenne: 113.84\n",
      "Longueur moyenne d'épisode: 65.8\n",
      "\n",
      "Épisodes 110-119\n",
      "Collecte d'expériences...\n",
      "Taux de succès du batch: 100.00%\n",
      "Mise à jour de la politique...\n",
      "Récompense moyenne: 113.75\n",
      "Longueur moyenne d'épisode: 67.6\n",
      "\n",
      "Épisodes 120-129\n",
      "Collecte d'expériences...\n",
      "Taux de succès du batch: 100.00%\n",
      "Mise à jour de la politique...\n",
      "Récompense moyenne: 113.67\n",
      "Longueur moyenne d'épisode: 73.7\n",
      "\n",
      "Épisodes 130-139\n",
      "Collecte d'expériences...\n",
      "Taux de succès du batch: 100.00%\n",
      "Mise à jour de la politique...\n",
      "Récompense moyenne: 113.88\n",
      "Longueur moyenne d'épisode: 54.0\n",
      "\n",
      "Épisodes 140-149\n",
      "Collecte d'expériences...\n",
      "Taux de succès du batch: 100.00%\n",
      "Mise à jour de la politique...\n",
      "Récompense moyenne: 113.90\n",
      "Longueur moyenne d'épisode: 52.3\n",
      "\n",
      "Épisodes 150-159\n",
      "Collecte d'expériences...\n",
      "Taux de succès du batch: 100.00%\n",
      "Mise à jour de la politique...\n",
      "Évaluation de l'agent...\n",
      "Taux de succès moyen: 100.00%\n",
      "Score d'efficacité moyen: 66.68%\n",
      "Score combiné: 83.34%\n",
      "Taux de succès d'évaluation: 83.34%\n",
      "Récompense moyenne: 113.88\n",
      "Longueur moyenne d'épisode: 55.2\n",
      "\n",
      "Épisodes 160-169\n",
      "Collecte d'expériences...\n",
      "Taux de succès du batch: 100.00%\n",
      "Mise à jour de la politique...\n",
      "Récompense moyenne: 113.72\n",
      "Longueur moyenne d'épisode: 68.7\n",
      "\n",
      "Épisodes 170-179\n",
      "Collecte d'expériences...\n",
      "Taux de succès du batch: 100.00%\n",
      "Mise à jour de la politique...\n",
      "Récompense moyenne: 113.76\n",
      "Longueur moyenne d'épisode: 63.5\n",
      "\n",
      "Épisodes 180-189\n",
      "Collecte d'expériences...\n",
      "Taux de succès du batch: 100.00%\n",
      "Mise à jour de la politique...\n",
      "Récompense moyenne: 113.82\n",
      "Longueur moyenne d'épisode: 58.2\n",
      "\n",
      "Épisodes 190-199\n",
      "Collecte d'expériences...\n",
      "Taux de succès du batch: 100.00%\n",
      "Mise à jour de la politique...\n",
      "Récompense moyenne: 113.91\n",
      "Longueur moyenne d'épisode: 50.2\n",
      "\n",
      "Épisodes 200-209\n",
      "Collecte d'expériences...\n",
      "Taux de succès du batch: 100.00%\n",
      "Mise à jour de la politique...\n",
      "Évaluation de l'agent...\n",
      "Taux de succès moyen: 100.00%\n",
      "Score d'efficacité moyen: 65.82%\n",
      "Score combiné: 82.91%\n",
      "Taux de succès d'évaluation: 82.91%\n",
      "Récompense moyenne: 113.82\n",
      "Longueur moyenne d'épisode: 54.8\n",
      "\n",
      "Épisodes 210-219\n",
      "Collecte d'expériences...\n",
      "Taux de succès du batch: 100.00%\n",
      "Mise à jour de la politique...\n",
      "Récompense moyenne: 113.71\n",
      "Longueur moyenne d'épisode: 61.4\n",
      "\n",
      "Épisodes 220-229\n",
      "Collecte d'expériences...\n",
      "Taux de succès du batch: 100.00%\n",
      "Mise à jour de la politique...\n",
      "Récompense moyenne: 113.66\n",
      "Longueur moyenne d'épisode: 61.4\n",
      "\n",
      "Épisodes 230-239\n",
      "Collecte d'expériences...\n",
      "Taux de succès du batch: 100.00%\n",
      "Mise à jour de la politique...\n",
      "Récompense moyenne: 113.83\n",
      "Longueur moyenne d'épisode: 45.3\n",
      "\n",
      "Épisodes 240-249\n",
      "Collecte d'expériences...\n",
      "Taux de succès du batch: 100.00%\n",
      "Mise à jour de la politique...\n",
      "Récompense moyenne: 113.85\n",
      "Longueur moyenne d'épisode: 52.9\n",
      "\n",
      "Épisodes 250-259\n",
      "Collecte d'expériences...\n",
      "Taux de succès du batch: 100.00%\n",
      "Mise à jour de la politique...\n",
      "Évaluation de l'agent...\n",
      "Taux de succès moyen: 100.00%\n",
      "Score d'efficacité moyen: 66.60%\n",
      "Score combiné: 83.30%\n",
      "Taux de succès d'évaluation: 83.30%\n",
      "Récompense moyenne: 113.92\n",
      "Longueur moyenne d'épisode: 50.1\n",
      "\n",
      "Épisodes 260-269\n",
      "Collecte d'expériences...\n",
      "Taux de succès du batch: 100.00%\n",
      "Mise à jour de la politique...\n",
      "Récompense moyenne: 113.75\n",
      "Longueur moyenne d'épisode: 64.3\n",
      "\n",
      "Épisodes 270-279\n",
      "Collecte d'expériences...\n",
      "Taux de succès du batch: 100.00%\n",
      "Mise à jour de la politique...\n",
      "Récompense moyenne: 113.97\n",
      "Longueur moyenne d'épisode: 50.8\n",
      "\n",
      "Épisodes 280-289\n",
      "Collecte d'expériences...\n",
      "Taux de succès du batch: 100.00%\n",
      "Mise à jour de la politique...\n",
      "Récompense moyenne: 113.90\n",
      "Longueur moyenne d'épisode: 49.9\n",
      "\n",
      "Épisodes 290-299\n",
      "Collecte d'expériences...\n",
      "Taux de succès du batch: 100.00%\n",
      "Mise à jour de la politique...\n",
      "Récompense moyenne: 113.80\n",
      "Longueur moyenne d'épisode: 53.9\n",
      "\n",
      "Épisodes 300-309\n",
      "Collecte d'expériences...\n",
      "Taux de succès du batch: 100.00%\n",
      "Mise à jour de la politique...\n",
      "Évaluation de l'agent...\n",
      "Taux de succès moyen: 100.00%\n",
      "Score d'efficacité moyen: 66.13%\n",
      "Score combiné: 83.07%\n",
      "Taux de succès d'évaluation: 83.07%\n",
      "Pas d'amélioration depuis 50 épisodes. Arrêt de l'entraînement.\n",
      "\n",
      "Entraînement terminé en 961.19 secondes!\n",
      "Meilleur taux de succès: 84.20%\n",
      "Amélioration: 1.38%\n",
      "Agent chargé depuis models/efficient_navigator.pkl\n",
      "\n",
      "--- Comparaison d'efficacité ---\n",
      "training_1:\n",
      "  Agent par défaut: 62.3 étapes\n",
      "  Agent entraîné:   65.4 étapes\n",
      "  Amélioration:     -5.0%\n",
      "training_2:\n",
      "  Agent par défaut: 28.3 étapes\n",
      "  Agent entraîné:   28.3 étapes\n",
      "  Amélioration:     0.0%\n",
      "training_3:\n",
      "  Agent par défaut: 64.5 étapes\n",
      "  Agent entraîné:   62.8 étapes\n",
      "  Amélioration:     2.6%\n"
     ]
    }
   ],
   "source": [
    "# Créer et utiliser l'entraîneur amélioré\n",
    "improved_trainer = ImprovedPolicyGradientTrainer(\n",
    "    agent=WindAwareNavigator(),\n",
    "    initial_windfields=['training_1', 'training_2', 'training_3'],\n",
    "    learning_rate=0.01,\n",
    "    gamma=0.99,\n",
    "    batch_size=10,\n",
    "    max_episodes=1000,\n",
    "    model_save_path=\"models/efficient_navigator.pkl\"\n",
    ")\n",
    "\n",
    "# Évaluation initiale\n",
    "print(\"Évaluation de l'agent initial...\")\n",
    "initial_score = improved_trainer.evaluate_agent()\n",
    "\n",
    "# Entraînement complet\n",
    "print(\"\\nDémarrage de l'entraînement pour optimiser l'efficacité...\")\n",
    "trained_agent = improved_trainer.train()\n",
    "\n",
    "# Fonction pour tester l'efficacité des agents\n",
    "def compare_efficiency(agent_default, agent_trained):\n",
    "    \"\"\"Compare l'efficacité entre deux agents.\"\"\"\n",
    "    from src.evaluation import evaluate_agent\n",
    "    from src.initial_windfields import get_initial_windfield\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for name in ['training_1', 'training_2', 'training_3']:\n",
    "        initial_windfield = get_initial_windfield(name)\n",
    "        \n",
    "        # Évaluer l'agent par défaut\n",
    "        default_results = evaluate_agent(\n",
    "            agent=agent_default,\n",
    "            initial_windfield=initial_windfield,\n",
    "            seeds=list(range(10)),\n",
    "            max_horizon=200,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        # Évaluer l'agent entraîné\n",
    "        trained_results = evaluate_agent(\n",
    "            agent=agent_trained,\n",
    "            initial_windfield=initial_windfield,\n",
    "            seeds=list(range(10)),\n",
    "            max_horizon=200,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        # Comparer les résultats\n",
    "        results[name] = {\n",
    "            'default_steps': default_results['mean_steps'],\n",
    "            'trained_steps': trained_results['mean_steps'],\n",
    "            'improvement': (default_results['mean_steps'] - trained_results['mean_steps']) / default_results['mean_steps'] * 100\n",
    "        }\n",
    "    \n",
    "    # Afficher les résultats\n",
    "    print(\"\\n--- Comparaison d'efficacité ---\")\n",
    "    for name, res in results.items():\n",
    "        print(f\"{name}:\")\n",
    "        print(f\"  Agent par défaut: {res['default_steps']:.1f} étapes\")\n",
    "        print(f\"  Agent entraîné:   {res['trained_steps']:.1f} étapes\")\n",
    "        print(f\"  Amélioration:     {res['improvement']:.1f}%\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Comparer l'agent entraîné avec l'agent d'origine\n",
    "agent_default = WindAwareNavigator()\n",
    "efficiency_results = compare_efficiency(agent_default, trained_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implémentation de PPO pour le WindAwareNavigator\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "import os\n",
    "from collections import deque\n",
    "\n",
    "class PPOTrainer:\n",
    "    \"\"\"\n",
    "    Entraîneur PPO (Proximal Policy Optimization) pour le WindAwareNavigator.\n",
    "    Optimise spécifiquement l'efficacité des trajectoires.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                agent,\n",
    "                initial_windfields=['training_1', 'training_2', 'training_3'],\n",
    "                learning_rate=0.001,\n",
    "                gamma=0.99,\n",
    "                clip_ratio=0.2,\n",
    "                batch_size=20,\n",
    "                epochs=5,\n",
    "                max_episodes=1000,\n",
    "                model_save_path='models/ppo_navigator.pkl',\n",
    "                seed=None):\n",
    "        \"\"\"\n",
    "        Initialise l'entraîneur PPO.\n",
    "        \n",
    "        Args:\n",
    "            agent: L'agent WindAwareNavigator à entraîner\n",
    "            initial_windfields: Liste des configurations de vent pour l'entraînement\n",
    "            learning_rate: Taux d'apprentissage pour la mise à jour des poids\n",
    "            gamma: Facteur d'actualisation pour le calcul des retours\n",
    "            clip_ratio: Paramètre d'écrêtage de PPO pour limiter les mises à jour\n",
    "            batch_size: Nombre d'épisodes avant mise à jour des poids\n",
    "            epochs: Nombre d'époques de mise à jour pour chaque batch\n",
    "            max_episodes: Nombre maximum d'épisodes d'entraînement\n",
    "            model_save_path: Chemin où sauvegarder l'agent entraîné\n",
    "        \"\"\"\n",
    "        self.agent = agent\n",
    "        self.initial_windfields = initial_windfields\n",
    "        self.learning_rate = learning_rate\n",
    "        self.gamma = gamma\n",
    "        self.clip_ratio = clip_ratio\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.max_episodes = max_episodes\n",
    "        self.model_save_path = model_save_path\n",
    "        \n",
    "        # Variables de suivi de l'entraînement\n",
    "        self.episode_rewards = []\n",
    "        self.episode_lengths = []\n",
    "        self.success_rates = []\n",
    "        self.evaluation_scores = []\n",
    "        \n",
    "        # Buffer pour stocker les expériences\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "        self.next_states = []\n",
    "        self.action_probs = []  # Probabilités d'action pour calculer le ratio\n",
    "        self.np_random = np.random.RandomState(seed)\n",
    "        # Créer le répertoire pour sauvegarder les modèles si nécessaire\n",
    "        os.makedirs(os.path.dirname(model_save_path), exist_ok=True)\n",
    "        \n",
    "        # Historique des valeurs\n",
    "        self.value_history = deque(maxlen=1000)\n",
    "    \n",
    "    def create_efficient_environment(self, initial_windfield_name):\n",
    "        \"\"\"Crée un environnement qui encourage l'efficacité du parcours.\"\"\"\n",
    "        from src.initial_windfields import get_initial_windfield\n",
    "        from src.env_sailing import SailingEnv\n",
    "        \n",
    "        class EfficientSailingEnv(SailingEnv):\n",
    "            \"\"\"Version modifiée qui récompense l'efficacité.\"\"\"\n",
    "            \n",
    "            def __init__(self, *args, **kwargs):\n",
    "                super().__init__(*args, **kwargs)\n",
    "                self.last_distance = None\n",
    "                self.step_penalty = -0.01  # Petite pénalité pour chaque étape\n",
    "                self.cumulative_distance = 0.0\n",
    "                self.last_position = None\n",
    "            \n",
    "            def reset(self, *args, **kwargs):\n",
    "                observation, info = super().reset(*args, **kwargs)\n",
    "                self.last_distance = np.linalg.norm(self.position - self.goal_position)\n",
    "                self.cumulative_distance = 0.0\n",
    "                self.last_position = self.position.copy()\n",
    "                return observation, info\n",
    "            \n",
    "            def step(self, action):\n",
    "                # Position avant le mouvement\n",
    "                old_position = self.position.copy()\n",
    "                \n",
    "                # Faire le pas normal\n",
    "                observation, reward, terminated, truncated, info = super().step(action)\n",
    "                \n",
    "                # Calculer la nouvelle distance au but\n",
    "                current_distance = np.linalg.norm(self.position - self.goal_position)\n",
    "                \n",
    "                # Distance parcourue dans ce pas\n",
    "                step_distance = np.linalg.norm(self.position - old_position)\n",
    "                self.cumulative_distance += step_distance\n",
    "                \n",
    "                # Calculer la récompense modifiée\n",
    "                if reward > 0:  # Si l'objectif est atteint\n",
    "                    # Récompense principale + bonus pour l'efficacité\n",
    "                    efficiency_bonus = 50.0 * (1.0 - self.cumulative_distance / (2.0 * self.last_distance))\n",
    "                    efficiency_bonus = max(0, efficiency_bonus)  # Pas de bonus négatif\n",
    "                    modified_reward = reward + efficiency_bonus\n",
    "                    \n",
    "                    # Ajouter l'information d'efficacité\n",
    "                    info['efficiency'] = efficiency_bonus / 50.0\n",
    "                    info['path_length'] = self.cumulative_distance\n",
    "                else:\n",
    "                    # Petite récompense pour se rapprocher de l'objectif\n",
    "                    approach_reward = (self.last_distance - current_distance) * 0.5\n",
    "                    \n",
    "                    # Pénalité pour chaque étape pour encourager la rapidité\n",
    "                    modified_reward = approach_reward + self.step_penalty\n",
    "                    \n",
    "                    # Pénalité supplémentaire pour les zigzags inefficaces\n",
    "                    if self.last_position is not None:\n",
    "                        # Vecteur vers l'objectif\n",
    "                        goal_vector = self.goal_position - old_position\n",
    "                        goal_direction = goal_vector / (np.linalg.norm(goal_vector) + 1e-10)\n",
    "                        \n",
    "                        # Vecteur du mouvement actuel\n",
    "                        movement_vector = self.position - old_position\n",
    "                        if np.linalg.norm(movement_vector) > 0.001:\n",
    "                            movement_direction = movement_vector / np.linalg.norm(movement_vector)\n",
    "                            \n",
    "                            # Alignement avec la direction de l'objectif\n",
    "                            goal_alignment = np.dot(movement_direction, goal_direction)\n",
    "                            \n",
    "                            # Bonus pour l'alignement avec l'objectif (si possible selon le vent)\n",
    "                            current_wind = self._get_wind_at_position(old_position)\n",
    "                            wind_norm = current_wind / (np.linalg.norm(current_wind) + 1e-10)\n",
    "                            wind_from = -wind_norm\n",
    "                            \n",
    "                            # Vérifier si l'alignement avec l'objectif est possible\n",
    "                            # (pas dans la zone interdite contre le vent)\n",
    "                            angle_wind_goal = np.arccos(np.clip(np.dot(wind_from, goal_direction), -1.0, 1.0))\n",
    "                            \n",
    "                            if angle_wind_goal > np.pi/6:  # Si pas contre le vent\n",
    "                                # Bonus pour l'alignement avec l'objectif\n",
    "                                modified_reward += 0.2 * goal_alignment\n",
    "                    \n",
    "                    # Mettre à jour la dernière distance et position\n",
    "                    self.last_distance = current_distance\n",
    "                    self.last_position = self.position.copy()\n",
    "                \n",
    "                return observation, modified_reward, terminated, truncated, info\n",
    "        \n",
    "        initial_windfield = get_initial_windfield(initial_windfield_name)\n",
    "        env = EfficientSailingEnv(\n",
    "            wind_init_params=initial_windfield['wind_init_params'],\n",
    "            wind_evol_params=initial_windfield['wind_evol_params'],\n",
    "            render_mode=None\n",
    "        )\n",
    "        return env\n",
    "    \n",
    "    def preprocess_state(self, observation):\n",
    "        \"\"\"Prétraite l'observation pour l'utiliser avec les réseaux de neurones.\"\"\"\n",
    "        position = np.array([observation[0], observation[1]])\n",
    "        velocity = np.array([observation[2], observation[3]])\n",
    "        wind = np.array([observation[4], observation[5]])\n",
    "        \n",
    "        # Normaliser les entrées\n",
    "        pos_norm = position / np.array(self.agent.grid_size)\n",
    "        vel_norm = velocity / 2.0 if np.linalg.norm(velocity) > 0 else np.zeros(2)\n",
    "        wind_norm = wind / 5.0 if np.linalg.norm(wind) > 0 else np.zeros(2)\n",
    "        \n",
    "        # Concaténer les caractéristiques normalisées\n",
    "        return np.concatenate([pos_norm, vel_norm, wind_norm])\n",
    "    \n",
    "    def get_action_probabilities(self, observation):\n",
    "        \"\"\"\n",
    "        Calcule les probabilités pour chaque action en fonction des évaluations du réseau.\n",
    "        Nécessaire pour le calcul des ratios dans PPO.\n",
    "        \"\"\"\n",
    "        # Extraire les informations de l'observation\n",
    "        position = np.array([observation[0], observation[1]])\n",
    "        velocity = np.array([observation[2], observation[3]])\n",
    "        wind_at_position = np.array([observation[4], observation[5]])\n",
    "        \n",
    "        # Évaluer chaque action possible\n",
    "        action_values = np.zeros(9)  # 9 actions possibles\n",
    "        valid_actions = []\n",
    "        \n",
    "        for action in range(9):\n",
    "            direction = self.agent.action_to_direction(action)\n",
    "            next_pos = position + direction\n",
    "            \n",
    "            # Vérifier les limites pour les actions de mouvement\n",
    "            if action < 8:  # Toutes les actions sauf \"rester en place\"\n",
    "                if not (0 <= next_pos[0] < self.agent.grid_size[0] and \n",
    "                        0 <= next_pos[1] < self.agent.grid_size[1]):\n",
    "                    action_values[action] = -float('inf')\n",
    "                    continue\n",
    "            \n",
    "            # Estimer la nouvelle vitesse pour les actions de mouvement\n",
    "            if action < 8:\n",
    "                boat_dir = direction / np.linalg.norm(direction)\n",
    "                wind_norm = wind_at_position / (np.linalg.norm(wind_at_position) + 1e-10)\n",
    "                sailing_efficiency = self.agent.calculate_sailing_efficiency(boat_dir, wind_norm)\n",
    "                next_vel = boat_dir * sailing_efficiency * np.linalg.norm(wind_at_position) * 0.4\n",
    "            else:\n",
    "                next_vel = velocity * 0.3  # Inertie réduite pour \"rester en place\"\n",
    "            \n",
    "            # Évaluer l'état résultant\n",
    "            state_value = self.agent.evaluate_state(next_pos, next_vel, wind_at_position)\n",
    "            action_values[action] = state_value\n",
    "            valid_actions.append(action)\n",
    "        \n",
    "        # Convertir en probabilités avec softmax\n",
    "        # Appliquer seulement aux actions valides\n",
    "        max_value = np.max(action_values)\n",
    "        exp_values = np.exp(action_values - max_value)  # Pour stabilité numérique\n",
    "        probs = exp_values / np.sum(exp_values)\n",
    "        \n",
    "        return probs\n",
    "    \n",
    "    def collect_experience(self, num_episodes):\n",
    "        \"\"\"\n",
    "        Collecte des expériences en exécutant l'agent dans l'environnement.\n",
    "        Stocke aussi les probabilités d'action pour le calcul de PPO.\n",
    "        \"\"\"\n",
    "        # Réinitialiser les buffers\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "        self.next_states = []\n",
    "        self.action_probs = []\n",
    "        \n",
    "        episode_rewards_buffer = []\n",
    "        episode_lengths_buffer = []\n",
    "        successes = 0\n",
    "        \n",
    "        for _ in range(num_episodes):\n",
    "            # Choisir aléatoirement une configuration de vent\n",
    "            initial_windfield_name = np.random.choice(self.initial_windfields)\n",
    "            env = self.create_efficient_environment(initial_windfield_name)\n",
    "            \n",
    "            # Réinitialiser l'environnement et l'agent\n",
    "            observation, _ = env.reset(seed=np.random.randint(0, 1000))\n",
    "            self.agent.reset()\n",
    "            \n",
    "            episode_reward = 0\n",
    "            episode_states = []\n",
    "            episode_actions = []\n",
    "            episode_rewards = []\n",
    "            episode_dones = []\n",
    "            episode_next_states = []\n",
    "            episode_action_probs = []\n",
    "            \n",
    "            # Exécuter un épisode\n",
    "            for step in range(500):  # Limite élevée pour les scénarios difficiles\n",
    "                # Calculer les probabilités d'action avec l'état actuel\n",
    "                action_probs = self.get_action_probabilities(observation)\n",
    "                \n",
    "                # Obtenir l'action de l'agent\n",
    "                action = self.agent.act(observation)\n",
    "                \n",
    "                # Stocker la probabilité de l'action choisie\n",
    "                action_prob = action_probs[action]\n",
    "                \n",
    "                # Exécuter l'action\n",
    "                next_observation, reward, done, truncated, _ = env.step(action)\n",
    "                \n",
    "                # Stocker l'expérience\n",
    "                state = self.preprocess_state(observation)\n",
    "                next_state = self.preprocess_state(next_observation)\n",
    "                \n",
    "                episode_states.append(state)\n",
    "                episode_actions.append(action)\n",
    "                episode_rewards.append(reward)\n",
    "                episode_dones.append(done or truncated)\n",
    "                episode_next_states.append(next_state)\n",
    "                episode_action_probs.append(action_prob)\n",
    "                \n",
    "                episode_reward += reward\n",
    "                observation = next_observation\n",
    "                \n",
    "                if done or truncated:\n",
    "                    break\n",
    "            \n",
    "            # Ajouter les données de l'épisode aux buffers\n",
    "            self.states.extend(episode_states)\n",
    "            self.actions.extend(episode_actions)\n",
    "            self.rewards.extend(episode_rewards)\n",
    "            self.dones.extend(episode_dones)\n",
    "            self.next_states.extend(episode_next_states)\n",
    "            self.action_probs.extend(episode_action_probs)\n",
    "            \n",
    "            # Mettre à jour les métriques\n",
    "            episode_rewards_buffer.append(episode_reward)\n",
    "            episode_lengths_buffer.append(step + 1)\n",
    "            if episode_reward > 0:  # Si une récompense positive, considérer comme succès\n",
    "                successes += 1\n",
    "        \n",
    "        # Calculer les statistiques\n",
    "        self.episode_rewards.append(np.mean(episode_rewards_buffer))\n",
    "        self.episode_lengths.append(np.mean(episode_lengths_buffer))\n",
    "        success_rate = successes / num_episodes\n",
    "        self.success_rates.append(success_rate)\n",
    "        \n",
    "        return success_rate\n",
    "    \n",
    "    def compute_returns_and_advantages(self):\n",
    "        \"\"\"\n",
    "        Calcule les retours (discounted returns) et les avantages pour PPO.\n",
    "        \n",
    "        Returns:\n",
    "            returns: Liste des retours pour chaque état\n",
    "            advantages: Liste des avantages pour chaque état\n",
    "        \"\"\"\n",
    "        # Identifier les limites des épisodes\n",
    "        episode_ends = [i for i, done in enumerate(self.dones) if done]\n",
    "        if not episode_ends or episode_ends[-1] != len(self.dones) - 1:\n",
    "            episode_ends.append(len(self.dones) - 1)\n",
    "        \n",
    "        # Calculer les retours et les avantages pour chaque épisode\n",
    "        returns = np.zeros(len(self.rewards))\n",
    "        \n",
    "        # Valeurs de base pour chaque état\n",
    "        state_values = np.zeros(len(self.states))\n",
    "        for i, state in enumerate(self.states):\n",
    "            # Convertir l'état prétraité en paramètres pour evaluate_state\n",
    "            position = np.array([state[0], state[1]]) * self.agent.grid_size\n",
    "            velocity = np.array([state[2], state[3]]) * 2.0\n",
    "            wind = np.array([state[4], state[5]]) * 5.0\n",
    "            \n",
    "            # Obtenir la valeur d'état de base\n",
    "            state_values[i] = self.agent.evaluate_state(position, velocity, wind)\n",
    "        \n",
    "        # Stocker les valeurs pour analyse\n",
    "        self.value_history.extend(state_values)\n",
    "        \n",
    "        start_idx = 0\n",
    "        advantages = np.zeros(len(self.rewards))\n",
    "        \n",
    "        for end_idx in episode_ends:\n",
    "            # Calculer les retours pour cet épisode\n",
    "            episode_returns = np.zeros(end_idx - start_idx + 1)\n",
    "            discounted_sum = 0\n",
    "            \n",
    "            # Calculer les retours (du futur vers le présent)\n",
    "            for i in range(end_idx, start_idx - 1, -1):\n",
    "                discounted_sum = self.rewards[i] + self.gamma * discounted_sum * (1 - self.dones[i])\n",
    "                episode_returns[i - start_idx] = discounted_sum\n",
    "            \n",
    "            # Calculer les avantages (returns - baseline)\n",
    "            episode_advantages = episode_returns - state_values[start_idx:end_idx + 1]\n",
    "            \n",
    "            # Normaliser les avantages pour cet épisode\n",
    "            if len(episode_advantages) > 1:\n",
    "                episode_advantages = (episode_advantages - np.mean(episode_advantages)) / (np.std(episode_advantages) + 1e-8)\n",
    "            \n",
    "            # Stocker les retours et avantages\n",
    "            returns[start_idx:end_idx + 1] = episode_returns\n",
    "            advantages[start_idx:end_idx + 1] = episode_advantages\n",
    "            \n",
    "            # Passer à l'épisode suivant\n",
    "            start_idx = end_idx + 1\n",
    "        \n",
    "        return returns, advantages\n",
    "    \n",
    "    def update_policy_ppo(self, returns, advantages):\n",
    "        \"\"\"\n",
    "        Met à jour les poids des réseaux selon l'algorithme PPO.\n",
    "        \n",
    "        Args:\n",
    "            returns: Retours calculés pour chaque état\n",
    "            advantages: Avantages calculés pour chaque état\n",
    "        \"\"\"\n",
    "        # Convertir les listes en tableaux numpy\n",
    "        states = np.array(self.states)\n",
    "        actions = np.array(self.actions)\n",
    "        old_probs = np.array(self.action_probs)\n",
    "        \n",
    "        # Effectuer plusieurs passes d'optimisation sur le même batch\n",
    "        for epoch in range(self.epochs):\n",
    "            # Parcourir les données par mini-batchs (on peut simplifier en utilisant tout le batch)\n",
    "            indices = np.arange(len(states))\n",
    "            self.np_random.shuffle(indices)\n",
    "            \n",
    "            for start_idx in range(0, len(indices), 256):  # Taille de mini-batch = 256\n",
    "                end_idx = min(start_idx + 256, len(indices))\n",
    "                batch_indices = indices[start_idx:end_idx]\n",
    "                \n",
    "                # Extraire les données du mini-batch\n",
    "                batch_states = states[batch_indices]\n",
    "                batch_actions = actions[batch_indices]\n",
    "                batch_advantages = advantages[batch_indices]\n",
    "                batch_old_probs = old_probs[batch_indices]\n",
    "                \n",
    "                # Pour chaque exemple du mini-batch\n",
    "                for state, action, advantage, old_prob in zip(batch_states, batch_actions, batch_advantages, batch_old_probs):\n",
    "                    # Recalculer les probabilités actuelles\n",
    "                    # On pourrait optimiser en calculant toutes les probabilités en une fois\n",
    "                    position = np.array([state[0], state[1]]) * self.agent.grid_size\n",
    "                    velocity = np.array([state[2], state[3]]) * 2.0\n",
    "                    wind = np.array([state[4], state[5]]) * 5.0\n",
    "                    \n",
    "                    # Construire une observation factice pour get_action_probabilities\n",
    "                    fake_obs = np.concatenate([\n",
    "                        position, velocity, wind,\n",
    "                        np.zeros(self.agent.grid_size[0] * self.agent.grid_size[1] * 2)  # Wind field vide\n",
    "                    ])\n",
    "                    \n",
    "                    # Obtenir les probabilités actuelles (sans détachement du gradient, contrairement à PyTorch)\n",
    "                    current_probs = self.get_action_probabilities(fake_obs)\n",
    "                    current_prob = current_probs[action]\n",
    "                    \n",
    "                    # Calculer le ratio pour PPO\n",
    "                    ratio = current_prob / (old_prob + 1e-10)\n",
    "                    \n",
    "                    # Calculer les deux termes de la fonction objectif PPO\n",
    "                    obj_1 = ratio * advantage\n",
    "                    obj_2 = np.clip(ratio, 1.0 - self.clip_ratio, 1.0 + self.clip_ratio) * advantage\n",
    "                    \n",
    "                    # Prendre le minimum pour l'objectif clippé\n",
    "                    ppo_loss = -min(obj_1, obj_2)\n",
    "                    \n",
    "                    # Mise à jour manuelle des poids (approximation de la descente de gradient)\n",
    "                    # Note: Dans un framework comme PyTorch, on calculerait le gradient et ferait une mise à jour propre\n",
    "                    \n",
    "                    # Si on utilise une mise à jour manuelle, on peut faire:\n",
    "                    # (Les valeurs ajustées ici sont approximatives)\n",
    "                    if ppo_loss > 0:  # On veut minimiser la perte\n",
    "                        # Calculer le delta pour les poids\n",
    "                        # On utilise un signe opposé à la perte pour descendre le gradient\n",
    "                        update_factor = self.learning_rate * min(1.0, abs(advantage)) * np.sign(-ppo_loss)\n",
    "                        \n",
    "                        # Mise à jour des poids - version simplifiée\n",
    "                        # Dans un framework d'apprentissage profond, on ferait une vraie descente de gradient\n",
    "                        # Ici, on fait une modification directe des poids dans la direction qui améliore la politique\n",
    "                        \n",
    "                        # Les poids du modèle d'évaluation d'état influencent directement la sélection d'action\n",
    "                        # On peut donc les modifier pour promouvoir l'action si avantage positif ou la décourager si négatif\n",
    "                        hidden_activation = np.tanh(np.dot(state, self.agent.state_eval_model['W1']) + self.agent.state_eval_model['b1'])\n",
    "                        second_hidden = np.tanh(np.dot(hidden_activation, self.agent.state_eval_model['W2']) + self.agent.state_eval_model['b2'])\n",
    "                        \n",
    "                        # Modifier W3 pour favoriser/défavoriser l'action en fonction de l'avantage\n",
    "                        delta_W3 = update_factor * np.outer(second_hidden, np.ones(1))\n",
    "                        self.agent.state_eval_model['W3'] += delta_W3\n",
    "                        self.agent.state_eval_model['b3'] += update_factor\n",
    "    \n",
    "    def evaluate_agent(self):\n",
    "        \"\"\"\n",
    "        Évalue les performances de l'agent sur l'efficacité des trajectoires.\n",
    "        \n",
    "        Returns:\n",
    "            avg_score: Score combinant succès et efficacité\n",
    "        \"\"\"\n",
    "        from src.evaluation import evaluate_agent\n",
    "        from src.initial_windfields import get_initial_windfield\n",
    "        \n",
    "        success_rates = []\n",
    "        efficiency_scores = []\n",
    "        \n",
    "        for initial_windfield_name in self.initial_windfields:\n",
    "            # Créer l'environnement\n",
    "            initial_windfield = get_initial_windfield(initial_windfield_name)\n",
    "            \n",
    "            # Évaluer l'agent\n",
    "            results = evaluate_agent(\n",
    "                agent=self.agent,\n",
    "                initial_windfield=initial_windfield,\n",
    "                seeds=list(range(10)),\n",
    "                max_horizon=200,\n",
    "                verbose=False,\n",
    "                render=False,\n",
    "                full_trajectory=True  # Pour obtenir les positions\n",
    "            )\n",
    "            \n",
    "            success_rates.append(results['success_rate'])\n",
    "            \n",
    "            # Calculer l'efficacité moyenne du chemin\n",
    "            if results.get('positions') and results['success_rate'] > 0:\n",
    "                # Pour chaque trajectoire réussie, calculer l'efficacité\n",
    "                path_efficiency = []\n",
    "                \n",
    "                for seed_idx, positions in enumerate(results.get('positions', [])):\n",
    "                    if positions:\n",
    "                        # Calculer la longueur du chemin\n",
    "                        path_length = 0\n",
    "                        for i in range(1, len(positions)):\n",
    "                            path_length += np.linalg.norm(np.array(positions[i]) - np.array(positions[i-1]))\n",
    "                        \n",
    "                        # Distance directe\n",
    "                        direct_distance = np.linalg.norm(np.array(positions[0]) - np.array(positions[-1]))\n",
    "                        \n",
    "                        # Efficacité = distance directe / longueur réelle du chemin\n",
    "                        # Plus ce ratio est proche de 1, plus le chemin est efficace\n",
    "                        efficiency = direct_distance / (path_length + 1e-10)\n",
    "                        efficiency = min(1.0, efficiency)  # Plafonner à 1.0\n",
    "                        path_efficiency.append(efficiency)\n",
    "                \n",
    "                if path_efficiency:\n",
    "                    avg_path_efficiency = np.mean(path_efficiency)\n",
    "                    efficiency_scores.append(avg_path_efficiency)\n",
    "            else:\n",
    "                efficiency_scores.append(0.0)\n",
    "        \n",
    "        # Calculer les scores moyens\n",
    "        avg_success_rate = np.mean(success_rates)\n",
    "        avg_efficiency = np.mean(efficiency_scores) if efficiency_scores else 0.0\n",
    "        \n",
    "        # Score combiné: 60% succès, 40% efficacité\n",
    "        combined_score = 0.6 * avg_success_rate + 0.4 * avg_efficiency\n",
    "        \n",
    "        self.evaluation_scores.append(combined_score)\n",
    "        \n",
    "        print(f\"Taux de succès: {avg_success_rate:.2%}\")\n",
    "        print(f\"Efficacité moyenne: {avg_efficiency:.2%}\")\n",
    "        print(f\"Score combiné: {combined_score:.2%}\")\n",
    "        \n",
    "        return combined_score\n",
    "    \n",
    "    def save_agent(self):\n",
    "        \"\"\"Sauvegarde l'agent entraîné.\"\"\"\n",
    "        with open(self.model_save_path, 'wb') as f:\n",
    "            pickle.dump({\n",
    "                'state_eval_model': self.agent.state_eval_model,\n",
    "                'wind_prediction_model': self.agent.wind_prediction_model\n",
    "            }, f)\n",
    "        print(f\"Agent sauvegardé dans {self.model_save_path}\")\n",
    "    \n",
    "    def load_agent(self):\n",
    "        \"\"\"Charge un agent préalablement entraîné.\"\"\"\n",
    "        try:\n",
    "            with open(self.model_save_path, 'rb') as f:\n",
    "                models = pickle.load(f)\n",
    "                self.agent.state_eval_model = models['state_eval_model']\n",
    "                self.agent.wind_prediction_model = models['wind_prediction_model']\n",
    "            print(f\"Agent chargé depuis {self.model_save_path}\")\n",
    "            return True\n",
    "        except:\n",
    "            print(f\"Impossible de charger l'agent depuis {self.model_save_path}\")\n",
    "            return False\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Exécute le processus complet d'entraînement avec PPO.\n",
    "        \n",
    "        Returns:\n",
    "            agent: L'agent entraîné\n",
    "        \"\"\"\n",
    "        print(\"Démarrage de l'entraînement PPO pour le WindAwareNavigator...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Évaluer l'agent avant l'entraînement\n",
    "        print(\"Évaluation initiale de l'agent...\")\n",
    "        initial_score = self.evaluate_agent()\n",
    "        print(f\"Score initial: {initial_score:.2%}\")\n",
    "        \n",
    "        # Boucle principale d'entraînement\n",
    "        best_score = initial_score\n",
    "        episodes_without_improvement = 0\n",
    "        \n",
    "        for episode in range(0, self.max_episodes, self.batch_size):\n",
    "            # Afficher la progression\n",
    "            print(f\"\\nÉpisodes {episode}-{episode + self.batch_size - 1}/{self.max_episodes}\")\n",
    "            \n",
    "            # Collecter des expériences\n",
    "            print(\"Collecte d'expériences...\")\n",
    "            success_rate = self.collect_experience(self.batch_size)\n",
    "            print(f\"Taux de succès du batch: {success_rate:.2%}\")\n",
    "            \n",
    "            # Calculer les retours et avantages\n",
    "            returns, advantages = self.compute_returns_and_advantages()\n",
    "            \n",
    "            # Mettre à jour la politique avec PPO\n",
    "            print(\"Mise à jour de la politique avec PPO...\")\n",
    "            self.update_policy_ppo(returns, advantages)\n",
    "            \n",
    "            # Évaluer périodiquement l'agent (tous les 5 batches)\n",
    "            if (episode // self.batch_size) % 5 == 0:\n",
    "                print(\"Évaluation de l'agent...\")\n",
    "                eval_score = self.evaluate_agent()\n",
    "                print(f\"Score d'évaluation: {eval_score:.2%}\")\n",
    "                \n",
    "                # Sauvegarder le meilleur modèle\n",
    "                if eval_score > best_score:\n",
    "                    best_score = eval_score\n",
    "                    self.save_agent()\n",
    "                    episodes_without_improvement = 0\n",
    "                    print(f\"Nouveau meilleur score: {best_score:.2%}\")\n",
    "                else:\n",
    "                    episodes_without_improvement += self.batch_size\n",
    "            \n",
    "            # Arrêter si pas d'amélioration pendant longtemps\n",
    "            if episodes_without_improvement >= 50:\n",
    "                print(\"Pas d'amélioration depuis 50 épisodes. Arrêt de l'entraînement.\")\n",
    "                break\n",
    "            \n",
    "            # Afficher les statistiques\n",
    "            print(f\"Récompense moyenne: {self.episode_rewards[-1]:.2f}\")\n",
    "            print(f\"Longueur moyenne d'épisode: {self.episode_lengths[-1]:.1f}\")\n",
    "        \n",
    "        # Durée totale d'entraînement\n",
    "        training_time = time.time() - start_time\n",
    "        print(f\"\\nEntraînement terminé en {training_time:.2f} secondes!\")\n",
    "        print(f\"Meilleur score: {best_score:.2%}\")\n",
    "        print(f\"Amélioration: {best_score - initial_score:.2%}\")\n",
    "        \n",
    "        # Charger le meilleur modèle\n",
    "        self.load_agent()\n",
    "        \n",
    "        return self.agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Évaluation initiale...\n",
      "Taux de succès: 100.00%\n",
      "Efficacité moyenne: 0.00%\n",
      "Score combiné: 60.00%\n",
      "\n",
      "Démarrage de l'entraînement PPO pour 100 épisodes...\n",
      "\n",
      "Épisodes 0-19/100\n",
      "Collecte d'expériences...\n",
      "Taux de succès du batch: 100.00%\n",
      "Mise à jour de la politique avec PPO...\n",
      "Évaluation de l'agent...\n",
      "Taux de succès: 100.00%\n",
      "Efficacité moyenne: 0.00%\n",
      "Score combiné: 60.00%\n",
      "Score d'évaluation: 60.00%\n",
      "Agent sauvegardé dans models/ppo_navigator.pkl\n",
      "\n",
      "Épisodes 20-39/100\n",
      "Collecte d'expériences...\n",
      "Taux de succès du batch: 100.00%\n",
      "Mise à jour de la politique avec PPO...\n",
      "\n",
      "Épisodes 40-59/100\n",
      "Collecte d'expériences...\n",
      "Taux de succès du batch: 100.00%\n",
      "Mise à jour de la politique avec PPO...\n",
      "\n",
      "Épisodes 60-79/100\n",
      "Collecte d'expériences...\n",
      "Taux de succès du batch: 100.00%\n",
      "Mise à jour de la politique avec PPO...\n",
      "Évaluation de l'agent...\n",
      "Taux de succès: 100.00%\n",
      "Efficacité moyenne: 0.00%\n",
      "Score combiné: 60.00%\n",
      "Score d'évaluation: 60.00%\n",
      "Agent sauvegardé dans models/ppo_navigator.pkl\n",
      "\n",
      "Épisodes 80-99/100\n",
      "Collecte d'expériences...\n",
      "Taux de succès du batch: 100.00%\n",
      "Mise à jour de la politique avec PPO...\n",
      "Évaluation de l'agent...\n",
      "Taux de succès: 100.00%\n",
      "Efficacité moyenne: 0.00%\n",
      "Score combiné: 60.00%\n",
      "Score d'évaluation: 60.00%\n",
      "Agent sauvegardé dans models/ppo_navigator.pkl\n",
      "\n",
      "Évaluation finale après entraînement...\n",
      "Taux de succès: 100.00%\n",
      "Efficacité moyenne: 0.00%\n",
      "Score combiné: 60.00%\n",
      "\n",
      "Amélioration: 0.00%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1200 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Créer votre agent\n",
    "agent = WindAwareNavigator()\n",
    "# Initialiser l'entraîneur PPO\n",
    "ppo_trainer = PPOTrainer(\n",
    "   agent=agent,\n",
    "   initial_windfields=['training_1', 'training_2', 'training_3'],\n",
    "   learning_rate=0.001,\n",
    "   gamma=0.99,\n",
    "   clip_ratio=0.2,\n",
    "   batch_size=20,\n",
    "   epochs=4,\n",
    "   max_episodes=500,\n",
    "   model_save_path=\"models/ppo_navigator.pkl\"\n",
    ")\n",
    "\n",
    "# Évaluation initiale\n",
    "print(\"Évaluation initiale...\")\n",
    "initial_score = ppo_trainer.evaluate_agent()\n",
    "\n",
    "# Entraînement court pour démonstration\n",
    "# Vous pouvez ajuster le nombre d'épisodes selon vos besoins\n",
    "num_episodes = 100  # Ajustez ce nombre pour un entraînement plus ou moins long\n",
    "\n",
    "print(f\"\\nDémarrage de l'entraînement PPO pour {num_episodes} épisodes...\")\n",
    "for episode in range(0, num_episodes, ppo_trainer.batch_size):\n",
    "   print(f\"\\nÉpisodes {episode}-{episode + ppo_trainer.batch_size - 1}/{num_episodes}\")\n",
    "   \n",
    "   # Collecter des expériences\n",
    "   print(\"Collecte d'expériences...\")\n",
    "   success_rate = ppo_trainer.collect_experience(ppo_trainer.batch_size)\n",
    "   print(f\"Taux de succès du batch: {success_rate:.2%}\")\n",
    "   \n",
    "   # Calculer les retours et avantages\n",
    "   returns, advantages = ppo_trainer.compute_returns_and_advantages()\n",
    "   \n",
    "   # Mettre à jour la politique avec PPO\n",
    "   print(\"Mise à jour de la politique avec PPO...\")\n",
    "   ppo_trainer.update_policy_ppo(returns, advantages)\n",
    "   \n",
    "   # Évaluer périodiquement\n",
    "   if (episode // ppo_trainer.batch_size) % 3 == 0 or episode + ppo_trainer.batch_size >= num_episodes:\n",
    "       print(\"Évaluation de l'agent...\")\n",
    "       eval_score = ppo_trainer.evaluate_agent()\n",
    "       print(f\"Score d'évaluation: {eval_score:.2%}\")\n",
    "       \n",
    "       # Sauvegarder l'agent\n",
    "       ppo_trainer.save_agent()\n",
    "\n",
    "# Évaluation finale\n",
    "print(\"\\nÉvaluation finale après entraînement...\")\n",
    "final_score = ppo_trainer.evaluate_agent()\n",
    "\n",
    "print(f\"\\nAmélioration: {final_score - initial_score:.2%}\")\n",
    "\n",
    "# Visualiser les résultats d'entraînement\n",
    "def plot_ppo_training_results(trainer):\n",
    "   \"\"\"Affiche les métriques d'entraînement PPO sous forme de graphiques.\"\"\"\n",
    "   fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(10, 12))\n",
    "   \n",
    "   # Graphique des récompenses\n",
    "   ax1.plot(trainer.episode_rewards)\n",
    "   ax1.set_title('Récompense moyenne par batch')\n",
    "   ax1.set_xlabel('Batch')\n",
    "   ax1.set_ylabel('Récompense')\n",
    "   ax1.grid(True)\n",
    "   \n",
    "   # Graphique des longueurs d'épisodes\n",
    "   ax2.plot(trainer.episode_lengths)\n",
    "   ax2.set_title('Longueur moyenne des épisodes')\n",
    "   ax2.set_xlabel('Batch')\n",
    "   ax2.set_ylabel('Étapes')\n",
    "   ax2.grid(True)\n",
    "   \n",
    "   # Graphique des taux de succès et scores d'évaluation\n",
    "   ax3.plot(trainer.success_rates, label='Taux de succès (batch)')\n",
    "   if trainer.evaluation_scores:\n",
    "       eval_x = np.linspace(0, len(trainer.success_rates)-1, len(trainer.evaluation_scores))\n",
    "       ax3.plot(eval_x, trainer.evaluation_scores, 'r-', label='Score d\\'évaluation')\n",
    "   ax3.set_title('Taux de succès et scores d\\'évaluation')\n",
    "   ax3.set_xlabel('Batch')\n",
    "   ax3.set_ylabel('Taux')\n",
    "   ax3.set_ylim(0, 1.05)\n",
    "   ax3.legend()\n",
    "   ax3.grid(True)\n",
    "   \n",
    "   plt.tight_layout()\n",
    "   plt.show()\n",
    "\n",
    "# Afficher les résultats\n",
    "plot_ppo_training_results(ppo_trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnn\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mF\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/__init__.py:409\u001b[39m\n\u001b[32m    407\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m USE_GLOBAL_DEPS:\n\u001b[32m    408\u001b[39m         _load_global_deps()\n\u001b[32m--> \u001b[39m\u001b[32m409\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_C\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[32m    412\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mSymInt\u001b[39;00m:\n\u001b[32m    413\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    414\u001b[39m \u001b[33;03m    Like an int (including magic methods), but redirects all operations on the\u001b[39;00m\n\u001b[32m    415\u001b[39m \u001b[33;03m    wrapped node. This is used in particular to symbolically record operations\u001b[39;00m\n\u001b[32m    416\u001b[39m \u001b[33;03m    in the symbolic shape workflow.\u001b[39;00m\n\u001b[32m    417\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:463\u001b[39m, in \u001b[36m_lock_unlock_module\u001b[39m\u001b[34m(name)\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import pickle\n",
    "import os\n",
    "from collections import deque\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    \"\"\"Réseau de politique pour l'agent WindAwareNavigator.\"\"\"\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128, output_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "    \n",
    "    def get_action_probs(self, x):\n",
    "        \"\"\"Retourne les probabilités d'action après softmax.\"\"\"\n",
    "        logits = self.forward(x)\n",
    "        return F.softmax(logits, dim=-1)\n",
    "    \n",
    "    def get_action(self, x, deterministic=False):\n",
    "        \"\"\"\n",
    "        Retourne une action échantillonnée selon la politique.\n",
    "        Si deterministic=True, retourne l'action la plus probable.\n",
    "        \"\"\"\n",
    "        probs = self.get_action_probs(x)\n",
    "        if deterministic:\n",
    "            return torch.argmax(probs).item()\n",
    "        else:\n",
    "            dist = torch.distributions.Categorical(probs)\n",
    "            return dist.sample().item()\n",
    "\n",
    "class ValueNetwork(nn.Module):\n",
    "    \"\"\"Réseau de valeur (critique) pour l'agent WindAwareNavigator.\"\"\"\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "class EnhancedPPOTrainer:\n",
    "    \"\"\"\n",
    "    Entraîneur PPO amélioré pour le WindAwareNavigator.\n",
    "    Utilise PyTorch et implémente des fonctionnalités avancées.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                agent,\n",
    "                initial_windfields=['training_1', 'training_2', 'training_3'],\n",
    "                learning_rate=0.0003,       # Taux d'apprentissage ajusté\n",
    "                gamma=0.99,\n",
    "                clip_ratio=0.2,\n",
    "                value_coef=0.5,            # Coefficient pour la perte de valeur\n",
    "                entropy_coef=0.01,         # Coefficient pour la perte d'entropie\n",
    "                batch_size=20,\n",
    "                mini_batch_size=64,        # Taille des mini-batchs pour l'optimisation\n",
    "                epochs=4,\n",
    "                max_episodes=1000,\n",
    "                model_save_path='models/enhanced_ppo_navigator.pkl',\n",
    "                device=None):              # Périphérique pour calculs PyTorch\n",
    "        \"\"\"\n",
    "        Initialise l'entraîneur PPO amélioré.\n",
    "        \"\"\"\n",
    "        self.agent = agent\n",
    "        self.initial_windfields = initial_windfields\n",
    "        self.learning_rate = learning_rate\n",
    "        self.gamma = gamma\n",
    "        self.clip_ratio = clip_ratio\n",
    "        self.value_coef = value_coef\n",
    "        self.entropy_coef = entropy_coef\n",
    "        self.batch_size = batch_size\n",
    "        self.mini_batch_size = mini_batch_size\n",
    "        self.epochs = epochs\n",
    "        self.max_episodes = max_episodes\n",
    "        self.model_save_path = model_save_path\n",
    "        \n",
    "        # Déterminer le périphérique (CPU ou GPU)\n",
    "        self.device = device if device else ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"Utilisation de {self.device} pour l'entraînement\")\n",
    "        \n",
    "        # Variables pour la normalisation des états\n",
    "        self.max_observed_velocity = 2.0\n",
    "        self.max_observed_wind = 5.0\n",
    "        \n",
    "        # Dimensions d'entrée pour l'état amélioré\n",
    "        # Position (2), Vitesse (2), Vent local (2), Direction vers objectif (2), \n",
    "        # Angle relatif au vent (1), Angle relatif à l'objectif (1), Distance à l'objectif normalisée (1)\n",
    "        self.input_dim = 11\n",
    "        \n",
    "        # Créer les réseaux d'acteur et de critique\n",
    "        self.policy_net = PolicyNetwork(self.input_dim, 9).to(self.device)  # 9 actions possibles\n",
    "        self.value_net = ValueNetwork(self.input_dim).to(self.device)\n",
    "        \n",
    "        # Optimiseurs\n",
    "        self.policy_optimizer = optim.Adam(self.policy_net.parameters(), lr=learning_rate)\n",
    "        self.value_optimizer = optim.Adam(self.value_net.parameters(), lr=learning_rate)\n",
    "        \n",
    "        # Variables de suivi de l'entraînement\n",
    "        self.episode_rewards = []\n",
    "        self.episode_lengths = []\n",
    "        self.success_rates = []\n",
    "        self.evaluation_scores = []\n",
    "        self.efficiency_scores = []\n",
    "        \n",
    "        # Créer le répertoire pour sauvegarder les modèles si nécessaire\n",
    "        os.makedirs(os.path.dirname(model_save_path), exist_ok=True)\n",
    "        \n",
    "        # Buffer pour l'expérience\n",
    "        self.reset_buffer()\n",
    "    \n",
    "    def reset_buffer(self):\n",
    "        \"\"\"Réinitialise le buffer d'expérience.\"\"\"\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.values = []\n",
    "        self.dones = []\n",
    "        self.log_probs = []\n",
    "        self.next_states = []\n",
    "    \n",
    "    def create_efficient_environment(self, initial_windfield_name):\n",
    "        \"\"\"\n",
    "        Crée un environnement amélioré qui encourage l'efficacité du parcours\n",
    "        avec une fonction de récompense améliorée.\n",
    "        \"\"\"\n",
    "        from src.initial_windfields import get_initial_windfield\n",
    "        from src.env_sailing import SailingEnv\n",
    "        \n",
    "        class EnhancedSailingEnv(SailingEnv):\n",
    "            \"\"\"Version modifiée avec une fonction de récompense améliorée.\"\"\"\n",
    "            \n",
    "            def __init__(self, *args, **kwargs):\n",
    "                super().__init__(*args, **kwargs)\n",
    "                self.last_distance = None\n",
    "                self.step_penalty = -0.01  # Pénalité par étape, réduite\n",
    "                self.cumulative_distance = 0.0\n",
    "                self.last_position = None\n",
    "                self.optimal_path_length = None\n",
    "                self.starting_position = None\n",
    "                self.steps_taken = 0\n",
    "                self.max_allowed_steps = 200  # Limite pour éviter les trajets trop longs\n",
    "            \n",
    "            def reset(self, *args, **kwargs):\n",
    "                observation, info = super().reset(*args, **kwargs)\n",
    "                self.last_distance = np.linalg.norm(self.position - self.goal_position)\n",
    "                self.cumulative_distance = 0.0\n",
    "                self.last_position = self.position.copy()\n",
    "                self.starting_position = self.position.copy()\n",
    "                self.steps_taken = 0\n",
    "                \n",
    "                # Estimer une longueur de chemin optimale (en tenant compte qu'on ne peut pas aller en ligne droite)\n",
    "                self.optimal_path_length = np.linalg.norm(self.position - self.goal_position) * 1.3\n",
    "                \n",
    "                return observation, info\n",
    "            \n",
    "            def step(self, action):\n",
    "                # Augmenter le compteur d'étapes\n",
    "                self.steps_taken += 1\n",
    "                \n",
    "                # Position avant le mouvement\n",
    "                old_position = self.position.copy()\n",
    "                \n",
    "                # Faire le pas normal\n",
    "                observation, reward, terminated, truncated, info = super().step(action)\n",
    "                \n",
    "                # Calculer la nouvelle distance au but\n",
    "                current_distance = np.linalg.norm(self.position - self.goal_position)\n",
    "                \n",
    "                # Distance parcourue dans ce pas\n",
    "                step_distance = np.linalg.norm(self.position - old_position)\n",
    "                self.cumulative_distance += step_distance\n",
    "                \n",
    "                # Calculer la récompense modifiée avec des incitations plus sophistiquées\n",
    "                if reward > 0:  # Si l'objectif est atteint\n",
    "                    # Récompense basée sur l'efficacité du trajet\n",
    "                    direct_distance = np.linalg.norm(self.starting_position - self.goal_position)\n",
    "                    path_efficiency = direct_distance / max(self.cumulative_distance, direct_distance)\n",
    "                    \n",
    "                    # Bonus pour l'efficacité (1 pour parfait, 0 pour très inefficace)\n",
    "                    efficiency_bonus = 100.0 * path_efficiency\n",
    "                    \n",
    "                    # Bonus pour avoir atteint l'objectif rapidement\n",
    "                    time_bonus = max(0, 50.0 * (1.0 - self.steps_taken / self.max_allowed_steps))\n",
    "                    \n",
    "                    # Combiner les récompenses\n",
    "                    modified_reward = reward + efficiency_bonus + time_bonus\n",
    "                    \n",
    "                    # Ajouter l'information d'efficacité\n",
    "                    info['efficiency'] = path_efficiency\n",
    "                    info['path_length'] = self.cumulative_distance\n",
    "                    info['direct_distance'] = direct_distance\n",
    "                else:\n",
    "                    # ---------- RÉCOMPENSE AMÉLIORÉE ----------\n",
    "                    # 1. Récompense pour se rapprocher de l'objectif\n",
    "                    approach_reward = (self.last_distance - current_distance) * 1.0\n",
    "                    \n",
    "                    # 2. Obtenir des informations sur le vent actuel\n",
    "                    current_wind = self._get_wind_at_position(old_position)\n",
    "                    wind_strength = np.linalg.norm(current_wind)\n",
    "                    wind_direction = current_wind / (wind_strength + 1e-10)\n",
    "                    \n",
    "                    # 3. Calculer le vecteur vers l'objectif\n",
    "                    goal_vector = self.goal_position - old_position\n",
    "                    goal_distance = np.linalg.norm(goal_vector)\n",
    "                    goal_direction = goal_vector / (goal_distance + 1e-10)\n",
    "                    \n",
    "                    # 4. Vecteur du mouvement actuel\n",
    "                    movement_vector = self.position - old_position\n",
    "                    movement_distance = np.linalg.norm(movement_vector)\n",
    "                    \n",
    "                    # 5. Calculer des récompenses supplémentaires seulement si le bateau s'est déplacé\n",
    "                    direction_reward = 0\n",
    "                    wind_utilization_reward = 0\n",
    "                    progress_reward = 0\n",
    "                    \n",
    "                    if movement_distance > 0.001:\n",
    "                        movement_direction = movement_vector / movement_distance\n",
    "                        \n",
    "                        # 5.1. Récompense pour naviguer dans la direction de l'objectif\n",
    "                        alignment_with_goal = np.dot(movement_direction, goal_direction)\n",
    "                        direction_reward = alignment_with_goal * 0.3\n",
    "                        \n",
    "                        # 5.2. Récompense pour l'utilisation efficace du vent\n",
    "                        # Calculer l'angle entre la direction du vent et la direction du mouvement\n",
    "                        # Pour simuler l'efficacité de la voile à différents angles\n",
    "                        wind_from = -wind_direction  # Direction d'où vient le vent\n",
    "                        angle_with_wind = np.arccos(np.clip(np.dot(wind_from, movement_direction), -1.0, 1.0))\n",
    "                        \n",
    "                        # Efficacité selon l'angle par rapport au vent\n",
    "                        # Maximum d'efficacité autour de 90-120 degrés (au près)\n",
    "                        wind_angle_rad = angle_with_wind\n",
    "                        if wind_angle_rad < np.pi/6:  # Moins de 30 degrés (contre le vent)\n",
    "                            sailing_efficiency = 0.1\n",
    "                        elif wind_angle_rad < np.pi/3:  # Entre 30 et 60 degrés\n",
    "                            sailing_efficiency = 0.5 + (wind_angle_rad - np.pi/6) / (np.pi/6) * 0.3\n",
    "                        elif wind_angle_rad < 2*np.pi/3:  # Entre 60 et 120 degrés (optimal)\n",
    "                            sailing_efficiency = 0.8 + (wind_angle_rad - np.pi/3) / (np.pi/3) * 0.2\n",
    "                        elif wind_angle_rad < np.pi:  # Entre 120 et 180 degrés\n",
    "                            sailing_efficiency = 1.0 - (wind_angle_rad - 2*np.pi/3) / (np.pi/3) * 0.3\n",
    "                        else:  # Plus de 180 degrés (vent arrière)\n",
    "                            sailing_efficiency = 0.7\n",
    "                        \n",
    "                        # Récompense pour l'utilisation efficace du vent\n",
    "                        wind_utilization_reward = sailing_efficiency * wind_strength * 0.2\n",
    "                        \n",
    "                        # 5.3. Récompense pour un progrès global vers l'objectif\n",
    "                        initial_distance = np.linalg.norm(self.starting_position - self.goal_position)\n",
    "                        current_progress = 1.0 - (current_distance / initial_distance)\n",
    "                        progress_reward = current_progress * 0.1  # Petit bonus pour le progrès global\n",
    "                    \n",
    "                    # 6. Pénalité pour chaque étape pour encourager la rapidité\n",
    "                    # Pénalité progressive qui augmente avec le nombre d'étapes\n",
    "                    step_penalty = self.step_penalty * (1.0 + self.steps_taken / self.max_allowed_steps)\n",
    "                    \n",
    "                    # 7. Combiner toutes les récompenses\n",
    "                    modified_reward = approach_reward + direction_reward + wind_utilization_reward + progress_reward + step_penalty\n",
    "                    \n",
    "                    # 8. Pénalité forte si trop d'étapes pour éviter les boucles infinies\n",
    "                    if self.steps_taken >= self.max_allowed_steps:\n",
    "                        modified_reward -= 10.0\n",
    "                        truncated = True\n",
    "                    \n",
    "                    # Mettre à jour la dernière distance et position\n",
    "                    self.last_distance = current_distance\n",
    "                    self.last_position = self.position.copy()\n",
    "                \n",
    "                return observation, modified_reward, terminated, truncated, info\n",
    "        \n",
    "        initial_windfield = get_initial_windfield(initial_windfield_name)\n",
    "        env = EnhancedSailingEnv(\n",
    "            wind_init_params=initial_windfield['wind_init_params'],\n",
    "            wind_evol_params=initial_windfield['wind_evol_params'],\n",
    "            render_mode=None\n",
    "        )\n",
    "        return env\n",
    "    \n",
    "    def preprocess_state(self, observation):\n",
    "        \"\"\"\n",
    "        Prétraite l'observation pour créer un état amélioré avec des\n",
    "        caractéristiques dérivées.\n",
    "        \"\"\"\n",
    "        # Extraire les composantes de base\n",
    "        position = np.array([observation[0], observation[1]])\n",
    "        velocity = np.array([observation[2], observation[3]])\n",
    "        wind = np.array([observation[4], observation[5]])\n",
    "        \n",
    "        # Récupérer la position de l'objectif\n",
    "        # Note: Dans un vrai scénario, vous pourriez la récupérer de l'environnement\n",
    "        # Ici, nous supposons qu'elle est dans le coin supérieur droit de la grille\n",
    "        goal_position = np.array([self.agent.grid_size[0] - 1, self.agent.grid_size[1] - 1])\n",
    "        \n",
    "        # Calcul des caractéristiques dérivées\n",
    "        # 1. Distance à l'objectif\n",
    "        goal_vector = goal_position - position\n",
    "        distance_to_goal = np.linalg.norm(goal_vector)\n",
    "        normalized_distance = min(1.0, distance_to_goal / np.linalg.norm(self.agent.grid_size))\n",
    "        \n",
    "        # 2. Direction normalisée vers l'objectif\n",
    "        goal_direction = goal_vector / (distance_to_goal + 1e-10)\n",
    "        \n",
    "        # 3. Calcul des angles relatifs\n",
    "        velocity_norm = np.linalg.norm(velocity)\n",
    "        wind_norm = np.linalg.norm(wind)\n",
    "        \n",
    "        # Direction du bateau (basée sur la vitesse s'il se déplace, sinon sur la direction vers l'objectif)\n",
    "        boat_direction = velocity / (velocity_norm + 1e-10) if velocity_norm > 0.001 else goal_direction\n",
    "        \n",
    "        # Angles relatifs (normalisés entre 0 et 1)\n",
    "        wind_direction = wind / (wind_norm + 1e-10) if wind_norm > 0.001 else np.zeros(2)\n",
    "        wind_from = -wind_direction  # Direction d'où vient le vent\n",
    "        \n",
    "        wind_rel_angle = np.arccos(np.clip(np.dot(boat_direction, wind_from), -1.0, 1.0)) / np.pi\n",
    "        goal_rel_angle = np.arccos(np.clip(np.dot(boat_direction, goal_direction), -1.0, 1.0)) / np.pi\n",
    "        \n",
    "        # Normaliser les entrées de base\n",
    "        pos_norm = position / np.array(self.agent.grid_size)\n",
    "        vel_norm = velocity / self.max_observed_velocity\n",
    "        wind_norm = wind / self.max_observed_wind\n",
    "        \n",
    "        # Créer l'état amélioré\n",
    "        state = np.concatenate([\n",
    "            pos_norm,              # Position normalisée (2)\n",
    "            vel_norm,              # Vitesse normalisée (2)\n",
    "            wind_norm,             # Vent local normalisé (2)\n",
    "            goal_direction,        # Direction vers l'objectif (2)\n",
    "            [wind_rel_angle],      # Angle relatif au vent (1)\n",
    "            [goal_rel_angle],      # Angle relatif à l'objectif (1)\n",
    "            [normalized_distance]  # Distance normalisée à l'objectif (1)\n",
    "        ])\n",
    "        \n",
    "        return state\n",
    "    \n",
    "    def collect_experience(self, num_episodes):\n",
    "        \"\"\"\n",
    "        Collecte des expériences en exécutant l'agent dans l'environnement.\n",
    "        Stocke les états, actions, récompenses et log-probabilités pour PPO.\n",
    "        \"\"\"\n",
    "        # Réinitialiser les buffers\n",
    "        self.reset_buffer()\n",
    "        \n",
    "        episode_rewards_buffer = []\n",
    "        episode_lengths_buffer = []\n",
    "        successes = 0\n",
    "        efficiency_scores = []\n",
    "        \n",
    "        for _ in range(num_episodes):\n",
    "            # Choisir aléatoirement une configuration de vent\n",
    "            initial_windfield_name = np.random.choice(self.initial_windfields)\n",
    "            env = self.create_efficient_environment(initial_windfield_name)\n",
    "            \n",
    "            # Réinitialiser l'environnement et l'agent\n",
    "            observation, _ = env.reset(seed=np.random.randint(0, 1000))\n",
    "            self.agent.reset()\n",
    "            \n",
    "            episode_reward = 0\n",
    "            episode_efficiency = 0\n",
    "            done = False\n",
    "            truncated = False\n",
    "            \n",
    "            for step in range(500):  # Limite élevée pour les scénarios difficiles\n",
    "                # Prétraiter l'état\n",
    "                state = self.preprocess_state(observation)\n",
    "                state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "                \n",
    "                # Obtenir l'action selon la politique actuelle\n",
    "                with torch.no_grad():\n",
    "                    action_probs = self.policy_net.get_action_probs(state_tensor)\n",
    "                    action = torch.multinomial(action_probs, 1).item()\n",
    "                    action_log_prob = torch.log(action_probs[0, action] + 1e-10)\n",
    "                    value = self.value_net(state_tensor)\n",
    "                \n",
    "                # Exécuter l'action\n",
    "                next_observation, reward, done, truncated, info = env.step(action)\n",
    "                \n",
    "                # Prétraiter le prochain état\n",
    "                next_state = self.preprocess_state(next_observation)\n",
    "                \n",
    "                # Stocker l'expérience\n",
    "                self.states.append(state)\n",
    "                self.actions.append(action)\n",
    "                self.rewards.append(reward)\n",
    "                self.values.append(value.item())\n",
    "                self.dones.append(done or truncated)\n",
    "                self.log_probs.append(action_log_prob.item())\n",
    "                self.next_states.append(next_state)\n",
    "                \n",
    "                episode_reward += reward\n",
    "                observation = next_observation\n",
    "                \n",
    "                if done or truncated:\n",
    "                    # Stocker l'efficacité si disponible\n",
    "                    if 'efficiency' in info:\n",
    "                        episode_efficiency = info['efficiency']\n",
    "                    break\n",
    "            \n",
    "            # Mettre à jour les métriques\n",
    "            episode_rewards_buffer.append(episode_reward)\n",
    "            episode_lengths_buffer.append(step + 1)\n",
    "            if done and not truncated:  # Succès uniquement si terminé normalement (atteinte de l'objectif)\n",
    "                successes += 1\n",
    "                efficiency_scores.append(episode_efficiency)\n",
    "            \n",
    "        # Calculer les statistiques\n",
    "        self.episode_rewards.append(np.mean(episode_rewards_buffer))\n",
    "        self.episode_lengths.append(np.mean(episode_lengths_buffer))\n",
    "        success_rate = successes / num_episodes\n",
    "        self.success_rates.append(success_rate)\n",
    "        \n",
    "        # Calculer l'efficacité moyenne des trajets réussis\n",
    "        if efficiency_scores:\n",
    "            avg_efficiency = np.mean(efficiency_scores)\n",
    "            self.efficiency_scores.append(avg_efficiency)\n",
    "            print(f\"Efficacité moyenne des trajets réussis: {avg_efficiency:.2%}\")\n",
    "        else:\n",
    "            self.efficiency_scores.append(0.0)\n",
    "            print(\"Aucun trajet réussi, efficacité = 0\")\n",
    "        \n",
    "        return success_rate\n",
    "    \n",
    "    def compute_returns_and_advantages(self):\n",
    "        \"\"\"\n",
    "        Calcule les retours (discounted returns) et les avantages pour PPO.\n",
    "        \n",
    "        Returns:\n",
    "            returns: Tensor des retours pour chaque état\n",
    "            advantages: Tensor des avantages pour chaque état\n",
    "        \"\"\"\n",
    "        returns = []\n",
    "        advantages = []\n",
    "        gae = 0  # Avantage généralisé estimé\n",
    "        \n",
    "        # Fonctionnement par épisodes indépendants\n",
    "        episode_ends = [i for i, done in enumerate(self.dones) if done]\n",
    "        if not episode_ends or episode_ends[-1] != len(self.dones) - 1:\n",
    "            episode_ends.append(len(self.dones) - 1)\n",
    "        \n",
    "        start_idx = 0\n",
    "        for end_idx in episode_ends:\n",
    "            # Calculer les retours et avantages pour cet épisode\n",
    "            next_value = 0  # Valeur de l'état final (0 si l'épisode est terminé)\n",
    "            \n",
    "            # GAE (Generalized Advantage Estimation)\n",
    "            episode_returns = []\n",
    "            episode_advantages = []\n",
    "            \n",
    "            for t in range(end_idx, start_idx - 1, -1):\n",
    "                next_val = next_value if t == end_idx else self.values[t + 1]\n",
    "                delta = self.rewards[t] + self.gamma * next_val * (1 - self.dones[t]) - self.values[t]\n",
    "                gae = delta + self.gamma * 0.95 * (1 - self.dones[t]) * gae  # 0.95 est le paramètre lambda pour GAE\n",
    "                \n",
    "                # Insérer au début pour maintenir l'ordre chronologique\n",
    "                episode_advantages.insert(0, gae)\n",
    "                episode_returns.insert(0, gae + self.values[t])\n",
    "            \n",
    "            returns.extend(episode_returns)\n",
    "            advantages.extend(episode_advantages)\n",
    "            \n",
    "            # Passer à l'épisode suivant\n",
    "            start_idx = end_idx + 1\n",
    "            gae = 0\n",
    "        \n",
    "        # Convertir en tensors PyTorch\n",
    "        returns_tensor = torch.FloatTensor(returns).to(self.device)\n",
    "        advantages_tensor = torch.FloatTensor(advantages).to(self.device)\n",
    "        \n",
    "        # Normaliser les avantages (importante pour la stabilité)\n",
    "        advantages_tensor = (advantages_tensor - advantages_tensor.mean()) / (advantages_tensor.std() + 1e-8)\n",
    "        \n",
    "        return returns_tensor, advantages_tensor\n",
    "    \n",
    "    def update_policy_ppo(self):\n",
    "        \"\"\"\n",
    "        Met à jour les réseaux de politique et de valeur selon l'algorithme PPO.\n",
    "        \"\"\"\n",
    "        returns, advantages = self.compute_returns_and_advantages()\n",
    "        \n",
    "        # Convertir les listes en tensors\n",
    "        old_states = torch.FloatTensor(self.states).to(self.device)\n",
    "        old_actions = torch.LongTensor(self.actions).to(self.device)\n",
    "        old_log_probs = torch.FloatTensor(self.log_probs).to(self.device)\n",
    "        \n",
    "        # Effectuer plusieurs passes d'optimisation sur le même batch\n",
    "        for _ in range(self.epochs):\n",
    "            # Parcourir les données par mini-batchs\n",
    "            indices = torch.randperm(len(old_states))\n",
    "            \n",
    "            for start_idx in range(0, len(indices), self.mini_batch_size):\n",
    "                end_idx = min(start_idx + self.mini_batch_size, len(indices))\n",
    "                batch_indices = indices[start_idx:end_idx]\n",
    "                \n",
    "                # Extraire les données du mini-batch\n",
    "                batch_states = old_states[batch_indices]\n",
    "                batch_actions = old_actions[batch_indices]\n",
    "                batch_advantages = advantages[batch_indices]\n",
    "                batch_returns = returns[batch_indices]\n",
    "                batch_old_log_probs = old_log_probs[batch_indices]\n",
    "                \n",
    "                # Évaluer la politique et la valeur actuelles\n",
    "                action_probs = self.policy_net.get_action_probs(batch_states)\n",
    "                batch_new_log_probs = torch.log(torch.gather(action_probs, 1, batch_actions.unsqueeze(1)) + 1e-10).squeeze(1)\n",
    "                batch_values = self.value_net(batch_states).squeeze(1)\n",
    "                \n",
    "                # Calculer l'entropie pour favoriser l'exploration\n",
    "                entropy = -(action_probs * torch.log(action_probs + 1e-10)).sum(dim=1).mean()\n",
    "                \n",
    "                # Ratio pour PPO\n",
    "                ratio = torch.exp(batch_new_log_probs - batch_old_log_probs)\n",
    "                \n",
    "                # Calcul des deux termes de la fonction objectif PPO\n",
    "                surr1 = ratio * batch_advantages\n",
    "                surr2 = torch.clamp(ratio, 1.0 - self.clip_ratio, 1.0 + self.clip_ratio) * batch_advantages\n",
    "                \n",
    "                # Perte de politique (négative car on veut maximiser)\n",
    "                policy_loss = -torch.min(surr1, surr2).mean()\n",
    "                \n",
    "                # Perte de valeur\n",
    "                value_loss = F.mse_loss(batch_values, batch_returns)\n",
    "                \n",
    "                # Perte combinée\n",
    "                total_loss = policy_loss + self.value_coef * value_loss - self.entropy_coef * entropy\n",
    "                \n",
    "                # Mise à jour des réseaux\n",
    "                self.policy_optimizer.zero_grad()\n",
    "                self.value_optimizer.zero_grad()\n",
    "                total_loss.backward()\n",
    "                \n",
    "                # Clip gradient norm pour éviter les explosions de gradient\n",
    "                torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), 0.5)\n",
    "                torch.nn.utils.clip_grad_norm_(self.value_net.parameters(), 0.5)\n",
    "                \n",
    "                self.policy_optimizer.step()\n",
    "                self.value_optimizer.step()\n",
    "    \n",
    "    def evaluate_agent(self, num_seeds=10, render=False):\n",
    "        \"\"\"\n",
    "        Évalue les performances de l'agent sur l'efficacité des trajectoires.\n",
    "        \n",
    "        Args:\n",
    "            num_seeds: Nombre de graines pour l'évaluation\n",
    "            render: Activer le rendu pour visualiser\n",
    "        \n",
    "        Returns:\n",
    "            avg_score: Score combinant succès et efficacité\n",
    "        \"\"\"\n",
    "        from src.evaluation import evaluate_agent\n",
    "        from src.initial_windfields import get_initial_windfield\n",
    "        \n",
    "        success_rates = []\n",
    "        efficiency_scores = []\n",
    "        path_lengths = []\n",
    "        \n",
    "        # Modifier l'agent pour utiliser le réseau de politique\n",
    "        original_act = self.agent.act\n",
    "        \n",
    "        def pytorch_act(observation):\n",
    "            state = self.preprocess_state(observation)\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                # Utiliser l'action la plus probable (mode déterministe)\n",
    "                return self.policy_net.get_action(state_tensor, deterministic=True)\n",
    "        \n",
    "        # Remplacer temporairement la méthode d'action\n",
    "        self.agent.act = pytorch_act\n",
    "        \n",
    "        try:\n",
    "            for initial_windfield_name in self.initial_windfields:\n",
    "                # Créer l'environnement\n",
    "                initial_windfield = get_initial_windfield(initial_windfield_name)\n",
    "                \n",
    "                # Évaluer l'agent\n",
    "                results = evaluate_agent(\n",
    "                    agent=self.agent,\n",
    "                    initial_windfield=initial_windfield,\n",
    "                    seeds=list(range(num_seeds)),\n",
    "                    max_horizon=200,\n",
    "                    verbose=False,\n",
    "                    render=render,\n",
    "                    full_trajectory=True  # Pour obtenir les positions\n",
    "                )\n",
    "                \n",
    "                success_rates.append(results['success_rate'])\n",
    "                \n",
    "                # Calculer l'efficacité moyenne du chemin\n",
    "                if results.get('positions') and results['success_rate'] > 0:\n",
    "                    # Pour chaque trajectoire réussie, calculer l'efficacité\n",
    "                    path_efficiency = []\n",
    "                    successful_path_lengths = []\n",
    "                    \n",
    "                    for seed_idx, positions in enumerate(results.get('positions', [])):\n",
    "                        if positions and results.get('success', [])[seed_idx]:\n",
    "                            # Calculer la longueur du chemin\n",
    "                            path_length = 0\n",
    "                            for i in range(1, len(positions)):\n",
    "                                path_length += np.linalg.norm(np.array(positions[i]) - np.array(positions[i-1]))\n",
    "                            \n",
    "                            # Distance directe\n",
    "                            direct_distance = np.linalg.norm(np.array(positions[0]) - np.array(positions[-1]))\n",
    "                            \n",
    "                            # Efficacité = distance directe / longueur réelle du chemin\n",
    "                            efficiency = direct_distance / (path_length + 1e-10)\n",
    "                            efficiency = min(1.0, efficiency)  # Plafonner à 1.0\n",
    "                            \n",
    "                            path_efficiency.append(efficiency)\n",
    "                            successful_path_lengths.append(path_length)\n",
    "                    \n",
    "                    if path_efficiency:\n",
    "                        avg_path_efficiency = np.mean(path_efficiency)\n",
    "                        avg_path_length = np.mean(successful_path_lengths)\n",
    "                        efficiency_scores.append(avg_path_efficiency)\n",
    "                        path_lengths.append(avg_path_length)\n",
    "                else:\n",
    "                    efficiency_scores.append(0.0)\n",
    "                    path_lengths.append(float('inf'))\n",
    "        \n",
    "            # Calculer les scores moyens\n",
    "            avg_success_rate = np.mean(success_rates)\n",
    "            avg_efficiency = np.mean(efficiency_scores) if efficiency_scores else 0.0\n",
    "            avg_path_length = np.mean([length for length in path_lengths if length < float('inf')])\n",
    "            \n",
    "            # Score combiné: 60% succès, 40% efficacité\n",
    "            combined_score = 0.6 * avg_success_rate + 0.4 * avg_efficiency\n",
    "            \n",
    "            self.evaluation_scores.append(combined_score)\n",
    "            \n",
    "            print(f\"Taux de succès: {avg_success_rate:.2%}\")\n",
    "            print(f\"Efficacité moyenne: {avg_efficiency:.2%}\")\n",
    "            print(f\"Longueur moyenne des chemins réussis: {avg_path_length:.2f}\")\n",
    "            print(f\"Score combiné: {combined_score:.2%}\")\n",
    "            \n",
    "            return combined_score\n",
    "        \n",
    "        finally:\n",
    "            # Restaurer la méthode d'action originale\n",
    "            self.agent.act = original_act\n",
    "    \n",
    "    def save_agent(self):\n",
    "        \"\"\"Sauvegarde l'agent entraîné.\"\"\"\n",
    "        torch.save({\n",
    "            'policy_state_dict': self.policy_net.state_dict(),\n",
    "            'value_state_dict': self.value_net.state_dict(),\n",
    "            'policy_optimizer': self.policy_optimizer.state_dict(),\n",
    "            'value_optimizer': self.value_optimizer.state_dict(),\n",
    "            'training_stats': {\n",
    "                'episode_rewards': self.episode_rewards,\n",
    "                'episode_lengths': self.episode_lengths,\n",
    "                'success_rates': self.success_rates,\n",
    "                'evaluation_scores': self.evaluation_scores,\n",
    "                'efficiency_scores': self.efficiency_scores\n",
    "            }\n",
    "        }, self.model_save_path)\n",
    "        print(f\"Agent sauvegardé dans {self.model_save_path}\")\n",
    "    \n",
    "    def load_agent(self):\n",
    "        \"\"\"Charge un agent préalablement entraîné.\"\"\"\n",
    "        try:\n",
    "            checkpoint = torch.load(self.model_save_path, map_location=self.device)\n",
    "            self.policy_net.load_state_dict(checkpoint['policy_state_dict'])\n",
    "            self.value_net.load_state_dict(checkpoint['value_state_dict'])\n",
    "            self.policy_optimizer.load_state_dict(checkpoint['policy_optimizer'])\n",
    "            self.value_optimizer.load_state_dict(checkpoint['value_optimizer'])\n",
    "            \n",
    "            if 'training_stats' in checkpoint:\n",
    "                stats = checkpoint['training_stats']\n",
    "                self.episode_rewards = stats.get('episode_rewards', [])\n",
    "                self.episode_lengths = stats.get('episode_lengths', [])\n",
    "                self.success_rates = stats.get('success_rates', [])\n",
    "                self.evaluation_scores = stats.get('evaluation_scores', [])\n",
    "                self.efficiency_scores = stats.get('efficiency_scores', [])\n",
    "            \n",
    "            print(f\"Agent chargé depuis {self.model_save_path}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Impossible de charger l'agent depuis {self.model_save_path}: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def train(self, max_episodes=None):\n",
    "        \"\"\"\n",
    "        Exécute le processus complet d'entraînement avec PPO.\n",
    "        \n",
    "        Args:\n",
    "            max_episodes: Nombre maximal d'épisodes (remplace self.max_episodes si spécifié)\n",
    "        \n",
    "        Returns:\n",
    "            agent: L'agent entraîné\n",
    "        \"\"\"\n",
    "        if max_episodes is not None:\n",
    "            self.max_episodes = max_episodes\n",
    "            \n",
    "        print(\"Démarrage de l'entraînement PPO amélioré pour le WindAwareNavigator...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Évaluer l'agent avant l'entraînement\n",
    "        print(\"Évaluation initiale de l'agent...\")\n",
    "        initial_score = self.evaluate_agent()\n",
    "        print(f\"Score initial: {initial_score:.2%}\")\n",
    "        \n",
    "        # Boucle principale d'entraînement\n",
    "        best_score = initial_score\n",
    "        episodes_without_improvement = 0\n",
    "        \n",
    "        for episode in range(0, self.max_episodes, self.batch_size):\n",
    "            # Afficher la progression\n",
    "            print(f\"\\nÉpisodes {episode}-{episode + self.batch_size - 1}/{self.max_episodes}\")\n",
    "            \n",
    "            # Collecter des expériences\n",
    "            print(\"Collecte d'expériences...\")\n",
    "            success_rate = self.collect_experience(self.batch_size)\n",
    "            print(f\"Taux de succès du batch: {success_rate:.2%}\")\n",
    "            \n",
    "            # Mettre à jour la politique avec PPO\n",
    "            print(\"Mise à jour de la politique avec PPO...\")\n",
    "            self.update_policy_ppo()\n",
    "            \n",
    "            # Évaluer périodiquement l'agent\n",
    "            if (episode // self.batch_size) % 5 == 0 or episode + self.batch_size >= self.max_episodes:\n",
    "                print(\"Évaluation de l'agent...\")\n",
    "                eval_score = self.evaluate_agent()\n",
    "                print(f\"Score d'évaluation: {eval_score:.2%}\")\n",
    "                \n",
    "                # Sauvegarder le meilleur modèle\n",
    "                if eval_score > best_score:\n",
    "                    best_score = eval_score\n",
    "                    self.save_agent()\n",
    "                    episodes_without_improvement = 0\n",
    "                    print(f\"Nouveau meilleur score: {best_score:.2%}\")\n",
    "                else:\n",
    "                    episodes_without_improvement += self.batch_size\n",
    "            \n",
    "            # Arrêter si pas d'amélioration pendant longtemps\n",
    "            if episodes_without_improvement >= 50:\n",
    "                print(\"Pas d'amélioration depuis 50 épisodes. Arrêt de l'entraînement.\")\n",
    "                break\n",
    "            \n",
    "            # Afficher les statistiques\n",
    "            print(f\"Récompense moyenne: {self.episode_rewards[-1]:.2f}\")\n",
    "            print(f\"Longueur moyenne d'épisode: {self.episode_lengths[-1]:.1f}\")\n",
    "            if self.efficiency_scores:\n",
    "                print(f\"Efficacité moyenne: {self.efficiency_scores[-1]:.2%}\")\n",
    "        \n",
    "        # Durée totale d'entraînement\n",
    "        training_time = time.time() - start_time\n",
    "        print(f\"\\nEntraînement terminé en {training_time:.2f} secondes!\")\n",
    "        print(f\"Meilleur score: {best_score:.2%}\")\n",
    "        print(f\"Amélioration: {best_score - initial_score:.2%}\")\n",
    "        \n",
    "        # Charger le meilleur modèle\n",
    "        self.load_agent()\n",
    "        \n",
    "        return self.agent\n",
    "    \n",
    "    def visualize_agent_trajectory(self, initial_windfield_name, seed=0, max_steps=200):\n",
    "        \"\"\"\n",
    "        Visualise la trajectoire de l'agent sur un champ de vent spécifique.\n",
    "        \n",
    "        Args:\n",
    "            initial_windfield_name: Nom du champ de vent initial\n",
    "            seed: Graine pour la reproductibilité\n",
    "            max_steps: Nombre maximal d'étapes\n",
    "        \"\"\"\n",
    "        from src.initial_windfields import get_initial_windfield\n",
    "        import matplotlib.pyplot as plt\n",
    "        from matplotlib.patches import Circle\n",
    "        \n",
    "        # Créer l'environnement\n",
    "        initial_windfield = get_initial_windfield(initial_windfield_name)\n",
    "        env = self.create_efficient_environment(initial_windfield_name)\n",
    "        env.render_mode = 'human'  # Activer le rendu\n",
    "        \n",
    "        # Réinitialiser l'environnement\n",
    "        observation, _ = env.reset(seed=seed)\n",
    "        \n",
    "        # Modifier temporairement l'agent pour utiliser le réseau de politique\n",
    "        original_act = self.agent.act\n",
    "        \n",
    "        def pytorch_act(observation):\n",
    "            state = self.preprocess_state(observation)\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                # Mode déterministe pour visualisation\n",
    "                return self.policy_net.get_action(state_tensor, deterministic=True)\n",
    "        \n",
    "        self.agent.act = pytorch_act\n",
    "        \n",
    "        try:\n",
    "            # Exécuter l'agent et collecter les positions\n",
    "            positions = [env.position.copy()]\n",
    "            actions = []\n",
    "            rewards = []\n",
    "            \n",
    "            done = False\n",
    "            truncated = False\n",
    "            total_reward = 0\n",
    "            \n",
    "            for _ in range(max_steps):\n",
    "                # Prendre une action\n",
    "                action = self.agent.act(observation)\n",
    "                actions.append(action)\n",
    "                \n",
    "                # Exécuter l'action\n",
    "                observation, reward, done, truncated, _ = env.step(action)\n",
    "                rewards.append(reward)\n",
    "                total_reward += reward\n",
    "                \n",
    "                # Enregistrer la position\n",
    "                positions.append(env.position.copy())\n",
    "                \n",
    "                if done or truncated:\n",
    "                    break\n",
    "            \n",
    "            positions = np.array(positions)\n",
    "            \n",
    "            # Créer la visualisation\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            \n",
    "            # Tracer la grille\n",
    "            plt.xlim(0, self.agent.grid_size[0])\n",
    "            plt.ylim(0, self.agent.grid_size[1])\n",
    "            plt.grid(True)\n",
    "            \n",
    "            # Tracer la trajectoire\n",
    "            plt.plot(positions[:, 0], positions[:, 1], 'b-', linewidth=2, label='Trajectoire')\n",
    "            plt.scatter(positions[0, 0], positions[0, 1], c='g', s=100, label='Départ')\n",
    "            plt.scatter(positions[-1, 0], positions[-1, 1], c='r', s=100, label='Arrivée')\n",
    "            \n",
    "            # Tracer le champ de vent (simplifié)\n",
    "            grid_x = np.linspace(0, self.agent.grid_size[0], 10)\n",
    "            grid_y = np.linspace(0, self.agent.grid_size[1], 10)\n",
    "            X, Y = np.meshgrid(grid_x, grid_y)\n",
    "            \n",
    "            # Obtenir le vent à chaque point de la grille\n",
    "            U = np.zeros_like(X)\n",
    "            V = np.zeros_like(Y)\n",
    "            \n",
    "            for i in range(len(grid_x)):\n",
    "                for j in range(len(grid_y)):\n",
    "                    # Supposons que nous avons accès à la méthode _get_wind_at_position\n",
    "                    wind = env._get_wind_at_position(np.array([X[j, i], Y[j, i]]))\n",
    "                    U[j, i] = wind[0]\n",
    "                    V[j, i] = wind[1]\n",
    "            \n",
    "            # Normaliser les vecteurs de vent pour une meilleure visualisation\n",
    "            wind_magnitude = np.sqrt(U**2 + V**2)\n",
    "            max_magnitude = np.max(wind_magnitude)\n",
    "            U = U / max_magnitude\n",
    "            V = V / max_magnitude\n",
    "            \n",
    "            plt.quiver(X, Y, U, V, scale=10, color='cyan', alpha=0.6, label='Vent')\n",
    "            \n",
    "            # Ajouter un titre et une légende\n",
    "            success_status = \"Succès\" if done and not truncated else \"Échec\"\n",
    "            plt.title(f\"Trajectoire de l'agent ({success_status}, Récompense: {total_reward:.2f})\")\n",
    "            plt.legend()\n",
    "            \n",
    "            plt.show()\n",
    "            \n",
    "            return positions, actions, rewards\n",
    "        \n",
    "        finally:\n",
    "            # Restaurer la méthode d'action originale\n",
    "            self.agent.act = original_act\n",
    "    \n",
    "    def plot_training_results(self):\n",
    "        \"\"\"Affiche les métriques d'entraînement sous forme de graphiques.\"\"\"\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(12, 15))\n",
    "        \n",
    "        # Métriques disponibles\n",
    "        x = range(len(self.episode_rewards))\n",
    "        \n",
    "        # Graphique des récompenses\n",
    "        ax1.plot(x, self.episode_rewards, 'b-', label='Récompense par batch')\n",
    "        ax1.set_title('Récompense moyenne par batch')\n",
    "        ax1.set_xlabel('Batch')\n",
    "        ax1.set_ylabel('Récompense')\n",
    "        ax1.grid(True)\n",
    "        \n",
    "        # Graphique des longueurs d'épisodes\n",
    "        ax2.plot(x, self.episode_lengths, 'g-', label='Longueur d\\'épisode')\n",
    "        ax2.set_title('Longueur moyenne des épisodes')\n",
    "        ax2.set_xlabel('Batch')\n",
    "        ax2.set_ylabel('Étapes')\n",
    "        ax2.grid(True)\n",
    "        \n",
    "        # Graphique du taux de succès, score d'évaluation et efficacité\n",
    "        ax3.plot(x, self.success_rates, 'r-', label='Taux de succès')\n",
    "        \n",
    "        if self.evaluation_scores:\n",
    "            eval_x = np.linspace(0, len(self.success_rates)-1, len(self.evaluation_scores))\n",
    "            ax3.plot(eval_x, self.evaluation_scores, 'm-', label='Score d\\'évaluation')\n",
    "        \n",
    "        if self.efficiency_scores:\n",
    "            eff_x = np.linspace(0, len(self.success_rates)-1, len(self.efficiency_scores))\n",
    "            ax3.plot(eff_x, self.efficiency_scores, 'c-', label='Efficacité')\n",
    "        \n",
    "        ax3.set_title('Taux de succès, scores d\\'évaluation et efficacité')\n",
    "        ax3.set_xlabel('Batch')\n",
    "        ax3.set_ylabel('Taux')\n",
    "        ax3.set_ylim(0, 1.05)\n",
    "        ax3.legend()\n",
    "        ax3.grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "class WindAwareNavigatorPyTorch:\n",
    "    \"\"\"\n",
    "    Version PyTorch de l'agent WindAwareNavigator.\n",
    "    Utilise les réseaux de politique et de valeur pour naviguer.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, grid_size=(10, 10), ppo_trainer=None):\n",
    "        \"\"\"\n",
    "        Initialise l'agent.\n",
    "        \n",
    "        Args:\n",
    "            grid_size: Dimensions de la grille de navigation\n",
    "            ppo_trainer: Entraîneur PPO contenant les réseaux entraînés\n",
    "        \"\"\"\n",
    "        self.grid_size = grid_size\n",
    "        self.ppo_trainer = ppo_trainer\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        \n",
    "        # Prétraitement et extraction de features\n",
    "        self.max_observed_velocity = 2.0\n",
    "        self.max_observed_wind = 5.0\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Réinitialise l'état de l'agent.\"\"\"\n",
    "        pass  # Rien à réinitialiser pour cette implémentation\n",
    "    \n",
    "    def preprocess_state(self, observation):\n",
    "        \"\"\"\n",
    "        Prétraite l'observation pour extraire des features améliorées.\n",
    "        Identique à la méthode de l'entraîneur PPO.\n",
    "        \"\"\"\n",
    "        # Extraire les composantes de base\n",
    "        position = np.array([observation[0], observation[1]])\n",
    "        velocity = np.array([observation[2], observation[3]])\n",
    "        wind = np.array([observation[4], observation[5]])\n",
    "        \n",
    "        # Position de l'objectif (généralement coin supérieur droit)\n",
    "        goal_position = np.array([self.grid_size[0] - 1, self.grid_size[1] - 1])\n",
    "        \n",
    "        # Calcul des caractéristiques dérivées\n",
    "        goal_vector = goal_position - position\n",
    "        distance_to_goal = np.linalg.norm(goal_vector)\n",
    "        normalized_distance = min(1.0, distance_to_goal / np.linalg.norm(self.grid_size))\n",
    "        \n",
    "        goal_direction = goal_vector / (distance_to_goal + 1e-10)\n",
    "        \n",
    "        velocity_norm = np.linalg.norm(velocity)\n",
    "        wind_norm = np.linalg.norm(wind)\n",
    "        \n",
    "        boat_direction = velocity / (velocity_norm + 1e-10) if velocity_norm > 0.001 else goal_direction\n",
    "        \n",
    "        wind_direction = wind / (wind_norm + 1e-10) if wind_norm > 0.001 else np.zeros(2)\n",
    "        wind_from = -wind_direction\n",
    "        \n",
    "        wind_rel_angle = np.arccos(np.clip(np.dot(boat_direction, wind_from), -1.0, 1.0)) / np.pi\n",
    "        goal_rel_angle = np.arccos(np.clip(np.dot(boat_direction, goal_direction), -1.0, 1.0)) / np.pi\n",
    "        \n",
    "        pos_norm = position / np.array(self.grid_size)\n",
    "        vel_norm = velocity / self.max_observed_velocity\n",
    "        wind_norm = wind / self.max_observed_wind\n",
    "        \n",
    "        state = np.concatenate([\n",
    "            pos_norm,\n",
    "            vel_norm,\n",
    "            wind_norm,\n",
    "            goal_direction,\n",
    "            [wind_rel_angle],\n",
    "            [goal_rel_angle],\n",
    "            [normalized_distance]\n",
    "        ])\n",
    "        \n",
    "        return state\n",
    "    \n",
    "    def action_to_direction(self, action):\n",
    "        \"\"\"\n",
    "        Convertit une action en vecteur de direction.\n",
    "        \n",
    "        Args:\n",
    "            action: Indice de l'action (0-8)\n",
    "        \n",
    "        Returns:\n",
    "            direction: Vecteur 2D de direction\n",
    "        \"\"\"\n",
    "        # Actions 0-7: mouvement dans les 8 directions\n",
    "        # Action 8: rester en place\n",
    "        directions = [\n",
    "            np.array([1, 0]),    # Droite\n",
    "            np.array([1, 1]),    # Diagonale haut-droite\n",
    "            np.array([0, 1]),    # Haut\n",
    "            np.array([-1, 1]),   # Diagonale haut-gauche\n",
    "            np.array([-1, 0]),   # Gauche\n",
    "            np.array([-1, -1]),  # Diagonale bas-gauche\n",
    "            np.array([0, -1]),   # Bas\n",
    "            np.array([1, -1]),   # Diagonale bas-droite\n",
    "            np.array([0, 0])     # Rester en place\n",
    "        ]\n",
    "        \n",
    "        return directions[action]\n",
    "    \n",
    "    def act(self, observation):\n",
    "        \"\"\"\n",
    "        Détermine l'action à prendre en fonction de l'observation.\n",
    "        \n",
    "        Args:\n",
    "            observation: État actuel de l'environnement\n",
    "        \n",
    "        Returns:\n",
    "            action: Indice de l'action à prendre\n",
    "        \"\"\"\n",
    "        if self.ppo_trainer is None:\n",
    "            # Si pas d'entraîneur PPO, utiliser une heuristique simple\n",
    "            position = np.array([observation[0], observation[1]])\n",
    "            goal_position = np.array([self.grid_size[0] - 1, self.grid_size[1] - 1])\n",
    "            \n",
    "            # Aller vers l'objectif\n",
    "            goal_vector = goal_position - position\n",
    "            \n",
    "            # Déterminer la direction la plus proche\n",
    "            directions = [self.action_to_direction(i) for i in range(9)]\n",
    "            \n",
    "            if np.linalg.norm(goal_vector) < 0.1:\n",
    "                return 8  # Rester en place si proche de l'objectif\n",
    "            \n",
    "            # Choisir la direction la plus alignée avec le vecteur objectif\n",
    "            best_action = 0\n",
    "            best_alignment = -float('inf')\n",
    "            \n",
    "            for i, direction in enumerate(directions[:-1]):  # Exclure \"rester en place\"\n",
    "                if np.linalg.norm(direction) > 0:\n",
    "                    alignment = np.dot(direction, goal_vector) / (np.linalg.norm(direction) * np.linalg.norm(goal_vector))\n",
    "                    \n",
    "                    if alignment > best_alignment:\n",
    "                        # Vérifier si le mouvement est valide (dans les limites de la grille)\n",
    "                        next_pos = position + direction\n",
    "                        if 0 <= next_pos[0] < self.grid_size[0] and 0 <= next_pos[1] < self.grid_size[1]:\n",
    "                            best_alignment = alignment\n",
    "                            best_action = i\n",
    "            \n",
    "            return best_action\n",
    "        else:\n",
    "            # Utiliser le réseau de politique entraîné\n",
    "            state = self.preprocess_state(observation)\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                return self.ppo_trainer.policy_net.get_action(state_tensor, deterministic=True)\n",
    "    \n",
    "    def calculate_sailing_efficiency(self, boat_direction, wind_direction):\n",
    "        \"\"\"\n",
    "        Calcule l'efficacité de navigation en fonction de l'angle entre la direction du bateau et le vent.\n",
    "        \n",
    "        Args:\n",
    "            boat_direction: Direction normalisée du bateau\n",
    "            wind_direction: Direction normalisée du vent\n",
    "        \n",
    "        Returns:\n",
    "            efficiency: Efficacité de navigation (0.0 à 1.0)\n",
    "        \"\"\"\n",
    "        # Direction d'où vient le vent\n",
    "        wind_from = -wind_direction\n",
    "        \n",
    "        # Angle entre la direction du bateau et la direction d'où vient le vent\n",
    "        angle = np.arccos(np.clip(np.dot(boat_direction, wind_from), -1.0, 1.0))\n",
    "        \n",
    "        # Efficacité selon l'angle\n",
    "        if angle < np.pi/6:  # Moins de 30 degrés (contre le vent)\n",
    "            return 0.1\n",
    "        elif angle < np.pi/3:  # Entre 30 et 60 degrés\n",
    "            return 0.5 + (angle - np.pi/6) / (np.pi/6) * 0.3\n",
    "        elif angle < 2*np.pi/3:  # Entre 60 et 120 degrés (optimal)\n",
    "            return 0.8 + (angle - np.pi/3) / (np.pi/3) * 0.2\n",
    "        elif angle < np.pi:  # Entre 120 et 180 degrés\n",
    "            return 1.0 - (angle - 2*np.pi/3) / (np.pi/3) * 0.3\n",
    "        else:  # Plus de 180 degrés (impossible en pratique)\n",
    "            return 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utilisation de cpu pour l'entraînement\n",
      "Évaluation initiale...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'WindAwareNavigatorPyTorch' object has no attribute 'seed'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[120]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Évaluation initiale\u001b[39;00m\n\u001b[32m     25\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mÉvaluation initiale...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m initial_score = \u001b[43mppo_trainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevaluate_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# Entraînement\u001b[39;00m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# Vous pouvez ajuster le nombre d'épisodes selon vos besoins\u001b[39;00m\n\u001b[32m     30\u001b[39m num_episodes = \u001b[32m100\u001b[39m  \u001b[38;5;66;03m# Ajustez ce nombre pour un entraînement plus ou moins long\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[118]\u001b[39m\u001b[32m, line 589\u001b[39m, in \u001b[36mEnhancedPPOTrainer.evaluate_agent\u001b[39m\u001b[34m(self, num_seeds, render)\u001b[39m\n\u001b[32m    586\u001b[39m initial_windfield = get_initial_windfield(initial_windfield_name)\n\u001b[32m    588\u001b[39m \u001b[38;5;66;03m# Évaluer l'agent\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m results = \u001b[43mevaluate_agent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    590\u001b[39m \u001b[43m    \u001b[49m\u001b[43magent\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[43m    \u001b[49m\u001b[43minitial_windfield\u001b[49m\u001b[43m=\u001b[49m\u001b[43minitial_windfield\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    592\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseeds\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnum_seeds\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    593\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_horizon\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m200\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    594\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    595\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrender\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrender\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    596\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfull_trajectory\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Pour obtenir les positions\u001b[39;49;00m\n\u001b[32m    597\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    599\u001b[39m success_rates.append(results[\u001b[33m'\u001b[39m\u001b[33msuccess_rate\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m    601\u001b[39m \u001b[38;5;66;03m# Calculer l'efficacité moyenne du chemin\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/RL_project_sailing/src/evaluation.py:77\u001b[39m, in \u001b[36mevaluate_agent\u001b[39m\u001b[34m(agent, initial_windfield, seeds, max_horizon, verbose, render, full_trajectory)\u001b[39m\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m seed \u001b[38;5;129;01min\u001b[39;00m iterator:\n\u001b[32m     75\u001b[39m     \u001b[38;5;66;03m# Reset environment and agent\u001b[39;00m\n\u001b[32m     76\u001b[39m     env.seed(seed)\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m     \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mseed\u001b[49m(seed)\n\u001b[32m     78\u001b[39m     observation, _ = env.reset(seed=seed)  \u001b[38;5;66;03m# Explicitly pass seed to reset\u001b[39;00m\n\u001b[32m     79\u001b[39m     agent.reset()\n",
      "\u001b[31mAttributeError\u001b[39m: 'WindAwareNavigatorPyTorch' object has no attribute 'seed'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#from enhanced_ppo_trainer import EnhancedPPOTrainer, WindAwareNavigatorPyTorch\n",
    "\n",
    "# Créer votre agent avec PyTorch\n",
    "agent = WindAwareNavigatorPyTorch(grid_size=(10, 10))\n",
    "\n",
    "# Initialiser l'entraîneur PPO amélioré\n",
    "ppo_trainer = EnhancedPPOTrainer(\n",
    "    agent=agent,\n",
    "    initial_windfields=['training_1', 'training_2', 'training_3'],\n",
    "    learning_rate=0.0003,\n",
    "    gamma=0.99,\n",
    "    clip_ratio=0.2,\n",
    "    value_coef=0.5,\n",
    "    entropy_coef=0.01,\n",
    "    batch_size=20,\n",
    "    mini_batch_size=64,\n",
    "    epochs=4,\n",
    "    max_episodes=500,\n",
    "    model_save_path=\"models/enhanced_ppo_navigator.pkl\"\n",
    ")\n",
    "\n",
    "# Évaluation initiale\n",
    "print(\"Évaluation initiale...\")\n",
    "initial_score = ppo_trainer.evaluate_agent()\n",
    "\n",
    "# Entraînement\n",
    "# Vous pouvez ajuster le nombre d'épisodes selon vos besoins\n",
    "num_episodes = 100  # Ajustez ce nombre pour un entraînement plus ou moins long\n",
    "print(f\"\\nDémarrage de l'entraînement PPO amélioré pour {num_episodes} épisodes...\")\n",
    "\n",
    "# Option 1: Entraînement manuel par étapes\n",
    "for episode in range(0, num_episodes, ppo_trainer.batch_size):\n",
    "    print(f\"\\nÉpisodes {episode}-{episode + ppo_trainer.batch_size - 1}/{num_episodes}\")\n",
    "    \n",
    "    # Collecter des expériences\n",
    "    print(\"Collecte d'expériences...\")\n",
    "    success_rate = ppo_trainer.collect_experience(ppo_trainer.batch_size)\n",
    "    print(f\"Taux de succès du batch: {success_rate:.2%}\")\n",
    "    \n",
    "    # Mettre à jour la politique avec PPO\n",
    "    print(\"Mise à jour de la politique avec PPO...\")\n",
    "    ppo_trainer.update_policy_ppo()\n",
    "    \n",
    "    # Évaluer périodiquement\n",
    "    if (episode // ppo_trainer.batch_size) % 3 == 0 or episode + ppo_trainer.batch_size >= num_episodes:\n",
    "        print(\"Évaluation de l'agent...\")\n",
    "        eval_score = ppo_trainer.evaluate_agent()\n",
    "        print(f\"Score d'évaluation: {eval_score:.2%}\")\n",
    "        \n",
    "        # Sauvegarder l'agent\n",
    "        ppo_trainer.save_agent()\n",
    "\n",
    "# Option 2: Utiliser la méthode train() pour un entraînement automatique\n",
    "# agent = ppo_trainer.train(max_episodes=num_episodes)\n",
    "\n",
    "# Évaluation finale\n",
    "print(\"\\nÉvaluation finale après entraînement...\")\n",
    "final_score = ppo_trainer.evaluate_agent()\n",
    "print(f\"\\nAmélioration: {final_score - initial_score:.2%}\")\n",
    "\n",
    "# Visualiser une trajectoire\n",
    "print(\"\\nVisualisation d'une trajectoire...\")\n",
    "ppo_trainer.visualize_agent_trajectory('training_1', seed=0)\n",
    "\n",
    "# Afficher les résultats d'entraînement\n",
    "ppo_trainer.plot_training_results()\n",
    "\n",
    "# Remarque: Pour utiliser un agent déjà entraîné\n",
    "# Pour charger un modèle entraîné:\n",
    "# ppo_trainer.load_agent()\n",
    "# \n",
    "# # Ensuite, pour créer un agent qui utilise le modèle entraîné:\n",
    "# optimized_agent = WindAwareNavigatorPyTorch(grid_size=(10, 10), ppo_trainer=ppo_trainer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
