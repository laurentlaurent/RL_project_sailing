{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Designing Agents for the Sailing Challenge\n",
    "\n",
    "In this notebook, we'll explore how to design and implement agents for the Sailing Challenge. We'll cover:\n",
    "\n",
    "1. The requirements and interface for valid agents\n",
    "2. Understanding the greedy agent example \n",
    "3. Implementing a simple reinforcement learning agent\n",
    "\n",
    "By the end of this notebook, you'll have a clear understanding of how to create your own agents that can navigate the sailing environment effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Requirements\n",
    "\n",
    "All agents in the Sailing Challenge must implement a specific interface defined by the `BaseAgent` abstract class. Let's examine this class to understand what's required:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Add the src directory to the path\n",
    "sys.path.append(os.path.abspath('../src'))\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "# Import the BaseAgent class\n",
    "from src.agents.base_agent import BaseAgent\n",
    "\n",
    "# Display the BaseAgent class documentation\n",
    "#help(BaseAgent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required Methods\n",
    "\n",
    "As we can see from the `BaseAgent` class, any valid agent must implement:\n",
    "\n",
    "1. **`act(observation)`**: The core decision-making method that takes the current observation and returns an action\n",
    "   - Input: A numpy array containing [x, y, vx, vy, wx, wy, ...] representing position, velocity, wind, and the full wind field\n",
    "   - Output: An integer in the range [0-8] representing the action to take\n",
    "\n",
    "2. **`reset()`**: Resets the agent's internal state at the beginning of each episode\n",
    "   - This is particularly important for agents that maintain memory or state across steps\n",
    "\n",
    "3. **`seed(seed)`**: Sets the random seed for the agent to ensure reproducibility\n",
    "   - This is crucial for evaluation and comparison of different agents\n",
    "\n",
    "Additionally, while not strictly required, implementing `save()` and `load()` methods is recommended for storing and retrieving trained agent parameters.\n",
    "\n",
    "### The Validation Process\n",
    "\n",
    "When you submit an agent, it will be automatically validated against these requirements. The validation process checks:\n",
    "\n",
    "1. If the agent class inherits from `BaseAgent`\n",
    "2. If all required methods are implemented with correct parameters\n",
    "3. If the agent produces valid actions (integers in range [0-8])\n",
    "4. If the agent can interact with the environment without errors\n",
    "\n",
    "Let's create a minimal valid agent to understand this process better:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinimalAgent(BaseAgent):\n",
    "    \"\"\"A minimal valid agent that meets all interface requirements.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.np_random = np.random.default_rng()\n",
    "    \n",
    "    def act(self, observation: np.ndarray) -> int:\n",
    "        \"\"\"Choose an action randomly.\"\"\"\n",
    "        return self.np_random.integers(0, 9)  # Random action from 0-8\n",
    "    \n",
    "    def reset(self) -> None:\n",
    "        \"\"\"Reset the agent.\"\"\"\n",
    "        pass  # Nothing to reset in this simple agent\n",
    "    \n",
    "    def seed(self, seed: int = None) -> None:\n",
    "        \"\"\"Set the random seed.\"\"\"\n",
    "        self.np_random = np.random.default_rng(seed)\n",
    "\n",
    "# Create an instance of our minimal agent\n",
    "minimal_agent = MinimalAgent()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Agent's Validity\n",
    "\n",
    "Let's make the agent do a few steps to check that everything is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running the minimal agent for 5 steps:\n",
      "Step 1: Action=0, Position=[16  1], Reward=0.0\n",
      "Step 2: Action=6, Position=[15  1], Reward=0.0\n",
      "Step 3: Action=5, Position=[14  0], Reward=0.0\n",
      "Step 4: Action=3, Position=[15  0], Reward=0.0\n",
      "Step 5: Action=3, Position=[16  0], Reward=0.0\n"
     ]
    }
   ],
   "source": [
    "# Instead of validating the agent here, we'll just demonstrate it on a simple task\n",
    "from src.env_sailing import SailingEnv\n",
    "\n",
    "# Create a simple environment\n",
    "env = SailingEnv()\n",
    "observation, info = env.reset(seed=42)\n",
    "\n",
    "# Initialize our minimal agent\n",
    "minimal_agent = MinimalAgent()\n",
    "minimal_agent.seed(42)\n",
    "\n",
    "# Run the agent for a few steps\n",
    "print(\"Running the minimal agent for 5 steps:\")\n",
    "for i in range(5):\n",
    "    action = minimal_agent.act(observation)\n",
    "    observation, reward, done, truncated, info = env.step(action)\n",
    "    print(f\"Step {i+1}: Action={action}, Position={info['position']}, Reward={reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validating Your Agent\n",
    "\n",
    "After creating your agent, you'll want to ensure it meets all the requirements of the challenge. There are two ways to validate your agent:\n",
    "\n",
    "1. **Using the `validate_agent.ipynb` notebook:**\n",
    "   - This notebook provides a comprehensive interface for testing your agent\n",
    "   - It shows detailed validation results and explains any issues\n",
    "\n",
    "2. **Using the command line:**\n",
    "   ```bash\n",
    "   cd src\n",
    "   python test_agent_validity.py path/to/your_agent.py\n",
    "   ```\n",
    "\n",
    "We recommend using these tools after you've completed your agent implementation rather than trying to validate it during development.\n",
    "\n",
    "For now, let's focus on understanding agent design principles and implementing effective strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Observation and Action Space\n",
    "\n",
    "To design effective agents, it's important to understand:\n",
    "\n",
    "1. **What information is available to the agent (observations)**\n",
    "2. **What actions the agent can take**\n",
    "\n",
    "### Observation Space\n",
    "\n",
    "The observation provided to your agent is a numpy array with the following structure:\n",
    "\n",
    "`[x, y, vx, vy, wx, wy, flattened_wind_field]`\n",
    "\n",
    "\n",
    "Where:\n",
    "- `x, y`: Current position (grid coordinates)\n",
    "- `vx, vy`: Current velocity vector \n",
    "- `wx, wy`: Wind vector at the current position\n",
    "- `flattened_wind_field`: The entire wind field (can be reshaped to grid_size × grid_size × 2)\n",
    "\n",
    "For simpler agents, you might only need to use the first 6 values. More sophisticated agents can use the full wind field to plan ahead.\n",
    "\n",
    "### Action Space\n",
    "\n",
    "The agent can choose from 9 possible actions:\n",
    "\n",
    "- 0: Move North (up)\n",
    "- 1: Move Northeast\n",
    "- 2: Move East (right)\n",
    "- 3: Move Southeast\n",
    "- 4: Move South (down)\n",
    "- 5: Move Southwest\n",
    "- 6: Move West (left)\n",
    "- 7: Move Northwest\n",
    "- 8: Stay in place\n",
    "\n",
    "Each action represents a desired direction for the boat to move. However, the actual movement will be influenced by the wind and sailing physics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Naive Agent Example\n",
    "\n",
    "Let's examine the built-in `NaiveAgent`, which provides a simple baseline implementation. This agent always tries to move North (toward the goal), regardless of wind conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class NaiveAgent(BaseAgent):\n",
      "    \"\"\"\n",
      "    A naive agent for the Sailing Challenge.\n",
      "    \n",
      "    This is a very simple agent that always chooses to go North,\n",
      "    regardless of wind conditions or position. It serves as a minimal\n",
      "    working example that students can build upon.\n",
      "    \"\"\"\n",
      "    \n",
      "    def __init__(self):\n",
      "        \"\"\"Initialize the agent.\"\"\"\n",
      "        super().__init__()\n",
      "        self.np_random = np.random.default_rng()\n",
      "    \n",
      "    def act(self, observation: np.ndarray) -> int:\n",
      "        \"\"\"\n",
      "        Select an action based on the current observation.\n",
      "        \n",
      "        Args:\n",
      "            observation: A numpy array containing the current observation.\n",
      "                Format: [x, y, vx, vy, wx, wy] where:\n",
      "                - (x, y) is the current position\n",
      "                - (vx, vy) is the current velocity\n",
      "                - (wx, wy) is the current wind vector\n",
      "        \n",
      "        Returns:\n",
      "            action: An integer in [0, 8] representing the action to take:\n",
      "                - 0: Move North\n",
      "                - 1: Move Northeast\n",
      "                - 2: Move East\n",
      "                - 3: Move Southeast\n",
      "                - 4: Move South\n",
      "                - 5: Move Southwest\n",
      "                - 6: Move West\n",
      "                - 7: Move Northwest\n",
      "                - 8: Stay in place\n",
      "        \"\"\"\n",
      "        # This agent always chooses to go North (action 0)\n",
      "        return 0\n",
      "    \n",
      "    def reset(self) -> None:\n",
      "        \"\"\"Reset the agent's internal state between episodes.\"\"\"\n",
      "        # Nothing to reset for this simple agent\n",
      "        pass\n",
      "        \n",
      "    def seed(self, seed: int = None) -> None:\n",
      "        \"\"\"Set the random seed for reproducibility.\"\"\"\n",
      "        self.np_random = np.random.default_rng(seed)\n",
      "        \n",
      "    def save(self, path: str) -> None:\n",
      "        \"\"\"\n",
      "        Save the agent's learned parameters to a file.\n",
      "        \n",
      "        Args:\n",
      "            path: Path to save the agent's state\n",
      "        \"\"\"\n",
      "        # No parameters to save for this simple agent\n",
      "        pass\n",
      "        \n",
      "    def load(self, path: str) -> None:\n",
      "        \"\"\"\n",
      "        Load the agent's learned parameters from a file.\n",
      "        \n",
      "        Args:\n",
      "            path: Path to load the agent's state from\n",
      "        \"\"\"\n",
      "        # No parameters to load for this simple agent\n",
      "        pass \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the NaiveAgent\n",
    "from src.agents.agent_naive import NaiveAgent\n",
    "\n",
    "# Display the source code\n",
    "import inspect\n",
    "print(inspect.getsource(NaiveAgent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of the Naive Agent\n",
    "\n",
    "The `NaiveAgent` is extremely simple but illustrates the key requirements for a valid agent:\n",
    "\n",
    "1. **Inheritance**: It inherits from `BaseAgent`\n",
    "2. **Required Methods**: It implements all required methods (`act`, `reset`, `seed`)\n",
    "3. **Action Selection**: It always returns action `0` (North)\n",
    "4. **Simplicity**: It maintains no internal state and requires no complex logic\n",
    "\n",
    "This agent provides a good baseline, but it has obvious limitations:\n",
    "\n",
    "- It ignores wind conditions completely\n",
    "- It will struggle when the wind is coming from the North\n",
    "- It doesn't adapt its strategy based on the environment\n",
    "\n",
    "Let's test the naive agent to see how well it performs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running the naive agent on the simple_static initial windfield:\n",
      "Step 10: Position=[16  7], Reward=0.0\n",
      "Step 20: Position=[16 12], Reward=0.0\n",
      "Step 30: Position=[16 13], Reward=0.0\n",
      "Step 40: Position=[16 13], Reward=0.0\n",
      "Step 50: Position=[16 14], Reward=0.0\n",
      "Step 60: Position=[16 14], Reward=0.0\n",
      "Step 70: Position=[16 15], Reward=0.0\n",
      "Step 80: Position=[16 15], Reward=0.0\n",
      "Step 90: Position=[16 16], Reward=0.0\n",
      "Step 100: Position=[16 16], Reward=0.0\n",
      "Step 110: Position=[16 17], Reward=0.0\n",
      "Step 120: Position=[16 18], Reward=0.0\n",
      "Step 130: Position=[16 18], Reward=0.0\n",
      "Step 140: Position=[16 19], Reward=0.0\n",
      "Step 150: Position=[16 19], Reward=0.0\n",
      "Step 160: Position=[16 20], Reward=0.0\n",
      "Step 170: Position=[16 20], Reward=0.0\n",
      "Step 180: Position=[16 21], Reward=0.0\n",
      "Step 190: Position=[16 21], Reward=0.0\n",
      "Step 200: Position=[16 22], Reward=0.0\n",
      "Step 210: Position=[16 22], Reward=0.0\n",
      "Step 220: Position=[16 23], Reward=0.0\n",
      "Step 230: Position=[16 24], Reward=0.0\n",
      "Step 240: Position=[16 24], Reward=0.0\n",
      "Step 250: Position=[16 25], Reward=0.0\n",
      "Step 260: Position=[16 25], Reward=0.0\n",
      "Step 270: Position=[16 26], Reward=0.0\n",
      "Step 280: Position=[16 27], Reward=0.0\n",
      "Step 290: Position=[16 27], Reward=0.0\n",
      "Step 300: Position=[16 29], Reward=0.0\n",
      "\n",
      "Episode finished after 301 steps with reward: 100.0\n",
      "Final position: [16 30]\n",
      "Goal reached: True\n"
     ]
    }
   ],
   "source": [
    "from src.env_sailing import SailingEnv\n",
    "from src.initial_windfields import get_initial_windfield\n",
    "\n",
    "# Create an environment with a simple test initial windfield\n",
    "env = SailingEnv(**get_initial_windfield('simple_static'))\n",
    "naive_agent = NaiveAgent()\n",
    "\n",
    "# Run a single episode\n",
    "observation, info = env.reset(seed=42)\n",
    "total_reward = 0\n",
    "done = False\n",
    "truncated = False\n",
    "step_count = 0\n",
    "\n",
    "print(\"Running the naive agent on the simple_static initial windfield:\")\n",
    "while not (done or truncated) and step_count < 1000:  # Limit to 100 steps\n",
    "    action = naive_agent.act(observation)\n",
    "    observation, reward, done, truncated, info = env.step(action)\n",
    "    total_reward += reward\n",
    "    step_count += 1\n",
    "    \n",
    "    # Print every 10 steps to avoid too much output\n",
    "    if step_count % 10 == 0:\n",
    "        print(f\"Step {step_count}: Position={info['position']}, Reward={reward}\")\n",
    "\n",
    "print(f\"\\nEpisode finished after {step_count} steps with reward: {total_reward}\")\n",
    "print(f\"Final position: {info['position']}\")\n",
    "print(f\"Goal reached: {done}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving on the Naive Agent\n",
    "\n",
    "The naive agent provides a good starting point, but there are many ways to improve it:\n",
    "\n",
    "1. **Wind-Aware Agent**: Consider wind direction when choosing actions\n",
    "2. **Goal-Directed Agent**: Calculate the direction to the goal and choose actions accordingly\n",
    "3. **Physics-Based Agent**: Use sailing physics equations to determine the most efficient action\n",
    "\n",
    "The key insight for sailing is that certain directions relative to the wind are more efficient than others:\n",
    "\n",
    "- The sailing efficiency is highest when moving perpendicular to the wind (beam reach)\n",
    "- It's difficult to sail directly into the wind (the \"no-go zone\" - less than 45° to the wind)\n",
    "- The boat maintains momentum (inertia) between steps\n",
    "\n",
    "Before diving into reinforcement learning, consider implementing a simple rule-based agent that incorporates these physics principles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amélioration d'un agent naive avec le vent l'objectif et les équations physiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedSailingAgent(BaseAgent):\n",
    "    \"\"\"\n",
    "    Un agent de voile amélioré qui utilise les principes de base de la physique de la navigation à voile.\n",
    "    Cet agent tient compte de:\n",
    "    1. La direction du vent\n",
    "    2. L'angle optimal par rapport au vent\n",
    "    3. La position de l'objectif\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.np_random = np.random.default_rng()\n",
    "        # Définir les angles optimaux pour la navigation\n",
    "        self.optimal_wind_angles = [90, 120]  # Angles en degrés (perpendiculaire et grand largue)\n",
    "        # La zone de non-navigation (no-go zone) est typiquement ±45° face au vent\n",
    "        self.no_go_angle = 45\n",
    "    \n",
    "    def act(self, observation: np.ndarray) -> int:\n",
    "        \"\"\"\n",
    "        Choisir une action en fonction des conditions de vent et de la position de l'objectif.\n",
    "        \n",
    "        Args:\n",
    "            observation: Un tableau numpy contenant l'observation actuelle.\n",
    "                Format: [x, y, vx, vy, wx, wy, ...] où:\n",
    "                - (x, y) est la position actuelle\n",
    "                - (vx, vy) est la vitesse actuelle\n",
    "                - (wx, wy) est le vecteur de vent actuel\n",
    "                - le reste contient le champ de vent complet\n",
    "        \n",
    "        Returns:\n",
    "            int: L'action choisie (0-8)\n",
    "        \"\"\"\n",
    "        # Extraire les informations pertinentes de l'observation\n",
    "        position = observation[:2]  # [x, y]\n",
    "        velocity = observation[2:4]  # [vx, vy]\n",
    "        wind = observation[4:6]  # [wx, wy]\n",
    "        \n",
    "        # Dans un environnement SailingEnv typique, l'objectif est en haut de la grille\n",
    "        # Nous supposons que l'objectif est à la position [16, 31] basé sur l'exemple\n",
    "        goal_position = np.array([16, 31])\n",
    "        \n",
    "        # Calculer le vecteur vers l'objectif\n",
    "        direction_to_goal = goal_position - position\n",
    "        distance_to_goal = np.linalg.norm(direction_to_goal)\n",
    "        \n",
    "        # Si nous sommes très proches de l'objectif, essayer de l'atteindre directement\n",
    "        if distance_to_goal < 2.0:\n",
    "            return self._get_action_towards_direction(direction_to_goal)\n",
    "        \n",
    "        # Calculer l'angle du vent (en degrés)\n",
    "        wind_angle = np.degrees(np.arctan2(wind[1], wind[0]))\n",
    "        \n",
    "        # Calculer l'angle vers l'objectif (en degrés)\n",
    "        goal_angle = np.degrees(np.arctan2(direction_to_goal[1], direction_to_goal[0]))\n",
    "        \n",
    "        # Angle relatif entre le vent et la direction vers l'objectif\n",
    "        relative_angle = (goal_angle - wind_angle) % 360\n",
    "        \n",
    "        # Vérifier si l'objectif est dans la zone de non-navigation\n",
    "        if relative_angle < self.no_go_angle or relative_angle > 360 - self.no_go_angle:\n",
    "            # L'objectif est face au vent, nous devons tacker (zigzaguer)\n",
    "            return self._tacking_strategy(wind_angle, position)\n",
    "        else:\n",
    "            # Nous pouvons naviguer plus directement vers l'objectif\n",
    "            # Mais ajustons notre angle pour optimiser la vitesse\n",
    "            return self._optimize_sailing_angle(wind_angle, goal_angle, direction_to_goal)\n",
    "    \n",
    "    def _get_action_towards_direction(self, direction):\n",
    "        \"\"\"Convertit un vecteur de direction en action discrète (0-8).\"\"\"\n",
    "        # Normaliser le vecteur de direction\n",
    "        if np.linalg.norm(direction) > 0:\n",
    "            direction = direction / np.linalg.norm(direction)\n",
    "        \n",
    "        # Cartographier les 8 directions aux actions\n",
    "        # [N, NE, E, SE, S, SW, W, NW]\n",
    "        actions_map = [\n",
    "            (0, 1),    # 0: Nord\n",
    "            (1, 1),    # 1: Nord-Est\n",
    "            (1, 0),    # 2: Est\n",
    "            (1, -1),   # 3: Sud-Est\n",
    "            (0, -1),   # 4: Sud\n",
    "            (-1, -1),  # 5: Sud-Ouest\n",
    "            (-1, 0),   # 6: Ouest\n",
    "            (-1, 1),   # 7: Nord-Ouest\n",
    "        ]\n",
    "        \n",
    "        # Calculer l'action qui correspond le mieux à la direction\n",
    "        best_action = 0\n",
    "        best_similarity = -float('inf')\n",
    "        \n",
    "        for i, action_dir in enumerate(actions_map):\n",
    "            similarity = direction[0] * action_dir[0] + direction[1] * action_dir[1]\n",
    "            if similarity > best_similarity:\n",
    "                best_similarity = similarity\n",
    "                best_action = i\n",
    "        \n",
    "        return best_action\n",
    "    \n",
    "    def _tacking_strategy(self, wind_angle, position):\n",
    "        \"\"\"\n",
    "        Implémente une stratégie de tacking (virement de bord) lorsque l'objectif est face au vent.\n",
    "        Alterne entre naviguer à environ 45-60° de chaque côté du vent.\n",
    "        \"\"\"\n",
    "        # Décider de quel côté virer en fonction de la position x\n",
    "        # Cela crée un zigzag naturel\n",
    "        if position[0] < 16:  # À gauche du centre, virer à tribord\n",
    "            tack_angle = wind_angle + 60\n",
    "        else:  # À droite du centre, virer à bâbord\n",
    "            tack_angle = wind_angle - 60\n",
    "        \n",
    "        # Convertir l'angle en vecteur de direction\n",
    "        tack_direction = np.array([np.cos(np.radians(tack_angle)), np.sin(np.radians(tack_angle))])\n",
    "        \n",
    "        # Obtenir l'action correspondante\n",
    "        return self._get_action_towards_direction(tack_direction)\n",
    "    \n",
    "    def _optimize_sailing_angle(self, wind_angle, goal_angle, direction_to_goal):\n",
    "        \"\"\"\n",
    "        Optimise l'angle de navigation pour maximiser la vitesse,\n",
    "        en se basant sur les angles optimaux par rapport au vent.\n",
    "        \"\"\"\n",
    "        # Trouver l'angle optimal le plus proche de notre direction vers l'objectif\n",
    "        best_angle = goal_angle  # Par défaut, viser directement l'objectif\n",
    "        \n",
    "        # Calculer les angles absolus optimaux (dans les deux sens)\n",
    "        optimal_angles = []\n",
    "        for optimal in self.optimal_wind_angles:\n",
    "            optimal_angles.append((wind_angle + optimal) % 360)\n",
    "            optimal_angles.append((wind_angle - optimal) % 360)\n",
    "        \n",
    "        # Trouver l'angle optimal le plus proche de notre direction vers l'objectif\n",
    "        angle_diff = [abs((angle - goal_angle) % 360) for angle in optimal_angles]\n",
    "        min_diff_index = np.argmin(angle_diff)\n",
    "        \n",
    "        # Si la différence d'angle est significative, utiliser l'angle optimal\n",
    "        if angle_diff[min_diff_index] < 45:\n",
    "            best_angle = optimal_angles[min_diff_index]\n",
    "        \n",
    "        # Convertir l'angle en vecteur de direction\n",
    "        optimal_direction = np.array([np.cos(np.radians(best_angle)), np.sin(np.radians(best_angle))])\n",
    "        \n",
    "        # Mélanger la direction optimale avec la direction vers l'objectif\n",
    "        # Cela permet de progresser vers l'objectif tout en maintenant une bonne vitesse\n",
    "        mixed_direction = 0.7 * optimal_direction + 0.3 * (direction_to_goal / np.linalg.norm(direction_to_goal))\n",
    "        \n",
    "        return self._get_action_towards_direction(mixed_direction)\n",
    "    \n",
    "    def reset(self) -> None:\n",
    "        \"\"\"Réinitialiser l'agent.\"\"\"\n",
    "        pass  # Rien à réinitialiser pour cet agent simple\n",
    "    \n",
    "    def seed(self, seed: int = None) -> None:\n",
    "        \"\"\"Définir la graine aléatoire.\"\"\"\n",
    "        self.np_random = np.random.default_rng(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running the naive agent on the simple_static initial windfield:\n",
      "Step 10: Position=[ 6 11], Reward=0.0\n",
      "Step 20: Position=[ 6 11], Reward=0.0\n",
      "Step 30: Position=[ 7 12], Reward=0.0\n",
      "Step 40: Position=[ 8 13], Reward=0.0\n",
      "Step 50: Position=[ 8 13], Reward=0.0\n",
      "Step 60: Position=[ 8 15], Reward=0.0\n",
      "Step 70: Position=[ 7 17], Reward=0.0\n",
      "Step 80: Position=[ 7 17], Reward=0.0\n",
      "Step 90: Position=[ 8 18], Reward=0.0\n",
      "Step 100: Position=[ 9 19], Reward=0.0\n",
      "Step 110: Position=[ 9 19], Reward=0.0\n",
      "Step 120: Position=[10 20], Reward=0.0\n",
      "Step 130: Position=[10 20], Reward=0.0\n",
      "Step 140: Position=[11 21], Reward=0.0\n",
      "Step 150: Position=[11 21], Reward=0.0\n",
      "Step 160: Position=[11 23], Reward=0.0\n",
      "Step 170: Position=[11 24], Reward=0.0\n",
      "Step 180: Position=[12 24], Reward=0.0\n",
      "Step 190: Position=[13 25], Reward=0.0\n",
      "Step 200: Position=[13 25], Reward=0.0\n",
      "Step 210: Position=[13 27], Reward=0.0\n",
      "Step 220: Position=[13 27], Reward=0.0\n",
      "Step 230: Position=[14 28], Reward=0.0\n",
      "Step 240: Position=[14 29], Reward=0.0\n",
      "Step 250: Position=[14 30], Reward=0.0\n",
      "Step 260: Position=[14 31], Reward=0.0\n",
      "\n",
      "Episode finished after 265 steps with reward: 100.0\n",
      "Final position: [15 31]\n",
      "Goal reached: True\n"
     ]
    }
   ],
   "source": [
    "from src.env_sailing import SailingEnv\n",
    "from src.initial_windfields import get_initial_windfield\n",
    "\n",
    "# Create an environment with a simple test initial windfield\n",
    "env = SailingEnv(**get_initial_windfield('simple_static'))\n",
    "naive_agent = ImprovedSailingAgent()\n",
    "\n",
    "# Run a single episode\n",
    "observation, info = env.reset(seed=42)\n",
    "total_reward = 0\n",
    "done = False\n",
    "truncated = False\n",
    "step_count = 0\n",
    "\n",
    "print(\"Running the naive agent on the simple_static initial windfield:\")\n",
    "while not (done or truncated) and step_count < 1000:  # Limit to 100 steps\n",
    "    action = naive_agent.act(observation)\n",
    "    observation, reward, done, truncated, info = env.step(action)\n",
    "    total_reward += reward\n",
    "    step_count += 1\n",
    "    \n",
    "    # Print every 10 steps to avoid too much output\n",
    "    if step_count % 10 == 0:\n",
    "        print(f\"Step {step_count}: Position={info['position']}, Reward={reward}\")\n",
    "\n",
    "print(f\"\\nEpisode finished after {step_count} steps with reward: {total_reward}\")\n",
    "print(f\"Final position: {info['position']}\")\n",
    "print(f\"Goal reached: {done}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedSailingAgent(BaseAgent):\n",
    "    \"\"\"\n",
    "    Agent de voile avancé avec optimisation complète basée sur la physique de la voile\n",
    "    et une planification de trajectoire intelligente.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.np_random = np.random.default_rng()\n",
    "        \n",
    "        # Paramètres de navigation à voile\n",
    "        self.polar_data = self._create_sailing_polar()  # Données d'efficacité polaire\n",
    "        self.tacking_distance = 3.0  # Distance minimale avant de virer de bord\n",
    "        self.last_tack_position = None  # Pour éviter le virement de bord trop fréquent\n",
    "        self.last_actions = []  # Historique des actions pour la stabilité\n",
    "        self.action_history_size = 3  # Nombre d'actions à stocker dans l'historique\n",
    "        self.wind_prediction_horizon = 5  # Nombre de pas pour la prédiction du vent\n",
    "        \n",
    "        # Paramètres adaptatifs\n",
    "        self.adaptation_rate = 0.1  # Taux d'adaptation aux conditions\n",
    "        self.performance_history = []  # Historique des performances\n",
    "        \n",
    "        # Stratégie globale\n",
    "        self.upwind_strategy = \"tacking\"  # tacking ou long_tacking\n",
    "        self.downwind_strategy = \"direct\"  # direct ou gybing\n",
    "        \n",
    "        # État de navigation\n",
    "        self.current_leg = \"upwind\"  # upwind, reaching, downwind\n",
    "        self.tack_side = \"port\"  # port (bâbord) ou starboard (tribord)\n",
    "        \n",
    "    def _create_sailing_polar(self):\n",
    "        \"\"\"\n",
    "        Crée une table polaire d'efficacité de navigation basée sur l'angle au vent.\n",
    "        Renvoie un dictionnaire avec {angle: efficacité} où l'angle est en degrés.\n",
    "        \"\"\"\n",
    "        polar = {}\n",
    "        \n",
    "        # Zone de non-navigation (irons) - très faible efficacité\n",
    "        for angle in range(0, 46):\n",
    "            polar[angle] = 0.1\n",
    "            polar[360-angle] = 0.1\n",
    "        \n",
    "        # Près (close-hauled) - efficacité modérée\n",
    "        for angle in range(46, 80):\n",
    "            factor = (angle - 45) / 35  # Augmentation progressive de l'efficacité\n",
    "            polar[angle] = 0.3 + 0.4 * factor\n",
    "            polar[360-angle] = 0.3 + 0.4 * factor\n",
    "        \n",
    "        # Travers (beam reach) - haute efficacité\n",
    "        for angle in range(80, 120):\n",
    "            polar[angle] = 0.9 + 0.1 * np.sin((angle - 80) * np.pi / 80)\n",
    "            polar[360-angle] = 0.9 + 0.1 * np.sin((angle - 80) * np.pi / 80)\n",
    "        \n",
    "        # Grand largue (broad reach) - très haute efficacité\n",
    "        for angle in range(120, 150):\n",
    "            polar[angle] = 0.95\n",
    "            polar[360-angle] = 0.95\n",
    "        \n",
    "        # Vent arrière (running) - haute efficacité mais pas optimale\n",
    "        for angle in range(150, 181):\n",
    "            factor = (180 - angle) / 30  # Diminution progressive\n",
    "            polar[angle] = 0.85 + 0.1 * factor\n",
    "            polar[360-angle] = 0.85 + 0.1 * factor\n",
    "        \n",
    "        return polar\n",
    "    \n",
    "    def act(self, observation: np.ndarray) -> int:\n",
    "        \"\"\"\n",
    "        Sélectionne l'action optimale basée sur une analyse complète des conditions.\n",
    "        \"\"\"\n",
    "        # Extraire les informations de l'observation\n",
    "        position = observation[:2]  # [x, y]\n",
    "        velocity = observation[2:4]  # [vx, vy]\n",
    "        wind_at_boat = observation[4:6]  # [wx, wy]\n",
    "        \n",
    "        # Extraire et analyser le champ de vent complet\n",
    "        wind_field_data = observation[6:]\n",
    "        wind_field = self._reconstruct_wind_field(wind_field_data)\n",
    "        \n",
    "        # Définir la position de l'objectif (basée sur l'exemple précédent)\n",
    "        goal_position = np.array([16, 31])\n",
    "        \n",
    "        # Calcul des vecteurs et angles principaux\n",
    "        direction_to_goal = goal_position - position\n",
    "        distance_to_goal = np.linalg.norm(direction_to_goal)\n",
    "        \n",
    "        wind_speed = np.linalg.norm(wind_at_boat)\n",
    "        wind_angle = np.degrees(np.arctan2(wind_at_boat[1], wind_at_boat[0])) % 360\n",
    "        \n",
    "        goal_angle = np.degrees(np.arctan2(direction_to_goal[1], direction_to_goal[0])) % 360\n",
    "        \n",
    "        # Angle du bateau (basé sur la vitesse si disponible)\n",
    "        boat_angle = None\n",
    "        if np.linalg.norm(velocity) > 0.01:\n",
    "            boat_angle = np.degrees(np.arctan2(velocity[1], velocity[0])) % 360\n",
    "        else:\n",
    "            # Si le bateau n'a pas de vitesse significative, utiliser l'angle vers l'objectif\n",
    "            boat_angle = goal_angle\n",
    "        \n",
    "        # Angle relatif entre le vent et la direction du bateau\n",
    "        wind_relative_angle = (boat_angle - wind_angle) % 360\n",
    "        \n",
    "        # Analyse de l'environnement\n",
    "        upwind_condition = self._is_upwind(goal_angle, wind_angle)\n",
    "        downwind_condition = self._is_downwind(goal_angle, wind_angle)\n",
    "        \n",
    "        # Déterminer la stratégie en fonction de la situation\n",
    "        if distance_to_goal < 2.0:\n",
    "            # Près de l'objectif - naviguer directement\n",
    "            return self._get_action_towards_direction(direction_to_goal)\n",
    "        elif upwind_condition:\n",
    "            # Navigation contre le vent - utiliser la stratégie de louvoyage\n",
    "            return self._upwind_strategy(position, wind_angle, boat_angle, goal_position, wind_field)\n",
    "        elif downwind_condition:\n",
    "            # Navigation avec le vent arrière\n",
    "            return self._downwind_strategy(position, wind_angle, boat_angle, goal_position, wind_field)\n",
    "        else:\n",
    "            # Navigation au travers ou au largue - optimiser l'angle\n",
    "            return self._optimal_reaching_strategy(position, velocity, wind_angle, boat_angle, goal_position, wind_field)\n",
    "    \n",
    "    def _reconstruct_wind_field(self, wind_field_data):\n",
    "        \"\"\"\n",
    "        Reconstruit le champ de vent à partir des données aplaties.\n",
    "        Pour simplifier, nous retournons les données brutes ici.\n",
    "        Dans une implémentation complète, vous reconstruiriez une grille 2D.\n",
    "        \"\"\"\n",
    "        return wind_field_data\n",
    "    \n",
    "    def _is_upwind(self, goal_angle, wind_angle):\n",
    "        \"\"\"Détermine si l'objectif est face au vent (± zone de non-navigation).\"\"\"\n",
    "        relative_angle = (goal_angle - wind_angle) % 360\n",
    "        return relative_angle < 50 or relative_angle > 310\n",
    "    \n",
    "    def _is_downwind(self, goal_angle, wind_angle):\n",
    "        \"\"\"Détermine si l'objectif est sous le vent.\"\"\"\n",
    "        relative_angle = (goal_angle - wind_angle) % 360\n",
    "        return 160 < relative_angle < 200\n",
    "    \n",
    "    def _get_sailing_efficiency(self, wind_relative_angle):\n",
    "        \"\"\"Obtient l'efficacité de navigation pour un angle donné par rapport au vent.\"\"\"\n",
    "        angle = int(wind_relative_angle % 360)\n",
    "        return self.polar_data.get(angle, 0.5)  # Valeur par défaut si l'angle n'est pas dans la table\n",
    "    \n",
    "    def _upwind_strategy(self, position, wind_angle, boat_angle, goal_position, wind_field):\n",
    "        \"\"\"\n",
    "        Stratégie de navigation au près (contre le vent).\n",
    "        Implémente le louvoyage (tacking) de manière optimisée.\n",
    "        \"\"\"\n",
    "        # Calculer la ligne imaginaire de progression vers l'objectif\n",
    "        goal_direction = goal_position - position\n",
    "        \n",
    "        # Déterminer le côté du tack en fonction de la position actuelle\n",
    "        # Stratégie: zigzag autour de la ligne vers l'objectif\n",
    "        cross_track = self._cross_track_distance(position, goal_position)\n",
    "        \n",
    "        # Changement de bord si nécessaire\n",
    "        if cross_track > self.tacking_distance:\n",
    "            self.tack_side = \"port\" if self.tack_side == \"starboard\" else \"starboard\"\n",
    "            self.last_tack_position = position.copy()\n",
    "        \n",
    "        # Calculer l'angle optimal de près\n",
    "        if self.tack_side == \"port\":\n",
    "            tack_angle = (wind_angle + 50) % 360  # 50° tribord du vent\n",
    "        else:\n",
    "            tack_angle = (wind_angle - 50) % 360  # 50° bâbord du vent\n",
    "        \n",
    "        # Vecteur de direction pour cet angle\n",
    "        tack_direction = np.array([np.cos(np.radians(tack_angle)), np.sin(np.radians(tack_angle))])\n",
    "        \n",
    "        # Vérifier si nous nous éloignons trop de l'objectif\n",
    "        direction_to_goal = goal_position - position\n",
    "        angle_to_goal = np.degrees(np.arctan2(direction_to_goal[1], direction_to_goal[0])) % 360\n",
    "        \n",
    "        # Si le tack nous éloigne trop de la direction générale, envisager de virer\n",
    "        angle_diff = min((tack_angle - angle_to_goal) % 360, (angle_to_goal - tack_angle) % 360)\n",
    "        if angle_diff > 100:  # Nous nous éloignons trop\n",
    "            # Virer de bord si nous avons parcouru une distance minimale depuis le dernier virement\n",
    "            if self.last_tack_position is None or np.linalg.norm(position - self.last_tack_position) > 5.0:\n",
    "                self.tack_side = \"port\" if self.tack_side == \"starboard\" else \"starboard\"\n",
    "                self.last_tack_position = position.copy()\n",
    "                # Recalculer l'angle après le virement\n",
    "                if self.tack_side == \"port\":\n",
    "                    tack_angle = (wind_angle + 50) % 360\n",
    "                else:\n",
    "                    tack_angle = (wind_angle - 50) % 360\n",
    "                tack_direction = np.array([np.cos(np.radians(tack_angle)), np.sin(np.radians(tack_angle))])\n",
    "        \n",
    "        # Obtenir l'action correspondante avec un léger facteur de stabilité\n",
    "        action = self._get_action_towards_direction(tack_direction)\n",
    "        \n",
    "        # Stabiliser l'action en utilisant l'historique\n",
    "        if len(self.last_actions) > 0:\n",
    "            # 80% nouvelle action, 20% moyenne des actions précédentes pour éviter les oscillations\n",
    "            prev_action = self.last_actions[-1]\n",
    "            if abs(action - prev_action) <= 1 or abs(action - prev_action) >= 7:  # Actions similaires ou opposées\n",
    "                action = action  # Garder la nouvelle action\n",
    "            else:\n",
    "                # Petite stabilisation\n",
    "                action = action if self.np_random.random() < 0.8 else prev_action\n",
    "        \n",
    "        # Mettre à jour l'historique des actions\n",
    "        self.last_actions.append(action)\n",
    "        if len(self.last_actions) > self.action_history_size:\n",
    "            self.last_actions.pop(0)\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def _downwind_strategy(self, position, wind_angle, boat_angle, goal_position, wind_field):\n",
    "        \"\"\"\n",
    "        Stratégie de navigation au portant (avec le vent arrière).\n",
    "        Implémente une navigation directe ou en zigzag selon les conditions.\n",
    "        \"\"\"\n",
    "        # Direction vers l'objectif\n",
    "        direction_to_goal = goal_position - position\n",
    "        goal_angle = np.degrees(np.arctan2(direction_to_goal[1], direction_to_goal[0])) % 360\n",
    "        \n",
    "        # Pour le vent arrière, la navigation directe n'est pas toujours optimale\n",
    "        # Nous pouvons naviguer légèrement de biais pour plus d'efficacité\n",
    "        \n",
    "        # Calculer les angles optimaux de chaque côté\n",
    "        port_angle = (wind_angle + 150) % 360\n",
    "        starboard_angle = (wind_angle - 150) % 360\n",
    "        \n",
    "        # Déterminer quel angle est le plus proche de la direction vers l'objectif\n",
    "        port_diff = min((port_angle - goal_angle) % 360, (goal_angle - port_angle) % 360)\n",
    "        starboard_diff = min((starboard_angle - goal_angle) % 360, (goal_angle - starboard_angle) % 360)\n",
    "        \n",
    "        # Choisir l'angle optimal\n",
    "        if port_diff < starboard_diff:\n",
    "            optimal_angle = port_angle\n",
    "        else:\n",
    "            optimal_angle = starboard_angle\n",
    "        \n",
    "        # Mixer avec la direction de l'objectif pour garantir la progression\n",
    "        optimal_direction = np.array([np.cos(np.radians(optimal_angle)), np.sin(np.radians(optimal_angle))])\n",
    "        goal_direction = direction_to_goal / np.linalg.norm(direction_to_goal)\n",
    "        \n",
    "        mixed_direction = 0.6 * optimal_direction + 0.4 * goal_direction\n",
    "        \n",
    "        # Obtenir l'action correspondante\n",
    "        action = self._get_action_towards_direction(mixed_direction)\n",
    "        \n",
    "        # Stabiliser l'action\n",
    "        if len(self.last_actions) > 0:\n",
    "            prev_action = self.last_actions[-1]\n",
    "            if action != prev_action and self.np_random.random() < 0.3:\n",
    "                action = prev_action\n",
    "        \n",
    "        # Mettre à jour l'historique\n",
    "        self.last_actions.append(action)\n",
    "        if len(self.last_actions) > self.action_history_size:\n",
    "            self.last_actions.pop(0)\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def _optimal_reaching_strategy(self, position, velocity, wind_angle, boat_angle, goal_position, wind_field):\n",
    "        \"\"\"\n",
    "        Stratégie optimale pour la navigation au travers ou au largue.\n",
    "        Maximise la vitesse tout en se dirigeant vers l'objectif.\n",
    "        \"\"\"\n",
    "        # Direction et angle vers l'objectif\n",
    "        direction_to_goal = goal_position - position\n",
    "        goal_angle = np.degrees(np.arctan2(direction_to_goal[1], direction_to_goal[0])) % 360\n",
    "        \n",
    "        # Angles de navigation optimaux (au travers)\n",
    "        optimal_angles = [(wind_angle + 90) % 360, (wind_angle - 90) % 360, \n",
    "                          (wind_angle + 120) % 360, (wind_angle - 120) % 360]\n",
    "        \n",
    "        # Trouver l'angle optimal le plus proche de notre direction\n",
    "        best_angle = None\n",
    "        min_diff = 180\n",
    "        \n",
    "        for angle in optimal_angles:\n",
    "            diff = min((angle - goal_angle) % 360, (goal_angle - angle) % 360)\n",
    "            if diff < min_diff:\n",
    "                min_diff = diff\n",
    "                best_angle = angle\n",
    "        \n",
    "        # Direction optimale\n",
    "        optimal_direction = np.array([np.cos(np.radians(best_angle)), np.sin(np.radians(best_angle))])\n",
    "        \n",
    "        # Équilibrer entre direction optimale et direction vers l'objectif\n",
    "        # Plus nous sommes proches de l'objectif, plus nous privilégions la direction directe\n",
    "        distance = np.linalg.norm(direction_to_goal)\n",
    "        goal_weight = max(0.3, min(0.8, 1.0 - distance / 20.0))\n",
    "        \n",
    "        goal_direction = direction_to_goal / np.linalg.norm(direction_to_goal)\n",
    "        mixed_direction = (1 - goal_weight) * optimal_direction + goal_weight * goal_direction\n",
    "        \n",
    "        # Obtenir l'action\n",
    "        action = self._get_action_towards_direction(mixed_direction)\n",
    "        \n",
    "        # Stabilisation\n",
    "        if len(self.last_actions) > 0:\n",
    "            prev_action = self.last_actions[-1]\n",
    "            if action != prev_action and self.np_random.random() < 0.2:\n",
    "                action = prev_action\n",
    "        \n",
    "        # Mettre à jour l'historique\n",
    "        self.last_actions.append(action)\n",
    "        if len(self.last_actions) > self.action_history_size:\n",
    "            self.last_actions.pop(0)\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def _get_action_towards_direction(self, direction):\n",
    "        \"\"\"Convertit un vecteur de direction en action discrète (0-8).\"\"\"\n",
    "        # Normaliser le vecteur\n",
    "        if np.linalg.norm(direction) > 0:\n",
    "            direction = direction / np.linalg.norm(direction)\n",
    "        \n",
    "        # Cartographier les 8 directions aux actions\n",
    "        actions_map = [\n",
    "            (0, 1),    # 0: Nord\n",
    "            (1, 1),    # 1: Nord-Est\n",
    "            (1, 0),    # 2: Est\n",
    "            (1, -1),   # 3: Sud-Est\n",
    "            (0, -1),   # 4: Sud\n",
    "            (-1, -1),  # 5: Sud-Ouest\n",
    "            (-1, 0),   # 6: Ouest\n",
    "            (-1, 1),   # 7: Nord-Ouest\n",
    "        ]\n",
    "        \n",
    "        # Ajouter l'action \"rester sur place\"\n",
    "        actions_map.append((0, 0))  # 8: Rester sur place\n",
    "        \n",
    "        # Trouver l'action la plus similaire à la direction\n",
    "        best_action = 0\n",
    "        best_similarity = -float('inf')\n",
    "        \n",
    "        for i, action_dir in enumerate(actions_map):\n",
    "            # Pour \"rester sur place\", utiliser une similarité spéciale\n",
    "            if i == 8:\n",
    "                # N'utiliser cette action que si la direction est très faible\n",
    "                if np.linalg.norm(direction) < 0.2:\n",
    "                    similarity = 1.0\n",
    "                else:\n",
    "                    similarity = -1.0\n",
    "            else:\n",
    "                similarity = direction[0] * action_dir[0] + direction[1] * action_dir[1]\n",
    "            \n",
    "            if similarity > best_similarity:\n",
    "                best_similarity = similarity\n",
    "                best_action = i\n",
    "        \n",
    "        return best_action\n",
    "    \n",
    "    def _cross_track_distance(self, position, goal_position):\n",
    "        \"\"\"\n",
    "        Calcule la distance perpendiculaire (cross-track) entre la position actuelle\n",
    "        et la ligne droite allant de la position initiale à l'objectif.\n",
    "        Pour simplifier, nous supposons que la position initiale est [16, 0].\n",
    "        \"\"\"\n",
    "        start_position = np.array([16, 0])\n",
    "        \n",
    "        # Vecteur de la ligne (de départ à objectif)\n",
    "        track_vector = goal_position - start_position\n",
    "        track_length = np.linalg.norm(track_vector)\n",
    "        \n",
    "        if track_length == 0:\n",
    "            return np.linalg.norm(position - start_position)\n",
    "        \n",
    "        # Normaliser\n",
    "        track_unit = track_vector / track_length\n",
    "        \n",
    "        # Vecteur de la position actuelle au point de départ\n",
    "        pos_vector = position - start_position\n",
    "        \n",
    "        # Projection sur la ligne\n",
    "        projection = np.dot(pos_vector, track_unit)\n",
    "        \n",
    "        # Calculer le point projeté\n",
    "        projected_point = start_position + projection * track_unit\n",
    "        \n",
    "        # Distance perpendiculaire\n",
    "        return np.linalg.norm(position - projected_point)\n",
    "    \n",
    "    def reset(self) -> None:\n",
    "        \"\"\"Réinitialiser l'agent.\"\"\"\n",
    "        self.last_tack_position = None\n",
    "        self.last_actions = []\n",
    "        self.performance_history = []\n",
    "        self.tack_side = \"port\" if self.np_random.random() < 0.5 else \"starboard\"\n",
    "    \n",
    "    def seed(self, seed: int = None) -> None:\n",
    "        \"\"\"Définir la graine aléatoire.\"\"\"\n",
    "        self.np_random = np.random.default_rng(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running the naive agent on the simple_static initial windfield:\n",
      "Step 10: Position=[ 6 11], Reward=0.0\n",
      "Step 20: Position=[ 6 11], Reward=0.0\n",
      "Step 30: Position=[ 7 12], Reward=0.0\n",
      "Step 40: Position=[ 8 13], Reward=0.0\n",
      "Step 50: Position=[ 8 13], Reward=0.0\n",
      "Step 60: Position=[ 8 15], Reward=0.0\n",
      "Step 70: Position=[ 7 17], Reward=0.0\n",
      "Step 80: Position=[ 7 17], Reward=0.0\n",
      "Step 90: Position=[ 8 18], Reward=0.0\n",
      "Step 100: Position=[ 9 19], Reward=0.0\n",
      "Step 110: Position=[ 9 19], Reward=0.0\n",
      "Step 120: Position=[10 20], Reward=0.0\n",
      "Step 130: Position=[10 20], Reward=0.0\n",
      "Step 140: Position=[11 21], Reward=0.0\n",
      "Step 150: Position=[11 21], Reward=0.0\n",
      "Step 160: Position=[12 22], Reward=0.0\n",
      "Step 170: Position=[13 23], Reward=0.0\n",
      "Step 180: Position=[13 23], Reward=0.0\n",
      "Step 190: Position=[14 24], Reward=0.0\n",
      "Step 200: Position=[14 24], Reward=0.0\n",
      "Step 210: Position=[15 25], Reward=0.0\n",
      "Step 220: Position=[15 25], Reward=0.0\n",
      "Step 230: Position=[15 27], Reward=0.0\n",
      "Step 240: Position=[15 28], Reward=0.0\n",
      "Step 250: Position=[15 29], Reward=0.0\n",
      "\n",
      "Episode finished after 251 steps with reward: 100.0\n",
      "Final position: [15 30]\n",
      "Goal reached: True\n"
     ]
    }
   ],
   "source": [
    "from src.env_sailing import SailingEnv\n",
    "from src.initial_windfields import get_initial_windfield\n",
    "\n",
    "# Create an environment with a simple test initial windfield\n",
    "env = SailingEnv(**get_initial_windfield('simple_static'))\n",
    "naive_agent = AdvancedSailingAgent()\n",
    "\n",
    "# Run a single episode\n",
    "observation, info = env.reset(seed=42)\n",
    "total_reward = 0\n",
    "done = False\n",
    "truncated = False\n",
    "step_count = 0\n",
    "\n",
    "print(\"Running the naive agent on the simple_static initial windfield:\")\n",
    "while not (done or truncated) and step_count < 1000:  # Limit to 100 steps\n",
    "    action = naive_agent.act(observation)\n",
    "    observation, reward, done, truncated, info = env.step(action)\n",
    "    total_reward += reward\n",
    "    step_count += 1\n",
    "    \n",
    "    # Print every 10 steps to avoid too much output\n",
    "    if step_count % 10 == 0:\n",
    "        print(f\"Step {step_count}: Position={info['position']}, Reward={reward}\")\n",
    "\n",
    "print(f\"\\nEpisode finished after {step_count} steps with reward: {total_reward}\")\n",
    "print(f\"Final position: {info['position']}\")\n",
    "print(f\"Goal reached: {done}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing a Simple RL Agent\n",
    "\n",
    "## Implementing a Q-Learning Agent\n",
    "\n",
    "Now let's implement a basic Q-learning agent for our sailing environment. Q-learning is a model-free reinforcement learning algorithm that learns to make decisions by estimating the value of state-action pairs.\n",
    "\n",
    "Our implementation will use a simplified state representation based on:\n",
    "1. Agent's current position\n",
    "2. Agent's current velocity \n",
    "3. Local wind at the agent's position\n",
    "\n",
    "This simplified approach makes the agent more interpretable and faster to train, while still capturing essential local information for effective navigation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent(BaseAgent):\n",
    "    \"\"\"A simple Q-learning agent for the sailing environment using only local information.\"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.1, discount_factor=0.9, exploration_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.np_random = np.random.default_rng()\n",
    "        \n",
    "        # Learning parameters\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.exploration_rate = exploration_rate\n",
    "        \n",
    "        # State discretization parameters\n",
    "        self.position_bins = 8     # Discretize the grid into 8x8\n",
    "        self.velocity_bins = 4     # Discretize velocity into 4 bins\n",
    "        self.wind_bins = 8         # Discretize wind directions into 8 bins\n",
    "        \n",
    "        # Initialize Q-table\n",
    "        # State space: position_x, position_y, velocity_direction, wind_direction\n",
    "        # Action space: 9 possible actions\n",
    "        self.q_table = {}\n",
    "        \n",
    "    def discretize_state(self, observation):\n",
    "        \"\"\"Convert continuous observation to discrete state for Q-table lookup.\"\"\"\n",
    "        # Extract position, velocity and wind from observation\n",
    "        x, y = observation[0], observation[1]\n",
    "        vx, vy = observation[2], observation[3]\n",
    "        wx, wy = observation[4], observation[5]\n",
    "        \n",
    "        # Discretize position (assume 32x32 grid)\n",
    "        grid_size = 32\n",
    "        x_bin = min(int(x / grid_size * self.position_bins), self.position_bins - 1)\n",
    "        y_bin = min(int(y / grid_size * self.position_bins), self.position_bins - 1)\n",
    "        \n",
    "        # Discretize velocity direction (ignoring magnitude for simplicity)\n",
    "        v_magnitude = np.sqrt(vx**2 + vy**2)\n",
    "        if v_magnitude < 0.1:  # If velocity is very small, consider it as a separate bin\n",
    "            v_bin = 0\n",
    "        else:\n",
    "            v_direction = np.arctan2(vy, vx)  # Range: [-pi, pi]\n",
    "            v_bin = int(((v_direction + np.pi) / (2 * np.pi) * (self.velocity_bins-1)) + 1) % self.velocity_bins\n",
    "        \n",
    "        # Discretize wind direction\n",
    "        wind_direction = np.arctan2(wy, wx)  # Range: [-pi, pi]\n",
    "        wind_bin = int(((wind_direction + np.pi) / (2 * np.pi) * self.wind_bins)) % self.wind_bins\n",
    "        \n",
    "        # Return discrete state tuple\n",
    "        return (x_bin, y_bin, v_bin, wind_bin)\n",
    "        \n",
    "    def act(self, observation):\n",
    "        \"\"\"Choose an action using epsilon-greedy policy.\"\"\"\n",
    "        # Discretize the state\n",
    "        state = self.discretize_state(observation)\n",
    "        \n",
    "        # Epsilon-greedy action selection\n",
    "        if self.np_random.random() < self.exploration_rate:\n",
    "            # Explore: choose a random action\n",
    "            return self.np_random.integers(0, 9)\n",
    "        else:\n",
    "            # Exploit: choose the best action according to Q-table\n",
    "            if state not in self.q_table:\n",
    "                # If state not in Q-table, initialize it\n",
    "                self.q_table[state] = np.zeros(9)\n",
    "            \n",
    "            # Return action with highest Q-value\n",
    "            return np.argmax(self.q_table[state])\n",
    "    \n",
    "    def learn(self, state, action, reward, next_state):\n",
    "        \"\"\"Update Q-table based on observed transition.\"\"\"\n",
    "        # Initialize Q-values if states not in table\n",
    "        if state not in self.q_table:\n",
    "            self.q_table[state] = np.zeros(9)\n",
    "        if next_state not in self.q_table:\n",
    "            self.q_table[next_state] = np.zeros(9)\n",
    "        \n",
    "        # Q-learning update\n",
    "        best_next_action = np.argmax(self.q_table[next_state])\n",
    "        td_target = reward + self.discount_factor * self.q_table[next_state][best_next_action]\n",
    "        td_error = td_target - self.q_table[state][action]\n",
    "        self.q_table[state][action] += self.learning_rate * td_error\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset the agent for a new episode.\"\"\"\n",
    "        # Nothing to reset for Q-learning agent\n",
    "        pass\n",
    "        \n",
    "    def seed(self, seed=None):\n",
    "        \"\"\"Set the random seed.\"\"\"\n",
    "        self.np_random = np.random.default_rng(seed)\n",
    "        \n",
    "    def save(self, path):\n",
    "        \"\"\"Save the Q-table to a file.\"\"\"\n",
    "        import pickle\n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump(self.q_table, f)\n",
    "            \n",
    "    def load(self, path):\n",
    "        \"\"\"Load the Q-table from a file.\"\"\"\n",
    "        import pickle\n",
    "        with open(path, 'rb') as f:\n",
    "            self.q_table = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Q-Learning Agent\n",
    "\n",
    "Now let's train our Q-learning agent on a simple initial windfield. We'll start with a small number of episodes (10) to demonstrate the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training with 10 episodes (debug run)...\n",
      "Episode 1: Steps=1000, Reward=0.0, Position=[ 8 30], Goal reached=True\n",
      "Episode 2: Steps=1000, Reward=0.0, Position=[ 1 30], Goal reached=True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3: Steps=1000, Reward=0.0, Position=[ 0 31], Goal reached=True\n",
      "Episode 4: Steps=1000, Reward=0.0, Position=[ 1 24], Goal reached=True\n",
      "Episode 5: Steps=1000, Reward=0.0, Position=[ 4 30], Goal reached=True\n",
      "Episode 6: Steps=1000, Reward=0.0, Position=[ 0 27], Goal reached=True\n",
      "Episode 7: Steps=1000, Reward=0.0, Position=[ 4 31], Goal reached=True\n",
      "Episode 8: Steps=1000, Reward=0.0, Position=[ 7 29], Goal reached=True\n",
      "Episode 9: Steps=1000, Reward=0.0, Position=[ 1 31], Goal reached=True\n",
      "Episode 10: Steps=716, Reward=100.0, Position=[15 30], Goal reached=True\n",
      "\n",
      "Debug training completed!\n",
      "Q-table size: 215 states\n"
     ]
    }
   ],
   "source": [
    "# Create our Q-learning agent\n",
    "ql_agent = QLearningAgent(learning_rate=0.1, discount_factor=0.99, exploration_rate=0.2)\n",
    "\n",
    "# Set fixed seed for reproducibility\n",
    "np.random.seed(42)\n",
    "ql_agent.seed(42)\n",
    "\n",
    "# Create environment with a simple initial windfield\n",
    "env = SailingEnv(**get_initial_windfield('simple_static'))\n",
    "\n",
    "# Training parameters\n",
    "num_episodes = 10  # Small number for debugging\n",
    "max_steps = 1000\n",
    "\n",
    "# Training loop\n",
    "print(\"Starting training with 10 episodes (debug run)...\")\n",
    "for episode in range(num_episodes):\n",
    "    # Reset environment and get initial state\n",
    "    observation, info = env.reset(seed=episode)  # Different seed each episode\n",
    "    state = ql_agent.discretize_state(observation)\n",
    "    \n",
    "    total_reward = 0\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        # Select action and take step\n",
    "        action = ql_agent.act(observation)\n",
    "        next_observation, reward, done, truncated, info = env.step(action)\n",
    "        next_state = ql_agent.discretize_state(next_observation)\n",
    "        \n",
    "        # Update Q-table\n",
    "        ql_agent.learn(state, action, reward, next_state)\n",
    "        \n",
    "        # Update state and total reward\n",
    "        state = next_state\n",
    "        observation = next_observation\n",
    "        total_reward += reward\n",
    "        \n",
    "        # Break if episode is done\n",
    "        if done or truncated:\n",
    "            break\n",
    "    \n",
    "    print(f\"Episode {episode+1}: Steps={step+1}, Reward={total_reward}, \" +\n",
    "          f\"Position={info['position']}, Goal reached={done}\")\n",
    "    \n",
    "    # Update exploration rate (optional: decrease exploration over time)\n",
    "    ql_agent.exploration_rate = max(0.05, ql_agent.exploration_rate * 0.95)\n",
    "\n",
    "print(\"\\nDebug training completed!\")\n",
    "print(f\"Q-table size: {len(ql_agent.q_table)} states\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "méthode hybride"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridSailingQLearningAgent(BaseAgent):\n",
    "    \"\"\"\n",
    "    Agent hybride combinant Q-learning et connaissances de navigation à voile.\n",
    "    \n",
    "    Cet agent utilise:\n",
    "    1. Q-learning pour l'adaptation et l'apprentissage des politiques optimales\n",
    "    2. Connaissances physiques de la voile pour:\n",
    "       - Une meilleure représentation des états\n",
    "       - Une initialisation intelligente du Q-table\n",
    "       - Un mécanisme d'action guidée pour accélérer l'apprentissage\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.1, discount_factor=0.99, exploration_rate=0.2):\n",
    "        super().__init__()\n",
    "        self.np_random = np.random.default_rng()\n",
    "        \n",
    "        # Paramètres d'apprentissage\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.exploration_rate = exploration_rate\n",
    "        self.initial_exploration_rate = exploration_rate\n",
    "        self.min_exploration_rate = 0.05\n",
    "        self.exploration_decay = 0.9999  # Décroissance plus lente\n",
    "        \n",
    "        # Paramètres de discrétisation améliorés\n",
    "        self.position_bins = 10     # Plus de bins pour une meilleure précision\n",
    "        self.velocity_bins = 6      # Inclut magnitude et direction\n",
    "        self.wind_angle_bins = 12   # Discrétisation plus fine des angles de vent\n",
    "        \n",
    "        # Connaissance de la voile\n",
    "        self.sailing_efficiency = self._create_sailing_efficiency()\n",
    "        self.action_mapping = self._create_action_mapping()\n",
    "        \n",
    "        # Paramètres de récompense façonnée\n",
    "        self.use_shaped_rewards = True\n",
    "        self.progress_weight = 0.3  # Poids pour la récompense de progression\n",
    "        \n",
    "        # Variables d'état\n",
    "        self.previous_position = None\n",
    "        self.previous_distance_to_goal = None\n",
    "        self.episode_steps = 0\n",
    "        self.total_episodes = 0\n",
    "        self.goal_position = np.array([16, 31])  # Basé sur l'exemple précédent\n",
    "        \n",
    "        # Q-table et expérience\n",
    "        self.q_table = {}\n",
    "        self.experience_buffer = []  # Pour l'expérience replay simplifié\n",
    "        self.buffer_size = 10000\n",
    "        self.batch_size = 32\n",
    "        self.learn_every = 5  # Apprendre tous les N pas\n",
    "        \n",
    "        # Initialisation guidée\n",
    "        self._initialize_q_table_with_sailing_knowledge()\n",
    "    \n",
    "    def _create_sailing_efficiency(self):\n",
    "        \"\"\"\n",
    "        Crée une table d'efficacité de navigation basée sur l'angle au vent.\n",
    "        Cette connaissance sera utilisée pour guider l'exploration et initialiser la Q-table.\n",
    "        \"\"\"\n",
    "        efficiency = {}\n",
    "        \n",
    "        # Zone de non-navigation (irons) - très faible efficacité\n",
    "        for angle in range(0, 46):\n",
    "            efficiency[angle] = 0.1\n",
    "            efficiency[360-angle] = 0.1\n",
    "        \n",
    "        # Près (close-hauled) - efficacité modérée\n",
    "        for angle in range(46, 80):\n",
    "            factor = (angle - 45) / 35  # Augmentation progressive de l'efficacité\n",
    "            efficiency[angle] = 0.3 + 0.4 * factor\n",
    "            efficiency[360-angle] = 0.3 + 0.4 * factor\n",
    "        \n",
    "        # Travers (beam reach) - haute efficacité\n",
    "        for angle in range(80, 120):\n",
    "            efficiency[angle] = 0.9 + 0.1 * np.sin((angle - 80) * np.pi / 80)\n",
    "            efficiency[360-angle] = 0.9 + 0.1 * np.sin((angle - 80) * np.pi / 80)\n",
    "        \n",
    "        # Grand largue (broad reach) - très haute efficacité\n",
    "        for angle in range(120, 150):\n",
    "            efficiency[angle] = 0.95\n",
    "            efficiency[360-angle] = 0.95\n",
    "        \n",
    "        # Vent arrière (running) - haute efficacité mais pas optimale\n",
    "        for angle in range(150, 181):\n",
    "            factor = (180 - angle) / 30  # Diminution progressive\n",
    "            efficiency[angle] = 0.85 + 0.1 * factor\n",
    "            efficiency[360-angle] = 0.85 + 0.1 * factor\n",
    "        \n",
    "        return efficiency\n",
    "    \n",
    "    def _create_action_mapping(self):\n",
    "        \"\"\"\n",
    "        Crée une correspondance entre les actions discrètes et les directions.\n",
    "        \"\"\"\n",
    "        return [\n",
    "            (0, 1),    # 0: Nord\n",
    "            (1, 1),    # 1: Nord-Est\n",
    "            (1, 0),    # 2: Est\n",
    "            (1, -1),   # 3: Sud-Est\n",
    "            (0, -1),   # 4: Sud\n",
    "            (-1, -1),  # 5: Sud-Ouest\n",
    "            (-1, 0),   # 6: Ouest\n",
    "            (-1, 1),   # 7: Nord-Ouest\n",
    "            (0, 0)     # 8: Rester sur place\n",
    "        ]\n",
    "    \n",
    "    def _initialize_q_table_with_sailing_knowledge(self):\n",
    "        \"\"\"\n",
    "        Initialise intelligemment la Q-table avec des connaissances de navigation.\n",
    "        \"\"\"\n",
    "        # Cette méthode pourrait être étendue pour une initialisation plus sophistiquée\n",
    "        # Pour l'instant, nous allons simplement pré-initialiser certaines valeurs clés\n",
    "        \n",
    "        # Créer des états de base pour l'initialisation\n",
    "        for x_bin in range(self.position_bins):\n",
    "            for y_bin in range(self.position_bins):\n",
    "                for wind_bin in range(self.wind_angle_bins):\n",
    "                    # Convertir le bin de vent en angle de vent approximatif\n",
    "                    wind_angle = (wind_bin / self.wind_angle_bins) * 360\n",
    "                    \n",
    "                    # État simplifié (ignore vitesse pour l'initialisation)\n",
    "                    state = (x_bin, y_bin, 0, wind_bin)\n",
    "                    \n",
    "                    if state not in self.q_table:\n",
    "                        self.q_table[state] = np.zeros(9)\n",
    "                    \n",
    "                    # Pour chaque action, attribuer une valeur initiale basée sur l'efficacité de navigation\n",
    "                    for action, direction in enumerate(self.action_mapping):\n",
    "                        if action == 8:  # Rester sur place est rarement optimal\n",
    "                            self.q_table[state][action] = 0.1\n",
    "                            continue\n",
    "                        \n",
    "                        # Calculer l'angle relatif entre le vent et la direction de l'action\n",
    "                        action_angle = np.degrees(np.arctan2(direction[1], direction[0])) % 360\n",
    "                        relative_angle = (action_angle - wind_angle) % 360\n",
    "                        \n",
    "                        # Obtenir l'efficacité pour cet angle\n",
    "                        relative_angle_int = int(relative_angle)\n",
    "                        efficiency = self.sailing_efficiency.get(relative_angle_int, 0.5)\n",
    "                        \n",
    "                        # Valeur Q initiale basée sur l'efficacité et la progression vers l'objectif\n",
    "                        # Favoriser les actions qui font progresser vers le haut de la grille\n",
    "                        progress_factor = direction[1] * 0.5  # Valeur plus élevée pour les directions vers le nord\n",
    "                        \n",
    "                        # Initialiser la valeur Q\n",
    "                        self.q_table[state][action] = 0.5 * efficiency + 0.5 * progress_factor\n",
    "    \n",
    "    def discretize_state(self, observation):\n",
    "        \"\"\"\n",
    "        Convertit l'observation continue en état discret pour la recherche dans la Q-table.\n",
    "        Cette version améliorée utilise une discrétisation plus sophistiquée.\n",
    "        \"\"\"\n",
    "        # Extraire position, vitesse et vent de l'observation\n",
    "        x, y = observation[0], observation[1]\n",
    "        vx, vy = observation[2], observation[3]\n",
    "        wx, wy = observation[4], observation[5]\n",
    "        \n",
    "        # Discrétiser la position (grille 32x32 supposée)\n",
    "        grid_size = 32\n",
    "        x_bin = min(int(x / grid_size * self.position_bins), self.position_bins - 1)\n",
    "        y_bin = min(int(y / grid_size * self.position_bins), self.position_bins - 1)\n",
    "        \n",
    "        # Discrétiser la vitesse (magnitude et direction)\n",
    "        v_magnitude = np.sqrt(vx**2 + vy**2)\n",
    "        if v_magnitude < 0.1:  # Si la vitesse est très faible\n",
    "            v_bin = 0\n",
    "        else:\n",
    "            # Combiner magnitude et direction\n",
    "            v_direction = np.arctan2(vy, vx)  # Plage: [-pi, pi]\n",
    "            v_direction_norm = ((v_direction + np.pi) / (2 * np.pi))  # Normaliser à [0, 1]\n",
    "            \n",
    "            # Créer un indice composite basé sur magnitude et direction\n",
    "            v_magnitude_norm = min(v_magnitude / 5.0, 1.0)  # Supposer que 5.0 est la vitesse max\n",
    "            v_bin = int((v_direction_norm * (self.velocity_bins-2)) + 1 + (v_magnitude_norm * 0.99)) % self.velocity_bins\n",
    "        \n",
    "        # Discrétiser la direction du vent de manière plus fine\n",
    "        wind_direction = np.arctan2(wy, wx)  # Plage: [-pi, pi]\n",
    "        wind_bin = int(((wind_direction + np.pi) / (2 * np.pi) * self.wind_angle_bins)) % self.wind_angle_bins\n",
    "        \n",
    "        # Retourner le tuple d'état discret\n",
    "        return (x_bin, y_bin, v_bin, wind_bin)\n",
    "    \n",
    "    def get_shaped_reward(self, observation, reward, done):\n",
    "        \"\"\"\n",
    "        Crée une récompense façonnée pour guider l'apprentissage.\n",
    "        \"\"\"\n",
    "        if done:  # Si l'objectif est atteint, conserver la grande récompense\n",
    "            return reward\n",
    "        \n",
    "        # Extraire la position\n",
    "        position = observation[:2]\n",
    "        \n",
    "        # Calculer la distance à l'objectif\n",
    "        current_distance = np.linalg.norm(position - self.goal_position)\n",
    "        \n",
    "        # Si c'est le premier pas, initialiser les valeurs précédentes\n",
    "        if self.previous_position is None or self.previous_distance_to_goal is None:\n",
    "            self.previous_position = position\n",
    "            self.previous_distance_to_goal = current_distance\n",
    "            return 0.0\n",
    "        \n",
    "        # Récompense basée sur le progrès vers l'objectif\n",
    "        distance_improvement = self.previous_distance_to_goal - current_distance\n",
    "        progress_reward = distance_improvement * self.progress_weight\n",
    "        \n",
    "        # Extraire vent et vitesse\n",
    "        vx, vy = observation[2], observation[3]\n",
    "        wx, wy = observation[4], observation[5]\n",
    "        \n",
    "        # Calculer l'angle relatif entre le vent et la direction du mouvement\n",
    "        v_magnitude = np.sqrt(vx**2 + vy**2)\n",
    "        \n",
    "        # Seulement si le bateau se déplace\n",
    "        efficiency_reward = 0.0\n",
    "        if v_magnitude > 0.1:\n",
    "            movement_direction = np.arctan2(vy, vx)\n",
    "            wind_direction = np.arctan2(wy, wx)\n",
    "            \n",
    "            # Calculer l'angle relatif et le normaliser à [0, 360]\n",
    "            relative_angle = (np.degrees(movement_direction - wind_direction)) % 360\n",
    "            \n",
    "            # Obtenir l'efficacité pour cet angle\n",
    "            relative_angle_int = int(relative_angle)\n",
    "            sailing_efficiency = self.sailing_efficiency.get(relative_angle_int, 0.5)\n",
    "            \n",
    "            # Récompense basée sur l'efficacité * vitesse\n",
    "            efficiency_reward = sailing_efficiency * v_magnitude * 0.01\n",
    "        \n",
    "        # Mettre à jour les valeurs précédentes\n",
    "        self.previous_position = position\n",
    "        self.previous_distance_to_goal = current_distance\n",
    "        \n",
    "        # Récompense totale façonnée\n",
    "        shaped_reward = progress_reward + efficiency_reward\n",
    "        \n",
    "        return shaped_reward\n",
    "    \n",
    "    def act(self, observation):\n",
    "        \"\"\"\n",
    "        Choisit une action en utilisant une politique epsilon-greedy améliorée.\n",
    "        \"\"\"\n",
    "        # Discrétiser l'état\n",
    "        state = self.discretize_state(observation)\n",
    "        \n",
    "        # Décroissance de l'exploration\n",
    "        self.exploration_rate = max(\n",
    "            self.min_exploration_rate, \n",
    "            self.exploration_rate * self.exploration_decay\n",
    "        )\n",
    "        \n",
    "        # Sélection d'action epsilon-greedy\n",
    "        if self.np_random.random() < self.exploration_rate:\n",
    "            # Explorer: soit aléatoirement, soit guidé par les connaissances de navigation\n",
    "            if self.np_random.random() < 0.5:  # 50% du temps, utiliser une exploration guidée\n",
    "                action = self._get_guided_exploration_action(observation)\n",
    "            else:\n",
    "                action = self.np_random.integers(0, 9)\n",
    "        else:\n",
    "            # Exploiter: choisir la meilleure action selon la Q-table\n",
    "            if state not in self.q_table:\n",
    "                # Si l'état n'est pas dans la Q-table, l'initialiser\n",
    "                self.q_table[state] = np.zeros(9)\n",
    "            \n",
    "            # Retourner l'action avec la valeur Q la plus élevée\n",
    "            action = np.argmax(self.q_table[state])\n",
    "        \n",
    "        # Incrémenter le compteur d'étapes\n",
    "        self.episode_steps += 1\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def _get_guided_exploration_action(self, observation):\n",
    "        \"\"\"\n",
    "        Génère une action d'exploration guidée par les connaissances de navigation.\n",
    "        \"\"\"\n",
    "        # Extraire position, vitesse et vent de l'observation\n",
    "        position = observation[:2]\n",
    "        vx, vy = observation[2:4]\n",
    "        wx, wy = observation[4:6]\n",
    "        \n",
    "        # Direction vers l'objectif\n",
    "        direction_to_goal = self.goal_position - position\n",
    "        goal_angle = np.degrees(np.arctan2(direction_to_goal[1], direction_to_goal[0])) % 360\n",
    "        \n",
    "        # Angle du vent\n",
    "        wind_angle = np.degrees(np.arctan2(wy, wx)) % 360\n",
    "        \n",
    "        # Angle relatif entre le vent et la direction vers l'objectif\n",
    "        relative_angle = (goal_angle - wind_angle) % 360\n",
    "        \n",
    "        # Déterminer si l'objectif est face au vent (zone de non-navigation)\n",
    "        upwind_condition = relative_angle < 45 or relative_angle > 315\n",
    "        \n",
    "        actions_probabilities = np.ones(9) * 0.01  # Probabilité de base pour toutes les actions\n",
    "        \n",
    "        if upwind_condition:\n",
    "            # Stratégie de tacking (virement de bord)\n",
    "            port_tack_angle = (wind_angle + 60) % 360\n",
    "            starboard_tack_angle = (wind_angle - 60) % 360\n",
    "            \n",
    "            # Choisir le bord qui rapproche le plus de l'objectif\n",
    "            port_diff = min((port_tack_angle - goal_angle) % 360, (goal_angle - port_tack_angle) % 360)\n",
    "            starboard_diff = min((starboard_tack_angle - goal_angle) % 360, (goal_angle - starboard_tack_angle) % 360)\n",
    "            \n",
    "            best_tack_angle = port_tack_angle if port_diff < starboard_diff else starboard_tack_angle\n",
    "            \n",
    "            # Trouver l'action la plus proche de cet angle\n",
    "            best_action = self._get_action_for_angle(best_tack_angle)\n",
    "            actions_probabilities[best_action] = 0.8  # 80% de chance de choisir cette action\n",
    "        else:\n",
    "            # Pour les autres situations, privilégier les angles efficaces\n",
    "            for action, direction in enumerate(self.action_mapping):\n",
    "                if action == 8:  # Ignorer l'action \"rester sur place\"\n",
    "                    continue\n",
    "                \n",
    "                # Calculer l'angle de cette action\n",
    "                action_angle = np.degrees(np.arctan2(direction[1], direction[0])) % 360\n",
    "                \n",
    "                # Angle relatif au vent\n",
    "                action_rel_wind = (action_angle - wind_angle) % 360\n",
    "                \n",
    "                # Obtenir l'efficacité de navigation\n",
    "                efficiency = self.sailing_efficiency.get(int(action_rel_wind), 0.5)\n",
    "                \n",
    "                # Direction vers l'objectif\n",
    "                angle_to_goal = min((action_angle - goal_angle) % 360, (goal_angle - action_angle) % 360)\n",
    "                goal_factor = max(0.0, 1.0 - angle_to_goal / 180.0)\n",
    "                \n",
    "                # Calculer la probabilité\n",
    "                actions_probabilities[action] = efficiency * 0.5 + goal_factor * 0.5\n",
    "        \n",
    "        # Normaliser et retourner une action selon ces probabilités\n",
    "        actions_probabilities = actions_probabilities / np.sum(actions_probabilities)\n",
    "        return self.np_random.choice(9, p=actions_probabilities)\n",
    "    \n",
    "    def _get_action_for_angle(self, angle_deg):\n",
    "        \"\"\"\n",
    "        Trouve l'action qui correspond le mieux à un angle donné.\n",
    "        \"\"\"\n",
    "        # Convertir l'angle en vecteur de direction\n",
    "        angle_rad = np.radians(angle_deg)\n",
    "        direction = np.array([np.cos(angle_rad), np.sin(angle_rad)])\n",
    "        \n",
    "        # Trouver l'action la plus similaire\n",
    "        best_action = 0\n",
    "        best_similarity = -float('inf')\n",
    "        \n",
    "        for action, action_dir in enumerate(self.action_mapping):\n",
    "            if action == 8:  # Ignorer l'action \"rester sur place\"\n",
    "                continue\n",
    "                \n",
    "            similarity = direction[0] * action_dir[0] + direction[1] * action_dir[1]\n",
    "            if similarity > best_similarity:\n",
    "                best_similarity = similarity\n",
    "                best_action = action\n",
    "        \n",
    "        return best_action\n",
    "    \n",
    "    def learn(self, state, action, reward, next_state):\n",
    "        \"\"\"\n",
    "        Met à jour la Q-table basée sur la transition observée.\n",
    "        Utilise la récompense façonnée si activée.\n",
    "        \"\"\"\n",
    "        # Initialiser les valeurs Q si les états ne sont pas dans la table\n",
    "        if state not in self.q_table:\n",
    "            self.q_table[state] = np.zeros(9)\n",
    "        if next_state not in self.q_table:\n",
    "            self.q_table[next_state] = np.zeros(9)\n",
    "        \n",
    "        # Mise à jour Q-learning\n",
    "        best_next_action = np.argmax(self.q_table[next_state])\n",
    "        td_target = reward + self.discount_factor * self.q_table[next_state][best_next_action]\n",
    "        td_error = td_target - self.q_table[state][action]\n",
    "        self.q_table[state][action] += self.learning_rate * td_error\n",
    "        \n",
    "        # Ajouter l'expérience au buffer pour le replay\n",
    "        self.experience_buffer.append((state, action, reward, next_state))\n",
    "        if len(self.experience_buffer) > self.buffer_size:\n",
    "            self.experience_buffer.pop(0)\n",
    "        \n",
    "        # Apprendre de mini-lots périodiquement (expérience replay simplifié)\n",
    "        if self.episode_steps % self.learn_every == 0 and len(self.experience_buffer) >= self.batch_size:\n",
    "            self._learn_from_batch()\n",
    "    \n",
    "    def _learn_from_batch(self):\n",
    "        \"\"\"\n",
    "        Apprend à partir d'un mini-lot d'expériences passées.\n",
    "        \"\"\"\n",
    "        # Échantillonner un mini-lot aléatoire\n",
    "        batch_indices = self.np_random.choice(\n",
    "            len(self.experience_buffer), \n",
    "            size=min(self.batch_size, len(self.experience_buffer)), \n",
    "            replace=False\n",
    "        )\n",
    "        \n",
    "        for i in batch_indices:\n",
    "            state, action, reward, next_state = self.experience_buffer[i]\n",
    "            \n",
    "            # Vérifier/initialiser les valeurs Q\n",
    "            if state not in self.q_table:\n",
    "                self.q_table[state] = np.zeros(9)\n",
    "            if next_state not in self.q_table:\n",
    "                self.q_table[next_state] = np.zeros(9)\n",
    "            \n",
    "            # Mise à jour Q-learning\n",
    "            best_next_action = np.argmax(self.q_table[next_state])\n",
    "            td_target = reward + self.discount_factor * self.q_table[next_state][best_next_action]\n",
    "            td_error = td_target - self.q_table[state][action]\n",
    "            self.q_table[state][action] += self.learning_rate * 0.5 * td_error  # Taux réduit pour la stabilité\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Réinitialise l'agent pour un nouvel épisode.\"\"\"\n",
    "        self.previous_position = None\n",
    "        self.previous_distance_to_goal = None\n",
    "        self.episode_steps = 0\n",
    "        self.total_episodes += 1\n",
    "        \n",
    "        # Diminuer progressivement l'exploration au fil des épisodes\n",
    "        if self.total_episodes % 100 == 0 and self.total_episodes > 0:\n",
    "            self.exploration_rate = max(\n",
    "                self.min_exploration_rate, \n",
    "                self.exploration_rate * 0.95\n",
    "            )\n",
    "    \n",
    "    def seed(self, seed=None):\n",
    "        \"\"\"Définit la graine aléatoire.\"\"\"\n",
    "        self.np_random = np.random.default_rng(seed)\n",
    "    \n",
    "    def save(self, path):\n",
    "        \"\"\"Sauvegarde la Q-table et les paramètres dans un fichier.\"\"\"\n",
    "        import pickle\n",
    "        save_data = {\n",
    "            'q_table': self.q_table,\n",
    "            'exploration_rate': self.exploration_rate,\n",
    "            'total_episodes': self.total_episodes\n",
    "        }\n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump(save_data, f)\n",
    "    \n",
    "    def load(self, path):\n",
    "        \"\"\"Charge la Q-table et les paramètres depuis un fichier.\"\"\"\n",
    "        import pickle\n",
    "        with open(path, 'rb') as f:\n",
    "            save_data = pickle.load(f)\n",
    "            self.q_table = save_data['q_table']\n",
    "            self.exploration_rate = save_data['exploration_rate']\n",
    "            self.total_episodes = save_data['total_episodes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training with 10 episodes (debug run)...\n",
      "Episode 1: Steps=1000, Reward=0.0, Position=[ 2 30], Goal reached=True\n",
      "Episode 2: Steps=1000, Reward=0.0, Position=[ 0 31], Goal reached=True\n",
      "Episode 3: Steps=1000, Reward=0.0, Position=[ 0 31], Goal reached=True\n",
      "Episode 4: Steps=1000, Reward=0.0, Position=[ 2 31], Goal reached=True\n",
      "Episode 5: Steps=1000, Reward=0.0, Position=[ 5 31], Goal reached=True\n",
      "Episode 6: Steps=1000, Reward=0.0, Position=[ 4 31], Goal reached=True\n",
      "Episode 7: Steps=1000, Reward=0.0, Position=[ 7 31], Goal reached=True\n",
      "Episode 8: Steps=1000, Reward=0.0, Position=[ 2 30], Goal reached=True\n",
      "Episode 9: Steps=1000, Reward=0.0, Position=[11 31], Goal reached=True\n",
      "Episode 10: Steps=1000, Reward=0.0, Position=[ 3 28], Goal reached=True\n",
      "\n",
      "Debug training completed!\n",
      "Q-table size: 1357 states\n"
     ]
    }
   ],
   "source": [
    "# Create our Q-learning agent\n",
    "ql_agent = HybridSailingQLearningAgent(learning_rate=0.1, discount_factor=0.99, exploration_rate=0.2)\n",
    "\n",
    "# Set fixed seed for reproducibility\n",
    "np.random.seed(42)\n",
    "ql_agent.seed(42)\n",
    "\n",
    "# Create environment with a simple initial windfield\n",
    "env = SailingEnv(**get_initial_windfield('simple_static'))\n",
    "\n",
    "# Training parameters\n",
    "num_episodes = 10  # Small number for debugging\n",
    "max_steps = 1000\n",
    "\n",
    "# Training loop\n",
    "print(\"Starting training with 10 episodes (debug run)...\")\n",
    "for episode in range(num_episodes):\n",
    "    # Reset environment and get initial state\n",
    "    observation, info = env.reset(seed=episode)  # Different seed each episode\n",
    "    state = ql_agent.discretize_state(observation)\n",
    "    \n",
    "    total_reward = 0\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        # Select action and take step\n",
    "        action = ql_agent.act(observation)\n",
    "        next_observation, reward, done, truncated, info = env.step(action)\n",
    "        next_state = ql_agent.discretize_state(next_observation)\n",
    "        \n",
    "        # Update Q-table\n",
    "        ql_agent.learn(state, action, reward, next_state)\n",
    "        \n",
    "        # Update state and total reward\n",
    "        state = next_state\n",
    "        observation = next_observation\n",
    "        total_reward += reward\n",
    "        \n",
    "        # Break if episode is done\n",
    "        if done or truncated:\n",
    "            break\n",
    "    \n",
    "    print(f\"Episode {episode+1}: Steps={step+1}, Reward={total_reward}, \" +\n",
    "          f\"Position={info['position']}, Goal reached={done}\")\n",
    "    \n",
    "    # Update exploration rate (optional: decrease exploration over time)\n",
    "    ql_agent.exploration_rate = max(0.05, ql_agent.exploration_rate * 0.95)\n",
    "\n",
    "print(\"\\nDebug training completed!\")\n",
    "print(f\"Q-table size: {len(ql_agent.q_table)} states\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Training Run\n",
    "\n",
    "Now let's train our agent for more episodes to get better performance. This will take longer but should result in a more effective agent.\n",
    "\n",
    "*Note: You might want to adjust the number of episodes based on your available time. More episodes generally lead to better performance.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our Q-learning agent for full training\n",
    "ql_agent_full = QLearningAgent(learning_rate=0.1, discount_factor=0.99, exploration_rate=0.3)\n",
    "\n",
    "# Set fixed seed for reproducibility\n",
    "np.random.seed(42)\n",
    "ql_agent_full.seed(42)\n",
    "\n",
    "# Create environment with a simple initial windfield\n",
    "env = SailingEnv(**get_initial_windfield('training_1'))\n",
    "\n",
    "# Training parameters\n",
    "num_episodes = 100  # More episodes for better learning\n",
    "max_steps = 1000\n",
    "\n",
    "# Progress tracking\n",
    "rewards_history = []\n",
    "steps_history = []\n",
    "success_history = []\n",
    "\n",
    "# Training loop\n",
    "print(\"Starting full training with 100 episodes...\")\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    # Reset environment and get initial state\n",
    "    observation, info = env.reset(seed=episode)  # Different seed each episode\n",
    "    state = ql_agent_full.discretize_state(observation)\n",
    "    \n",
    "    total_reward = 0\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        # Select action and take step\n",
    "        action = ql_agent_full.act(observation)\n",
    "        next_observation, reward, done, truncated, info = env.step(action)\n",
    "        next_state = ql_agent_full.discretize_state(next_observation)\n",
    "        \n",
    "        # Update Q-table\n",
    "        ql_agent_full.learn(state, action, reward, next_state)\n",
    "        \n",
    "        # Update state and total reward\n",
    "        state = next_state\n",
    "        observation = next_observation\n",
    "        total_reward += reward\n",
    "        \n",
    "        # Break if episode is done\n",
    "        if done or truncated:\n",
    "            break\n",
    "    \n",
    "    # Record metrics\n",
    "    rewards_history.append(total_reward)\n",
    "    steps_history.append(step+1)\n",
    "    success_history.append(done)\n",
    "    \n",
    "    # Update exploration rate (decrease over time)\n",
    "    ql_agent_full.exploration_rate = max(0.05, ql_agent_full.exploration_rate * 0.98)\n",
    "    \n",
    "    # Print progress every 10 episodes\n",
    "    if (episode + 1) % 10 == 0:\n",
    "        success_rate = sum(success_history[-10:]) / 10 * 100\n",
    "        print(f\"Episode {episode+1}/100: Success rate (last 10): {success_rate:.1f}%\")\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "# Calculate overall success rate\n",
    "success_rate = sum(success_history) / len(success_history) * 100\n",
    "\n",
    "print(f\"\\nTraining completed in {training_time:.1f} seconds!\")\n",
    "print(f\"Success rate: {success_rate:.1f}%\")\n",
    "print(f\"Average reward: {np.mean(rewards_history):.2f}\")\n",
    "print(f\"Average steps: {np.mean(steps_history):.1f}\")\n",
    "print(f\"Q-table size: {len(ql_agent_full.q_table)} states\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting full training with 100 episodes...\n",
      "Episode 10/100: Success rate (last 10): 100.0%\n",
      "Episode 20/100: Success rate (last 10): 100.0%\n",
      "Episode 30/100: Success rate (last 10): 100.0%\n",
      "Episode 40/100: Success rate (last 10): 100.0%\n",
      "Episode 50/100: Success rate (last 10): 100.0%\n",
      "Episode 60/100: Success rate (last 10): 100.0%\n",
      "Episode 70/100: Success rate (last 10): 100.0%\n",
      "Episode 80/100: Success rate (last 10): 100.0%\n",
      "Episode 90/100: Success rate (last 10): 100.0%\n",
      "Episode 100/100: Success rate (last 10): 100.0%\n",
      "Episode 110/100: Success rate (last 10): 100.0%\n",
      "Episode 120/100: Success rate (last 10): 100.0%\n",
      "Episode 130/100: Success rate (last 10): 100.0%\n",
      "Episode 140/100: Success rate (last 10): 100.0%\n",
      "Episode 150/100: Success rate (last 10): 100.0%\n",
      "Episode 160/100: Success rate (last 10): 100.0%\n",
      "Episode 170/100: Success rate (last 10): 100.0%\n",
      "Episode 180/100: Success rate (last 10): 100.0%\n",
      "Episode 190/100: Success rate (last 10): 100.0%\n",
      "Episode 200/100: Success rate (last 10): 100.0%\n",
      "Episode 210/100: Success rate (last 10): 100.0%\n",
      "Episode 220/100: Success rate (last 10): 100.0%\n",
      "Episode 230/100: Success rate (last 10): 100.0%\n",
      "Episode 240/100: Success rate (last 10): 100.0%\n",
      "Episode 250/100: Success rate (last 10): 100.0%\n",
      "Episode 260/100: Success rate (last 10): 100.0%\n",
      "Episode 270/100: Success rate (last 10): 100.0%\n",
      "Episode 280/100: Success rate (last 10): 100.0%\n",
      "Episode 290/100: Success rate (last 10): 100.0%\n",
      "Episode 300/100: Success rate (last 10): 100.0%\n",
      "Episode 310/100: Success rate (last 10): 100.0%\n",
      "Episode 320/100: Success rate (last 10): 100.0%\n",
      "Episode 330/100: Success rate (last 10): 100.0%\n",
      "Episode 340/100: Success rate (last 10): 100.0%\n",
      "Episode 350/100: Success rate (last 10): 100.0%\n",
      "Episode 360/100: Success rate (last 10): 100.0%\n",
      "Episode 370/100: Success rate (last 10): 100.0%\n",
      "Episode 380/100: Success rate (last 10): 100.0%\n",
      "Episode 390/100: Success rate (last 10): 100.0%\n",
      "Episode 400/100: Success rate (last 10): 100.0%\n",
      "Episode 410/100: Success rate (last 10): 100.0%\n",
      "Episode 420/100: Success rate (last 10): 100.0%\n",
      "Episode 430/100: Success rate (last 10): 100.0%\n",
      "Episode 440/100: Success rate (last 10): 100.0%\n",
      "Episode 450/100: Success rate (last 10): 100.0%\n",
      "Episode 460/100: Success rate (last 10): 100.0%\n",
      "Episode 470/100: Success rate (last 10): 100.0%\n",
      "Episode 480/100: Success rate (last 10): 100.0%\n",
      "Episode 490/100: Success rate (last 10): 100.0%\n",
      "Episode 500/100: Success rate (last 10): 100.0%\n",
      "Episode 510/100: Success rate (last 10): 100.0%\n",
      "Episode 520/100: Success rate (last 10): 100.0%\n",
      "Episode 530/100: Success rate (last 10): 100.0%\n",
      "Episode 540/100: Success rate (last 10): 100.0%\n",
      "Episode 550/100: Success rate (last 10): 100.0%\n",
      "Episode 560/100: Success rate (last 10): 100.0%\n",
      "Episode 570/100: Success rate (last 10): 100.0%\n",
      "Episode 580/100: Success rate (last 10): 100.0%\n",
      "Episode 590/100: Success rate (last 10): 100.0%\n",
      "Episode 600/100: Success rate (last 10): 100.0%\n",
      "Episode 610/100: Success rate (last 10): 100.0%\n",
      "Episode 620/100: Success rate (last 10): 100.0%\n",
      "Episode 630/100: Success rate (last 10): 100.0%\n",
      "Episode 640/100: Success rate (last 10): 100.0%\n",
      "Episode 650/100: Success rate (last 10): 100.0%\n",
      "Episode 660/100: Success rate (last 10): 100.0%\n",
      "Episode 670/100: Success rate (last 10): 100.0%\n",
      "Episode 680/100: Success rate (last 10): 100.0%\n",
      "Episode 690/100: Success rate (last 10): 100.0%\n",
      "Episode 700/100: Success rate (last 10): 100.0%\n",
      "Episode 710/100: Success rate (last 10): 100.0%\n",
      "Episode 720/100: Success rate (last 10): 100.0%\n",
      "Episode 730/100: Success rate (last 10): 100.0%\n",
      "Episode 740/100: Success rate (last 10): 100.0%\n",
      "Episode 750/100: Success rate (last 10): 100.0%\n",
      "Episode 760/100: Success rate (last 10): 100.0%\n",
      "Episode 770/100: Success rate (last 10): 100.0%\n",
      "Episode 780/100: Success rate (last 10): 100.0%\n",
      "Episode 790/100: Success rate (last 10): 100.0%\n",
      "Episode 800/100: Success rate (last 10): 100.0%\n",
      "Episode 810/100: Success rate (last 10): 100.0%\n",
      "Episode 820/100: Success rate (last 10): 100.0%\n",
      "Episode 830/100: Success rate (last 10): 100.0%\n",
      "Episode 840/100: Success rate (last 10): 100.0%\n",
      "Episode 850/100: Success rate (last 10): 100.0%\n",
      "Episode 860/100: Success rate (last 10): 100.0%\n",
      "Episode 870/100: Success rate (last 10): 100.0%\n",
      "Episode 880/100: Success rate (last 10): 100.0%\n",
      "Episode 890/100: Success rate (last 10): 100.0%\n",
      "Episode 900/100: Success rate (last 10): 100.0%\n",
      "Episode 910/100: Success rate (last 10): 100.0%\n",
      "Episode 920/100: Success rate (last 10): 100.0%\n",
      "Episode 930/100: Success rate (last 10): 100.0%\n",
      "Episode 940/100: Success rate (last 10): 100.0%\n",
      "Episode 950/100: Success rate (last 10): 100.0%\n",
      "Episode 960/100: Success rate (last 10): 100.0%\n",
      "Episode 970/100: Success rate (last 10): 100.0%\n",
      "Episode 980/100: Success rate (last 10): 100.0%\n",
      "Episode 990/100: Success rate (last 10): 100.0%\n",
      "Episode 1000/100: Success rate (last 10): 100.0%\n",
      "\n",
      "Training completed in 80.7 seconds!\n",
      "Success rate: 100.0%\n",
      "Average reward: 100.00\n",
      "Average steps: 150.8\n",
      "Q-table size: 2732 states\n"
     ]
    }
   ],
   "source": [
    "# Create our Q-learning agent for full training\n",
    "ql_agent_full = HybridSailingQLearningAgent(learning_rate=0.1, discount_factor=0.99, exploration_rate=0.3)\n",
    "\n",
    "# Set fixed seed for reproducibility\n",
    "np.random.seed(42)\n",
    "ql_agent_full.seed(42)\n",
    "\n",
    "# Create environment with a simple initial windfield\n",
    "env = SailingEnv(**get_initial_windfield('training_1'))\n",
    "\n",
    "# Training parameters\n",
    "num_episodes = 100  # More episodes for better learning\n",
    "max_steps = 1000\n",
    "\n",
    "# Progress tracking\n",
    "rewards_history = []\n",
    "steps_history = []\n",
    "success_history = []\n",
    "\n",
    "# Training loop\n",
    "print(\"Starting full training with 100 episodes...\")\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    # Reset environment and get initial state\n",
    "    observation, info = env.reset(seed=episode)  # Different seed each episode\n",
    "    state = ql_agent_full.discretize_state(observation)\n",
    "    \n",
    "    total_reward = 0\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        # Select action and take step\n",
    "        action = ql_agent_full.act(observation)\n",
    "        next_observation, reward, done, truncated, info = env.step(action)\n",
    "        next_state = ql_agent_full.discretize_state(next_observation)\n",
    "        \n",
    "        # Update Q-table\n",
    "        ql_agent_full.learn(state, action, reward, next_state)\n",
    "        \n",
    "        # Update state and total reward\n",
    "        state = next_state\n",
    "        observation = next_observation\n",
    "        total_reward += reward\n",
    "        \n",
    "        # Break if episode is done\n",
    "        if done or truncated:\n",
    "            break\n",
    "    \n",
    "    # Record metrics\n",
    "    rewards_history.append(total_reward)\n",
    "    steps_history.append(step+1)\n",
    "    success_history.append(done)\n",
    "    \n",
    "    # Update exploration rate (decrease over time)\n",
    "    ql_agent_full.exploration_rate = max(0.05, ql_agent_full.exploration_rate * 0.98)\n",
    "    \n",
    "    # Print progress every 10 episodes\n",
    "    if (episode + 1) % 10 == 0:\n",
    "        success_rate = sum(success_history[-10:]) / 10 * 100\n",
    "        print(f\"Episode {episode+1}/100: Success rate (last 10): {success_rate:.1f}%\")\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "# Calculate overall success rate\n",
    "success_rate = sum(success_history) / len(success_history) * 100\n",
    "\n",
    "print(f\"\\nTraining completed in {training_time:.1f} seconds!\")\n",
    "print(f\"Success rate: {success_rate:.1f}%\")\n",
    "print(f\"Average reward: {np.mean(rewards_history):.2f}\")\n",
    "print(f\"Average steps: {np.mean(steps_history):.1f}\")\n",
    "print(f\"Q-table size: {len(ql_agent_full.q_table)} states\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Potential Extensions to the Q-Learning Agent\n",
    "\n",
    "This simplified Q-learning implementation provides a good starting point but has several limitations:\n",
    "\n",
    "1. **Limited State Representation**: It only uses local information (position, velocity, and local wind) without considering the full wind field, which limits the agent's ability to plan ahead.\n",
    "\n",
    "2. **Discrete State Space**: The discretization loses information and may not capture subtle differences in states.\n",
    "\n",
    "3. **Fixed Exploration Rate**: The exploration rate doesn't adapt based on learning progress.\n",
    "\n",
    "#### How to Extend the Agent:\n",
    "\n",
    "1. **Incorporating the Full Wind Field**:\n",
    "   - You could extend the state representation to include information from the full wind field (observation indices 6 onward).\n",
    "   - Create a more sophisticated discretization that captures wind patterns relevant to planning.\n",
    "   - Example approach: Sample key grid points ahead of the boat's position or in the direction of the goal.\n",
    "\n",
    "2. **Function Approximation**:\n",
    "   - Replace the discrete Q-table with a neural network for function approximation.\n",
    "   - This would allow handling continuous state spaces more effectively.\n",
    "\n",
    "3. **Advanced Exploration Strategies**:\n",
    "   - Implement techniques like intrinsic motivation or uncertainty-based exploration.\n",
    "   - Use count-based exploration bonuses for less-visited states.\n",
    "\n",
    "4. **Multi-step Learning**:\n",
    "   - Implement n-step Q-learning or TD(λ) to improve learning efficiency.\n",
    "\n",
    "When extending the agent, remember to modify the `save_qlearning_agent()` function accordingly to properly save your enhanced implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Training Results\n",
    "\n",
    "Let's visualize how our agent improved during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA90AAASmCAYAAAAzjMgKAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8ekN5oAAAACXBIWXMAAA9hAAAPYQGoP6dpAAEAAElEQVR4nOzdeXhU5fk//jsJENYEUdkEEcEK1IW6VFGLG4LKx6WlrrWCbdVaqFXrUlur1o1vrVarVWlrf9VarbZuVWtRVNyXuktdcUVBREES1kCS+f0BM2QlmSSHMOH1uq5ckHNmzjyTnHDxzv0895OXSqVSAQAAALS4/NYeAAAAALRVQjcAAAAkROgGAACAhAjdAAAAkBChGwAAABIidAMAAEBChG4AAABIiNANAAAACRG6AQAAICFCN8B6ZsKECbHFFls06bnnn39+5OXlteyASNR///vf6NChQ3z00UetPZQWk5eXF+eff/46fc3m/NwkZYsttogJEyZkPn/00UcjLy8vHn300cyx9XHcG6pdd901zjzzzNYeBtAGCd0AjZSXl9eoj6r/od6QTJgwodrXoaioKLbffvu4/PLLo6ysrLWHt976xS9+EUcddVQMGDAgc+y///1v/OhHP4odd9wx2rdv3+AvUv785z/H0KFDo2PHjrHVVlvF1VdfnfSwoc0566yz4pprrom5c+e29lCANqZdaw8AIFfcdNNN1T7/61//GtOmTat1fOjQoc16nT/96U9RWVnZpOeec8458bOf/axZr98chYWFcf3110dExMKFC+OOO+6I008/PZ5//vm49dZbW21c66tXXnklHnrooXj66aerHb///vvj+uuvj+222y623HLLeOedd+q9xh/+8If44Q9/GOPGjYvTTjstnnjiiTj55JNj6dKlcdZZZyX9Fuq0bNmyaNfOfzEaozk/77SsQw45JIqKiuLaa6+NCy64oLWHA7QhealUKtXagwDIRZMmTYprrrkmGvpndOnSpdG5c+d1NKrWM2HChLj99ttj8eLFmWOVlZWxyy67xAsvvBCzZ8+Ovn371npeKpWK5cuXR6dOndbJONen78dPfvKTuPvuu+PDDz+sVs3+7LPPoqioKDp16rTW+2zZsmXRv3//2HXXXeO+++7LHD/mmGPi7rvvjo8//jg22mijdfJeWtuECRPi0UcfjQ8//DCx11iyZEl06dKl0Y/fYostYq+99oobbrghIlZNL997771j+vTpsddeeyUzyByV7dc2KT/+8Y/j3nvvjQ8++MBSHaDFmF4O0IL22muv2GabbeLFF1+MkSNHRufOnePnP/95RET861//irFjx0bfvn2jsLAwBg0aFBdeeGFUVFRUu0bNNZ7pQHbZZZfFH//4xxg0aFAUFhbGzjvvHM8//3y159a1pjsvLy8mTZoUd999d2yzzTZRWFgYX/3qV2Pq1Km1xv/oo4/GTjvtFB07doxBgwbFH/7wh2atE8/Pz8+Ei3QY2mKLLeL//u//4oEHHoiddtopOnXqFH/4wx8iIuL999+Pww47LHr06BGdO3eOXXfdNf7973/Xuu5HH30UBx98cHTp0iV69uwZp556ajzwwAO1pvev7ftRVlYW5513XgwePDgKCwujf//+ceaZZ9aaCj9t2rTYY489onv37tG1a9fYeuutM9dIu/rqq+OrX/1qdO7cOTbaaKPYaaed4pZbbmnw63P33XfHPvvsU+vr26tXr0b9EmL69Okxf/78+NGPflTt+MSJE2PJkiV1fu3q8p///Ce+8Y1vRJcuXaJbt24xduzYeP3116s9ZsKECdG1a9d4//33Y8yYMdGlS5fo27dvXHDBBbV+IVBzTfeiRYvilFNOiS222CIKCwujZ8+esd9++8VLL71U7Xn//Oc/Y8cdd4xOnTrFJptsEsccc0zMnj271njT93LHjh1jm222ibvuuqvO91VZWRlXXnllfPWrX42OHTtGr1694sQTT4wvv/yywa9J+v2+9957ceCBB0a3bt3iO9/5TkSsCog//elPo3///lFYWBhbb711XHbZZQ3+Aq6+12nqz3vEqq/ZsGHDqn0tGrtOvDH/Jk2aNCm6du0aS5curfX8o446Knr37l3t8dncS3V9bZ944ok47LDDYvPNN8/8XJ566qmxbNmyJr/3bO6D/fbbLz766KN45ZVXGvz6ATSWuV8ALWz+/PlxwAEHxJFHHhnHHHNM9OrVKyIibrjhhujatWucdtpp0bVr13jkkUfi3HPPjdLS0vjNb37T4HVvueWWWLRoUZx44omRl5cXl156aXzrW9+K999/P9q3b7/W5z755JNx5513xo9+9KPo1q1bXHXVVTFu3LiYNWtWbLzxxhER8fLLL8f+++8fffr0iV/96ldRUVERF1xwQWy66abN+nq89957ERGZ14mIePvtt+Ooo46KE088MY4//vjYeuut47PPPovddtstli5dGieffHJsvPHGceONN8bBBx8ct99+e3zzm9+MiFWBZ5999olPP/00fvKTn0Tv3r3jlltuienTp9f5+nV9PyorK+Pggw+OJ598Mk444YQYOnRozJgxI6644op455134u67746IiNdffz3+7//+L7bbbru44IILorCwMN5999146qmnMtf/05/+FCeffHJ8+9vfjp/85CexfPnyeO211+K5556Lo48+ut6vy+zZs2PWrFmxww47NPlr+/LLL0dExE477VTt+I477hj5+fnx8ssvxzHHHLPWa9x0000xfvz4GDNmTPz617+OpUuXxnXXXRd77LFHvPzyy9UCTEVFRey///6x6667xqWXXhpTp06N8847L8rLy9c6HfeHP/xh3H777TFp0qQYNmxYzJ8/P5588sl48803M+//hhtuiOOOOy523nnnmDx5cnz22Wfxu9/9Lp566ql4+eWXo3v37hER8eCDD8a4ceNi2LBhMXny5Jg/f34cd9xx0a9fv1qve+KJJ2aue/LJJ8cHH3wQv//97+Pll1+Op556qsGfm/Ly8hgzZkzssccecdlll0Xnzp0jlUrFwQcfHNOnT4/vf//7MXz48HjggQfijDPOiNmzZ8cVV1yx1ms2VmN+3v/973/HEUccEdtuu21Mnjw5vvzyy/j+978fm222WaNeozH/Jh1xxBFxzTXXxL///e847LDDMs9dunRp3HvvvTFhwoQoKCiIiOzupbq+thGrgvTSpUvjpJNOio033jj++9//xtVXXx2ffPJJ/POf/8w8P5v3ns19sOOOO0ZExFNPPRVf+9rXGvV1BGhQCoAmmThxYqrmP6N77rlnKiJSU6ZMqfX4pUuX1jp24oknpjp37pxavnx55tj48eNTAwYMyHz+wQcfpCIitfHGG6cWLFiQOf6vf/0rFRGpe++9N3PsvPPOqzWmiEh16NAh9e6772aOvfrqq6mISF199dWZYwcddFCqc+fOqdmzZ2eOzZw5M9WuXbta16zL+PHjU126dEl9/vnnqc8//zz17rvvpi655JJUXl5earvttss8bsCAAamISE2dOrXa80855ZRURKSeeOKJzLFFixalBg4cmNpiiy1SFRUVqVQqlbr88stTEZG6++67M49btmxZasiQIamISE2fPj1zvL7vx0033ZTKz8+v9lqpVCo1ZcqUVESknnrqqVQqlUpdccUVqYhIff755/W+70MOOST11a9+tcGvT00PPfRQre9fXeq6z6qeKygoqPPcpptumjryyCPXeu1Fixalunfvnjr++OOrHZ87d26quLi42vHx48enIiL14x//OHOssrIyNXbs2FSHDh2qfY0iInXeeedlPi8uLk5NnDix3nGsWLEi1bNnz9Q222yTWrZsWeb4fffdl4qI1Lnnnps5Nnz48FSfPn1SCxcuzBx78MEHUxFR7efmiSeeSEVE6uabb672WlOnTq3zeE3p9/uzn/2s2vG77747FRGpiy66qNrxb3/726m8vLxqP2cDBgxIjR8/PvP59OnTa92jzfl533bbbVP9+vVLLVq0KHPs0UcfrfW1qE9j/k2qrKxMbbbZZqlx48ZVe9w//vGPVESkHn/88VQq1bR7qebXtr4xTZ48OZWXl5f66KOPsn7vTbkPOnTokDrppJNqHQdoKtPLAVpYYWFhHHfccbWOV50uvGjRovjiiy/iG9/4RixdujTeeuutBq97xBFHVFuf+41vfCMiVk3JbsioUaNi0KBBmc+32267KCoqyjy3oqIiHnrooTj00EOrrbsePHhwHHDAAQ1eP23JkiWx6aabxqabbhqDBw+On//85zFixIha038HDhwYY8aMqXbs/vvvj69//euxxx57ZI517do1TjjhhPjwww/jjTfeiIiIqVOnxmabbRYHH3xw5nEdO3aM448/vs4x1fX9+Oc//xlDhw6NIUOGxBdffJH52GeffSIiMlXzdHX1X//6V73Nrrp37x6ffPJJnVN/12b+/PkREc1ac71s2bLo0KFDnec6duxY55TcqqZNmxYLFy6Mo446qtrXoaCgIHbZZZc6Zw9MmjQp8/f00oUVK1bEQw89VO/rdO/ePZ577rmYM2dOnedfeOGFmDdvXvzoRz+Kjh07Zo6PHTs2hgwZkpkm/+mnn8Yrr7wS48ePj+Li4szj9ttvvxg2bFi1a/7zn/+M4uLi2G+//aq9tx133DG6du1a78yImk466aRqn99///1RUFAQJ598crXjP/3pTyOVSsV//vOfRl23IQ39vM+ZMydmzJgRxx57bHTt2jXzuD333DO23XbbRr1GY/5NysvLi8MOOyzuv//+av0abrvttthss80yP69NuZdqfm1rjmnJkiXxxRdfxG677RapVCozsyOb996U+2CjjTaKL774olFfQ4DGML0coIVtttlmdQah119/Pc4555x45JFHorS0tNq5kpKSBq+7+eabV/s8/R/yxqxPrfnc9PPTz503b14sW7YsBg8eXOtxdR2rT8eOHePee++NiFVhd+DAgXVO+x04cGCtYx999FHssssutY6nu8F/9NFHsc0228RHH30UgwYNqrUOur5x1vX9mDlzZrz55pv1Tp2fN29eRKwKPtdff3384Ac/iJ/97Gex7777xre+9a349re/Hfn5q35vfdZZZ8VDDz0UX//612Pw4MExevToOProo2P33Xev89o1pZrRz7RTp06xYsWKOs9VbU63ePHiaoGpoKAgNt1005g5c2ZEROaXDTUVFRVV+zw/Pz+23HLLase+8pWvRESstYHZpZdeGuPHj4/+/fvHjjvuGAceeGAce+yxmWul9yjfeuutaz13yJAh8eSTT1Z73FZbbVXrcVtvvXW1NeIzZ86MkpKS6NmzZ51jSn+P16Zdu3a17t+PPvoo+vbtG926dat2vOp92hIa+nlPv059P7M118vXpbH/Jh1xxBFx5ZVXxj333BNHH310LF68OO6///7M1PeIyPpequtrGxExa9asOPfcc+Oee+6p9W9bekzZvPem3AepVEoTNaBFCd0ALayuBlgLFy6MPffcM4qKiuKCCy6IQYMGRceOHeOll16Ks846q1FbBqXXTdbUmNDWnOdmo6CgIEaNGtXg49ZVp/L6XquysjK23Xbb+O1vf1vnc/r375957uOPPx7Tp0+Pf//73zF16tS47bbbYp999okHH3wwCgoKYujQofH222/HfffdF1OnTo077rgjrr322jj33HPjV7/6Vb3jSq9xb8wvTerTp0+fqKioiHnz5lULFStWrIj58+dnZi1cdtll1cYyYMCA+PDDDzP33U033RS9e/eudf2W2vbr8MMPj2984xtx1113xYMPPhi/+c1v4te//nXceeedWc2kyEZlZWX07Nkzbr755jrPN6ZXQWFhYeaXK+ta0j+z2fybtOuuu8YWW2wR//jHP+Loo4+Oe++9N5YtWxZHHHFE5jHZ3kt1fW0rKipiv/32iwULFsRZZ50VQ4YMiS5dusTs2bNjwoQJTdparSn3wcKFC2OTTTbJ+rUA6iN0A6wDjz76aMyfPz/uvPPOGDlyZOb4Bx980IqjWqNnz57RsWPHePfdd2udq+tYEgYMGBBvv/12rePpaa4DBgzI/PnGG2/UqkZlM85BgwbFq6++Gvvuu2+DFa38/PzYd999Y999943f/va3cckll8QvfvGLmD59euYXDF26dIkjjjgijjjiiFixYkV861vfiosvvjjOPvvsatOlqxoyZEhENO8eGD58eESsmp594IEHZo6/8MILUVlZmTl/7LHHVpu2n/5FRHrJQc+ePRv1y5LKysp4//33M9XtiMjsId5Qt+w+ffrEj370o/jRj34U8+bNix122CEuvvjiOOCAAzLf27fffrtWpfTtt9+u9r2PWFNVrfm4qgYNGhQPPfRQ7L777i36S54BAwbEQw89FIsWLapW7a55nyYt/TpN/ZnN9t+kww8/PH73u99FaWlp3HbbbbHFFlvErrvumjmf7b1UlxkzZsQ777wTN954Yxx77LGZ49OmTav2uGzee7b3wezZs2PFihWZmQsALcGaboB1IF21qlqlWrFiRVx77bWtNaRq0hXqu+++u9q623fffbfF1qg25MADD4z//ve/8cwzz2SOLVmyJP74xz/GFltskVmzO2bMmJg9e3bcc889mcctX748/vSnPzX6tQ4//PCYPXt2nc9ZtmxZLFmyJCIiFixYUOt8OsimtxZLr81O69ChQwwbNixSqVSsXLmy3jFsttlm0b9//3jhhRcaPe6a9tlnn+jRo0dcd9111Y5fd9110blz5xg7dmxERGy55ZYxatSozEd66vuYMWOiqKgoLrnkkjrH+vnnn9c69vvf/z7z91QqFb///e+jffv2se+++9Y5xoqKilrLJ3r27Bl9+/bNfA132mmn6NmzZ0yZMqXalm3/+c9/4s0338y8jz59+sTw4cPjxhtvrHbNadOmZdb8px1++OFRUVERF154Ya0xlZeXx8KFC+scb0MOPPDAqKioqPZ1iIi44oorIi8vL7HKfU19+/aNbbbZJv76179WWzrw2GOPxYwZMxp8frb/Jh1xxBFRVlYWN954Y0ydOjUOP/zwauebci81ZkypVCp+97vfVXtcNu892/vgxRdfjIiI3XbbrcHxAjSWSjfAOrDbbrvFRhttFOPHj4+TTz458vLy4qabbmrx6d3Ncf7558eDDz4Yu+++e5x00kmZYLHNNtuskz1rf/azn8Xf//73OOCAA+Lkk0+OHj16xI033hgffPBB3HHHHZmpqCeeeGL8/ve/j6OOOip+8pOfRJ8+feLmm2/OVJQbsxbzu9/9bvzjH/+IH/7whzF9+vTYfffdo6KiIt566634xz/+kdlD/IILLojHH388xo4dGwMGDIh58+bFtddeG/369ctUjkePHh29e/eO3XffPXr16hVvvvlm/P73v4+xY8fWWvdb0yGHHBJ33XVXrar9Rx99FDfddFNERCaUX3TRRRGxqsr33e9+NyJWVawvvPDCmDhxYhx22GExZsyYeOKJJ+Jvf/tbXHzxxdGjR4+1vn5RUVFcd9118d3vfjd22GGHOPLII2PTTTeNWbNmxb///e/Yfffdq4XLjh07xtSpU2P8+PGxyy67xH/+85/497//HT//+c/rna69aNGi6NevX3z729+O7bffPrp27RoPPfRQPP/883H55ZdHRET79u3j17/+dRx33HGx5557xlFHHZXZMmyLLbaIU089NXO9yZMnx9ixY2OPPfaI733ve7FgwYLMPulVA9iee+4ZJ554YkyePDleeeWVGD16dLRv3z5mzpwZ//znP+N3v/tdfPvb317r16cuBx10UOy9997xi1/8Ij788MPYfvvt48EHH4x//etfccopp1RrWJi0Sy65JA455JDYfffd47jjjosvv/wy8zNb9WtRl2z/Tdphhx1i8ODB8Ytf/CLKysqqTS2PyP5eqsuQIUNi0KBBcfrpp8fs2bOjqKgo7rjjjjqXYDT2vWd7H0ybNi0233xz24UBLWud90sHaCPq2zKsvu2jnnrqqdSuu+6a6tSpU6pv376pM888M/XAAw80eguh3/zmN7WuGTW2Zqpvy7C6tmuquZ1RKpVKPfzww6mvfe1rqQ4dOqQGDRqUuv7661M//elPUx07dqznq7BGesuwhgwYMCA1duzYOs+99957qW9/+9up7t27pzp27Jj6+te/nrrvvvtqPe79999PjR07NtWpU6fUpptumvrpT3+auuOOO1IRkXr22Wczj1vb92PFihWpX//616mvfvWrqcLCwtRGG22U2nHHHVO/+tWvUiUlJZmvxyGHHJLq27dvqkOHDqm+ffumjjrqqNQ777yTuc4f/vCH1MiRI1Mbb7xxqrCwMDVo0KDUGWeckbnG2rz00ku1tklLpdZsLVXXx5577lnrOn/84x9TW2+9deb7dsUVV6QqKysbfP2qrzdmzJhUcXFxqmPHjqlBgwalJkyYkHrhhRcyj0l/f997773U6NGjU507d0716tUrdd5552W2c0urel+WlZWlzjjjjNT222+f6tatW6pLly6p7bffPnXttdfWGsdtt92W+trXvpYqLCxM9ejRI/Wd73wn9cknn9R63B133JEaOnRoqrCwMDVs2LDUnXfeWevnpurXZscdd0x16tQp1a1bt9S2226bOvPMM1Nz5sxZ69dkbffzokWLUqeeemqqb9++qfbt26e22mqr1G9+85taX/PmbBnWmJ/3VCqVuvXWW1NDhgxJFRYWprbZZpvUPffckxo3blxqyJAha31/qVTj/01K+8UvfpGKiNTgwYPrvWY291Jd3njjjdSoUaNSXbt2TW2yySap448/PrPF4V/+8pcmv/fG3AcVFRWpPn36pM4555wGvnIA2clLpdajMgsA651DDz00Xn/99TrX0a5Prrzyyjj11FPjk08+ic0226y1h9No++67b/Tt2zdT2V5fTZgwIW6//fYGK6i0vuHDh8emm25aay30hqA57/3uu++Oo48+Ot57773o06dPAqMDNlTWdAOQUXNf55kzZ8b9998fe+21V+sMqB41x7l8+fL4wx/+EFtttVVOBe6IVdNkb7vtthbbaooNx8qVK6O8vLzasUcffTReffXV9e5ntqUl8d5//etfx6RJkwRuoMVZ0w1AxpZbbhkTJkyILbfcMj766KO47rrrokOHDnHmmWe29tCq+da3vhWbb755DB8+PEpKSuJvf/tbvPXWW/VuC7Q+22WXXerdaxvWZvbs2TFq1Kg45phjom/fvvHWW2/FlClTonfv3vHDH/6wtYeXqCTee9UmjgAtSegGIGP//fePv//97zF37twoLCyMESNGxCWXXBJbbbVVaw+tmjFjxsT1118fN998c1RUVMSwYcPi1ltvrdXcCdqyjTbaKHbccce4/vrr4/PPP48uXbrE2LFj4//9v/+X2Qe+rdqQ3zuQe6zpBgAAgIRY0w0AAAAJEboBAAAgIdZ0R0RlZWXMmTMnunXrFnl5ea09HAAAANZzqVQqFi1aFH379o38/Prr2UJ3RMyZMyf69+/f2sMAAAAgx3z88cfRr1+/es8L3RHRrVu3iFj1xSoqKmrl0QAAALC+Ky0tjf79+2fyZH2E7ojMlPKioiKhGwAAgEZraImyRmoAAACQEKEbAAAAEiJ0AwAAQEKEbgAAAEiI0A0AAAAJEboBAAAgIUI3AAAAJEToBgAAgIQI3QAAAJAQoRsAAAASInQDAABAQoRuAAAASIjQDQAAAAkRugEAACAhQjcAAAAkROgGAACAhAjdAAAAkBChGwAAABIidAMAAEBChG4AAABIiNANAAAACRG6AQAAICFCNwAAACRE6AYAAICECN0AAACQEKEbAAAAEiJ0AwAAQEKEbgAAAEiI0A0AAAAJEboBAAAgIUI3AAAAJEToBgAAgIQI3QAAAJAQoRsAAAASInQDAABAQoRuAAAASIjQDQAAAAkRugEAACAhQjcAAAAkROgGAACAhAjdAAAAkBChGwAAABIidAMAAEBChG4AAABIiNANAAAACRG6AQAAICFCNwAAACRE6AYAAICECN0AAACQEKEbAAAAEiJ0AwAAQEKEbgAAAEiI0A0AAAAJEboBAAAgIUI3AAAAJEToBgAAgIQI3QAAAJAQoRsAAAASInQDAABAQoRuAAAASIjQDQAAAAkRugEAACAhQjcAAAAkROgGAACAhAjdAAAAkBChGwAAABIidAMAAEBChG4AAABISKuG7scffzwOOuig6Nu3b+Tl5cXdd99d7XwqlYpzzz03+vTpE506dYpRo0bFzJkz67xWWVlZDB8+PPLy8uKVV15JfvAAAADQgFYN3UuWLIntt98+rrnmmjrPX3rppXHVVVfFlClT4rnnnosuXbrEmDFjYvny5bUee+aZZ0bfvn2THjIAAAA0WrvWfPEDDjggDjjggDrPpVKpuPLKK+Occ86JQw45JCIi/vrXv0avXr3i7rvvjiOPPDLz2P/85z/x4IMPxh133BH/+c9/1snYAQAAoCHr7ZruDz74IObOnRujRo3KHCsuLo5ddtklnnnmmcyxzz77LI4//vi46aabonPnzq0xVAAAAKhTq1a612bu3LkREdGrV69qx3v16pU5l0qlYsKECfHDH/4wdtppp/jwww8bde2ysrIoKyvLfF5aWtoygwYAAIAq1ttKd2NcffXVsWjRojj77LOzet7kyZOjuLg489G/f/+ERggAAMCGbL0N3b17946IVdPHq/rss88y5x555JF45plnorCwMNq1axeDBw+OiIiddtopxo8fX++1zz777CgpKcl8fPzxxwm9CwAAADZk6+308oEDB0bv3r3j4YcfjuHDh0fEqmngzz33XJx00kkREXHVVVfFRRddlHnOnDlzYsyYMXHbbbfFLrvsUu+1CwsLo7CwMNHxAwAAQKuG7sWLF8e7776b+fyDDz6IV155JXr06BGbb755nHLKKXHRRRfFVlttFQMHDoxf/vKX0bdv3zj00EMjImLzzTevdr2uXbtGRMSgQYOiX79+6+x9AAAAQF1aNXS/8MILsffee2c+P+200yIiYvz48XHDDTfEmWeeGUuWLIkTTjghFi5cGHvssUdMnTo1Onbs2FpDBgAAgEbLS6VSqdYeRGsrLS2N4uLiKCkpiaKiotYeDgAAAOu5xubI9baRGgAAAOQ6oRsAAAASInQDAABAQoRuAAAASIjQDQAAAAkRugEAACAhQjcAAAAkROgGAACAhAjdAAAAkBChGwAAABIidAMAAEBChG4AAABIiNANAAAACRG6AQAAICFCNwAAACRE6AYAAICECN0AAACQEKEbAAAAEiJ0AwAAQEKEbgAAAEiI0A0AAAAJEboBAAAgIUI3AAAAJEToBgAAgIQI3QAAAJAQoRsAAAASInQDAABAQoRuAAAASIjQDQAAAAkRugEAACAhQjcAAAAkROgGAACAhAjdAAAAkBChGwAAABIidAMAAEBChG4AAABIiNANAAAACRG6AQAAICFCNwAAACRE6AYAAICECN0AAACQEKEbAAAAEiJ0AwAAQEKEbgAAAEiI0A0AAAAJEboBAAAgIUI3AAAAJEToBgAAgIQI3QAAAJAQoRsAAAASInQDAABAQoRuAAAASIjQDQAAAAkRugEAACAhQjcAAAAkROgGAACAhAjdAAAAkBChGwAAABIidAMAAEBChG4AAABIiNANAAAACRG6AQAAICFCNwAAACRE6AYAAICECN0AAACQEKEbAAAAEiJ0AwAAQEKEbgAAAEiI0A0AAAAJEboBAAAgIUI3AAAAJEToBgAAgIQI3QAAAJAQoRsAAAASInQDAABAQoRuAAAASIjQDQAAAAkRugEAACAhQjcAAAAkROgGAACAhAjdAAAAkBChGwAAABIidAMAAEBChG4AAABIiNANAAAACRG6AQAAICFCNwAAACRE6AYAAICECN0AAACQkFYN3Y8//ngcdNBB0bdv38jLy4u777672vlUKhXnnntu9OnTJzp16hSjRo2KmTNnZs5/+OGH8f3vfz8GDhwYnTp1ikGDBsV5550XK1asWMfvBAAAAGpr1dC9ZMmS2H777eOaa66p8/yll14aV111VUyZMiWee+656NKlS4wZMyaWL18eERFvvfVWVFZWxh/+8Id4/fXX44orrogpU6bEz3/+83X5NgAAAKBOealUKtXag4iIyMvLi7vuuisOPfTQiFhV5e7bt2/89Kc/jdNPPz0iIkpKSqJXr15xww03xJFHHlnndX7zm9/EddddF++//36jX7u0tDSKi4ujpKQkioqKmv1eAAAAaNsamyPX2zXdH3zwQcydOzdGjRqVOVZcXBy77LJLPPPMM/U+r6SkJHr06LEuhggAAABr1a61B1CfuXPnRkREr169qh3v1atX5lxN7777blx99dVx2WWXrfXaZWVlUVZWlvm8tLS0maMFAACA2tbbSne2Zs+eHfvvv38cdthhcfzxx6/1sZMnT47i4uLMR//+/dfRKAEAANiQrLehu3fv3hER8dlnn1U7/tlnn2XOpc2ZMyf23nvv2G233eKPf/xjg9c+++yzo6SkJPPx8ccft9zAAQAAYLX1NnQPHDgwevfuHQ8//HDmWGlpaTz33HMxYsSIzLHZs2fHXnvtFTvuuGP85S9/ifz8ht9SYWFhFBUVVfsAAACAltaqa7oXL14c7777bubzDz74IF555ZXo0aNHbL755nHKKafERRddFFtttVUMHDgwfvnLX0bfvn0zHc7TgXvAgAFx2WWXxeeff565Vs1qOAAAAKxrrRq6X3jhhdh7770zn5922mkRETF+/Pi44YYb4swzz4wlS5bECSecEAsXLow99tgjpk6dGh07doyIiGnTpsW7774b7777bvTr16/atdeTndAAAADYgK03+3S3Jvt0AwAAkI2c36cbAAAAcp3QDQAAAAkRugEAACAhQjcAAAAkROgGAACAhAjdAAAAkBChGwAAABIidAMAAEBChG4AAABIiNANAAAACRG6AQAAICFCNwAAACRE6AYAAICECN0AAACQEKEbAAAAEiJ0AwAAQEKEbgAAAEiI0A0AAAAJEboBAAAgIUI3AAAAJEToBgAAgIQI3QAAAJAQoRsAAAASInQDAABAQoRuAAAASIjQDQAAAAkRugEAACAhQjcAAAAkROgGAACAhAjdAAAAkBChGwAAABIidAMAAEBChG4AAABIiNANAAAACRG6AQAAICFCNwAAACRE6AYAAICECN0AAACQEKEbAAAAEtKuMQ967bXXGn3B7bbbrsmDAQAAgLakUaF7+PDhkZeXF6lUKvLy8tb62IqKihYZGAAAAOS6Rk0v/+CDD+L999+PDz74IO64444YOHBgXHvttfHyyy/Hyy+/HNdee20MGjQo7rjjjqTHCwAAADmjUZXuAQMGZP5+2GGHxVVXXRUHHnhg5th2220X/fv3j1/+8pdx6KGHtvggAQAAIBdl3UhtxowZMXDgwFrHBw4cGG+88UaLDAoAAADagqxD99ChQ2Py5MmxYsWKzLEVK1bE5MmTY+jQoS06OAAAAMhljZpeXtWUKVPioIMOin79+mU6lb/22muRl5cX9957b4sPEAAAAHJVXiqVSmX7pCVLlsTNN98cb731VkSsqn4fffTR0aVLlxYf4LpQWloaxcXFUVJSEkVFRa09HAAAANZzjc2RWVW6V65cGUOGDIn77rsvTjjhhGYPEgAAANqyrNZ0t2/fPpYvX57UWAAAAKBNybqR2sSJE+PXv/51lJeXJzEeAAAAaDOybqT2/PPPx8MPPxwPPvhgbLvttrXWcd95550tNjgAAADIZVmH7u7du8e4ceOSGAsAAAC0KVmH7r/85S9JjAMAAADanKzXdAMAAACNk3WlOyLi9ttvj3/84x8xa9asWLFiRbVzL730UosMDAAAAHJd1pXuq666Ko477rjo1atXvPzyy/H1r389Nt5443j//ffjgAMOSGKMAAAAkJOyDt3XXntt/PGPf4yrr746OnToEGeeeWZMmzYtTj755CgpKUlijAAAAJCTsg7ds2bNit122y0iIjp16hSLFi2KiIjvfve78fe//71lRwcAAAA5LOvQ3bt371iwYEFERGy++ebx7LPPRkTEBx98EKlUqmVHBwAAADks69C9zz77xD333BMREccdd1yceuqpsd9++8URRxwR3/zmN1t8gAAAAJCr8lJZlqcrKyujsrIy2rVb1fj81ltvjaeffjq22mqrOPHEE6NDhw6JDDRJpaWlUVxcHCUlJVFUVNTawwEAAGA919gcmXXobouEbgAAALLR2ByZ9T7dI0eOjL322iv23HPP2H333aNjx47NGigAAAC0VVmv6R49enQ8++yzccghh0T37t1jjz32iHPOOSemTZsWS5cuTWKMAAAAkJOaPL28vLw8nn/++Xjsscfi0UcfjUceeSTy8/Nj+fLlLT3GxJleDgAAQDYSm16e9v7778eMGTPi1Vdfjddeey26desWI0eObOrlAAAAoM3JOnQfffTR8dhjj0VZWVmMHDky9txzz/jZz34W2223XeTl5SUxRgAAAMhJWYfuW2+9NTbZZJP4wQ9+EPvss0/sscce0blz5yTGBgAAADkt60Zq8+fPj+uvvz5WrFgRZ599dmyyySax2267xc9//vN48MEHkxgjAAAA5KRm79P97rvvxkUXXRQ333xzVFZWRkVFRUuNbZ3RSA0AAIBsJNZIbf78+ZmO5Y8++mi88cYb0b179zjooINizz33bNagAQAAoC3JOnT37NkzNtlkk/jGN74Rxx9/fOy1116x7bbbJjE2AAAAyGlZh+7XXnstvvrVryYxFgAAAGhTsm6k9tWvfjXKy8vjoYceij/84Q+xaNGiiIiYM2dOLF68uMUHCAAAALkq60r3Rx99FPvvv3/MmjUrysrKYr/99otu3brFr3/96ygrK4spU6YkMU4AAADIOVlXun/yk5/ETjvtFF9++WV06tQpc/yb3/xmPPzwwy06OAAAAMhlWVe6n3jiiXj66aejQ4cO1Y5vscUWMXv27BYbGAAAAOS6rCvd9e3F/cknn0S3bt1aZFAAAADQFmQdukePHh1XXnll5vO8vLxYvHhxnHfeeXHggQe25NgAAAAgp+WlUqlUNk/45JNPYsyYMZFKpWLmzJmx0047xcyZM2OTTTaJxx9/PHr27JnUWBNTWloaxcXFUVJSEkVFRa09HAAAANZzjc2RWYfuiIjy8vK47bbb4tVXX43FixfHDjvsEN/5zneqNVbLJUI3AAAA2Ug0dNfl008/jYsvvjh+//vft8Tl1imhGwAAgGw0Nkdm1b389ddfj+nTp0eHDh3i8MMPj+7du8cXX3wRF198cUyZMiW23HLLZg8cAAAA2opGN1K755574mtf+1qcfPLJ8cMf/jB22mmnmD59egwdOjTefPPNuOuuu+L1119PcqwAAACQUxodui+66KKYOHFilJaWxm9/+9t4//334+STT477778/pk6dGvvvv3+S4wQAAICc0+g13cXFxfHiiy/G4MGDo6KiIgoLC2Pq1KkxatSopMeYOGu6AQAAyEZjc2SjK92LFi3KXKigoCA6depkDTcAAACsRVaN1B544IEoLi6OiIjKysp4+OGH43//+1+1xxx88MEtNzoAAADIYY2eXp6f33BRPC8vLyoqKpo9qHXN9HIAAACy0eJbhlVWVrbIwAAAAGBD0eg13Ul4/PHH46CDDoq+fftGXl5e3H333dXOp1KpOPfcc6NPnz7RqVOnGDVqVMycObPaYxYsWBDf+c53oqioKLp37x7f//73Y/HixevwXQAAAEDdWjV0L1myJLbffvu45ppr6jx/6aWXxlVXXRVTpkyJ5557Lrp06RJjxoyJ5cuXZx7zne98J15//fWYNm1a3HffffH444/HCSecsK7eAgAAANSr0Wu6k5aXlxd33XVXHHrooRGxqsrdt2/f+OlPfxqnn356RESUlJREr1694oYbbogjjzwy3nzzzRg2bFg8//zzsdNOO0VExNSpU+PAAw+MTz75JPr27duo117f13SnUqlYtjL31soDAAA0Vaf2BZGXl9faw6hXi6/pXtc++OCDmDt3brV9wIuLi2OXXXaJZ555Jo488sh45plnonv37pnAHRExatSoyM/Pj+eeey6++c1v1nntsrKyKCsry3xeWlqa3BtpActWVsSwcx9o7WEAAACsM29cMCY6d1hvI2ujter08rWZO3duRET06tWr2vFevXplzs2dOzd69uxZ7Xy7du2iR48emcfUZfLkyVFcXJz56N+/fwuPHgAAAJpY6V64cGHcfvvt8d5778UZZ5wRPXr0iJdeeil69eoVm222WUuPscWdffbZcdppp2U+Ly0tXa+Dd6f2BfHGBWNaexgAAADrTKf2Ba09hBaRdeh+7bXXYtSoUVFcXBwffvhhHH/88dGjR4+48847Y9asWfHXv/61RQbWu3fviIj47LPPok+fPpnjn332WQwfPjzzmHnz5lV7Xnl5eSxYsCDz/LoUFhZGYWFhi4xzXcjLy2sT0yoAAAA2NFlPLz/ttNNiwoQJMXPmzOjYsWPm+IEHHhiPP/54iw1s4MCB0bt373j44Yczx0pLS+O5556LESNGRETEiBEjYuHChfHiiy9mHvPII49EZWVl7LLLLi02FgAAAGiKrMunzz//fPzhD3+odXyzzTZb6zrquixevDjefffdzOcffPBBvPLKK9GjR4/YfPPN45RTTomLLroottpqqxg4cGD88pe/jL59+2Y6nA8dOjT233//OP7442PKlCmxcuXKmDRpUhx55JGN7lwOAAAASck6dBcWFtbZ7fudd96JTTfdNKtrvfDCC7H33ntnPk+vsx4/fnzccMMNceaZZ8aSJUvihBNOiIULF8Yee+wRU6dOrVZhv/nmm2PSpEmx7777Rn5+fowbNy6uuuqqbN8WAAAAtLis9+n+wQ9+EPPnz49//OMf0aNHj3jttdeioKAgDj300Bg5cmRceeWVCQ01Oev7Pt0AAACsXxqbI7Ne03355ZfH4sWLo2fPnrFs2bLYc889Y/DgwdGtW7e4+OKLmzVoAAAAaEuynl5eXFwc06ZNiyeffDJee+21WLx4ceywww4xatSoJMYHAAAAOSvr6eVtkenlAAAAZKOxOTLrSnd9Tcry8vKiY8eOMXjw4Bg5cmQUFLSNjcwBAACgqbIO3VdccUV8/vnnsXTp0thoo40iIuLLL7+Mzp07R9euXWPevHmx5ZZbxvTp06N///4tPmAAAADIFVk3Urvkkkti5513jpkzZ8b8+fNj/vz58c4778Quu+wSv/vd72LWrFnRu3fvOPXUU5MYLwAAAOSMrNd0Dxo0KO64444YPnx4teMvv/xyjBs3Lt5///14+umnY9y4cfHpp5+25FgTY003AAAA2Uhsy7BPP/00ysvLax0vLy+PuXPnRkRE3759Y9GiRdleGgAAANqUrEP33nvvHSeeeGK8/PLLmWMvv/xynHTSSbHPPvtERMSMGTNi4MCBLTdKAAAAyEFZh+4///nP0aNHj9hxxx2jsLAwCgsLY6eddooePXrEn//854iI6Nq1a1x++eUtPlgAAADIJU3ep/utt96Kd955JyIitt5669h6661bdGDrkjXdAAAAZCOxfbrThgwZEkOGDGnq0wEAAKDNa1Lo/uSTT+Kee+6JWbNmxYoVK6qd++1vf9siAwMAAIBcl3Xofvjhh+Pggw+OLbfcMt56663YZptt4sMPP4xUKhU77LBDEmMEAACAnJR1I7Wzzz47Tj/99JgxY0Z07Ngx7rjjjvj4449jzz33jMMOOyyJMQIAAEBOyjp0v/nmm3HsscdGRES7du1i2bJl0bVr17jgggvi17/+dYsPEAAAAHJV1qG7S5cumXXcffr0iffeey9z7osvvmi5kQEAAECOy3pN96677hpPPvlkDB06NA488MD46U9/GjNmzIg777wzdt111yTGCAAAADkp69D929/+NhYvXhwREb/61a9i8eLFcdttt8VWW22lczkAAABUkVXorqioiE8++SS22267iFg11XzKlCmJDAwAAAByXVZrugsKCmL06NHx5ZdfJjUeAAAAaDOybqS2zTbbxPvvv5/EWAAAAKBNyTp0X3TRRXH66afHfffdF59++mmUlpZW+wAAAABWyUulUqlsnpCfvyan5+XlZf6eSqUiLy8vKioqWm5060hpaWkUFxdHSUlJFBUVtfZwAAAAWM81Nkdm3b18+vTpzRoYAAAAbCiyDt177rlnEuMAAACANifrNd0REU888UQcc8wxsdtuu8Xs2bMjIuKmm26KJ598skUHBwAAALks69B9xx13xJgxY6JTp07x0ksvRVlZWURElJSUxCWXXNLiAwQAAIBc1aTu5VOmTIk//elP0b59+8zx3XffPV566aUWHRwAAADksqxD99tvvx0jR46sdby4uDgWLlzYEmMCAACANiHr0N27d+949913ax1/8sknY8stt2yRQQEAAEBbkHXoPv744+MnP/lJPPfcc5GXlxdz5syJm2++OU4//fQ46aSTkhgjAAAA5KSstwz72c9+FpWVlbHvvvvG0qVLY+TIkVFYWBinn356/PjHP05ijAAAAJCT8lKpVKopT1yxYkW8++67sXjx4hg2bFh07dq1pce2zpSWlkZxcXGUlJREUVFRaw8HAACA9Vxjc2TW08v/9re/xdKlS6NDhw4xbNiw+PrXv57TgRsAAACSknXoPvXUU6Nnz55x9NFHx/333x8VFRVJjAsAAAByXtah+9NPP41bb7018vLy4vDDD48+ffrExIkT4+mnn05ifAAAAJCzmrymOyJi6dKlcdddd8Utt9wSDz30UPTr1y/ee++9lhzfOmFNNwAAANlobI7Munt5VZ07d44xY8bEl19+GR999FG8+eabzbkcAAAAtClZTy+PWFXhvvnmm+PAAw+MzTbbLK688sr45je/Ga+//npLjw8AAAByVtaV7iOPPDLuu+++6Ny5cxx++OHxy1/+MkaMGJHE2AAAACCnZR26CwoK4h//+EeMGTMmCgoKqp373//+F9tss02LDQ4AAAByWdah++abb672+aJFi+Lvf/97XH/99fHiiy/aQgwAAABWa9Ka7oiIxx9/PMaPHx99+vSJyy67LPbZZ5949tlnW3JsAAAAkNOyqnTPnTs3brjhhvjzn/8cpaWlcfjhh0dZWVncfffdMWzYsKTGCAAAADmp0ZXugw46KLbeeut47bXX4sorr4w5c+bE1VdfneTYAAAAIKc1utL9n//8J04++eQ46aSTYquttkpyTAAAANAmNLrS/eSTT8aiRYtixx13jF122SV+//vfxxdffJHk2AAAACCnNTp077rrrvGnP/0pPv300zjxxBPj1ltvjb59+0ZlZWVMmzYtFi1alOQ4AQAAIOfkpVKpVFOf/Pbbb8ef//znuOmmm2LhwoWx3377xT333NOS41snSktLo7i4OEpKSqKoqKi1hwMAAMB6rrE5sslbhkVEbL311nHppZfGJ598En//+9+bcykAAABoc5pV6W4rVLoBAADIxjqpdAMAAAD1E7oBAAAgIUI3AAAAJEToBgAAgIQI3QAAAJAQoRsAAAASInQDAABAQoRuAAAASIjQDQAAAAkRugEAACAhQjcAAAAkROgGAACAhAjdAAAAkBChGwAAABIidAMAAEBChG4AAABIiNANAAAACRG6AQAAICFCNwAAACRE6AYAAICECN0AAACQEKEbAAAAEiJ0AwAAQEKEbgAAAEiI0A0AAAAJEboBAAAgIUI3AAAAJEToBgAAgIQI3QAAAJAQoRsAAAASInQDAABAQoRuAAAASIjQDQAAAAkRugEAACAhQjcAAAAkROgGAACAhAjdAAAAkJD1PnQvWrQoTjnllBgwYEB06tQpdtttt3j++ecz5xcvXhyTJk2Kfv36RadOnWLYsGExZcqUVhwxAAAArNKutQfQkB/84Afxv//9L2666abo27dv/O1vf4tRo0bFG2+8EZtttlmcdtpp8cgjj8Tf/va32GKLLeLBBx+MH/3oR9G3b984+OCDW3v4AAAAbMDW60r3smXL4o477ohLL700Ro4cGYMHD47zzz8/Bg8eHNddd11ERDz99NMxfvz42GuvvWKLLbaIE044Ibbffvv473//28qjBwAAYEO3Xofu8vLyqKioiI4dO1Y73qlTp3jyyScjImK33XaLe+65J2bPnh2pVCqmT58e77zzTowePbo1hgwAAAAZ6/X08m7dusWIESPiwgsvjKFDh0avXr3i73//ezzzzDMxePDgiIi4+uqr44QTToh+/fpFu3btIj8/P/70pz/FyJEj671uWVlZlJWVZT4vLS1N/L0AAACw4VmvK90RETfddFOkUqnYbLPNorCwMK666qo46qijIj9/1dCvvvrqePbZZ+Oee+6JF198MS6//PKYOHFiPPTQQ/Vec/LkyVFcXJz56N+//7p6OwAAAGxA8lKpVKq1B9EYS5YsidLS0ujTp08cccQRsXjx4rj99tujuLg47rrrrhg7dmzmsT/4wQ/ik08+ialTp9Z5rboq3f3794+SkpIoKipK/L0AAACQ20pLS6O4uLjBHLleTy+vqkuXLtGlS5f48ssv44EHHohLL700Vq5cGStXrsxUvdMKCgqisrKy3msVFhZGYWFh0kMGAABgA7feh+4HHnggUqlUbL311vHuu+/GGWecEUOGDInjjjsu2rdvH3vuuWecccYZ0alTpxgwYEA89thj8de//jV++9vftvbQAQAA2MCt96G7pKQkzj777Pjkk0+iR48eMW7cuLj44oujffv2ERFx6623xtlnnx3f+c53YsGCBTFgwIC4+OKL44c//GErjxwAAIANXc6s6U5SY+fiAwAAQETjc+R6370cAAAAcpXQDQAAAAkRugEAACAhQjcAAAAkROgGAACAhAjdAAAAkBChGwAAABIidAMAAEBChG4AAABIiNANAAAACRG6AQAAICFCNwAAACRE6AYAAICECN0AAACQEKEbAAAAEiJ0AwAAQEKEbgAAAEiI0A0AAAAJEboBAAAgIUI3AAAAJEToBgAAgIQI3QAAAJAQoRsAAAASInQDAABAQoRuAAAASIjQDQAAAAkRugEAACAhQjcAAAAkROgGAACAhAjdAAAAkBChGwAAABIidAMAAEBChG4AAABIiNANAAAACRG6AQAAICFCNwAAACRE6AYAAICECN0AAACQEKEbAAAAEiJ0AwAAQEKEbgAAAEiI0A0AAAAJEboBAAAgIUI3AAAAJEToBgAAgIQI3QAAAJAQoRsAAAASInQDAABAQoRuAAAASIjQDQAAAAkRugEAACAhQjcAAAAkROgGAACAhAjdAAAAkBChGwAAABIidAMAAEBChG4AAABIiNANAAAACRG6AQAAICFCNwAAACRE6AYAAICECN0AAACQEKEbAAAAEiJ0AwAAQEKEbgAAAEiI0A0AAAAJEboBAAAgIUI3AAAAJEToBgAAgIQI3QAAAJAQoRsAAAASInQDAABAQoRuAAAASIjQDQAAAAkRugEAACAhQjcAAAAkROgGAACAhAjdAAAAkBChGwAAABIidAMAAEBChG4AAABIiNANAAAACRG6AQAAICFCNwAAACRE6AYAAICECN0AAACQEKEbAAAAEiJ0AwAAQEKEbgAAAEiI0A0AAAAJWe9D96JFi+KUU06JAQMGRKdOnWK33XaL559/vtpj3nzzzTj44IOjuLg4unTpEjvvvHPMmjWrlUYMAAAAq6z3ofsHP/hBTJs2LW666aaYMWNGjB49OkaNGhWzZ8+OiIj33nsv9thjjxgyZEg8+uij8dprr8Uvf/nL6NixYyuPHAAAgA1dXiqVSrX2IOqzbNmy6NatW/zrX/+KsWPHZo7vuOOOccABB8RFF10URx55ZLRv3z5uuummJr9OaWlpFBcXR0lJSRQVFbXE0AEAAGjDGpsj1+tKd3l5eVRUVNSqWnfq1CmefPLJqKysjH//+9/xla98JcaMGRM9e/aMXXbZJe6+++61XresrCxKS0urfQAAAEBLW69Dd7du3WLEiBFx4YUXxpw5c6KioiL+9re/xTPPPBOffvppzJs3LxYvXhz/7//9v9h///3jwQcfjG9+85vxrW99Kx577LF6rzt58uQoLi7OfPTv338dvisAAAA2FOv19PKIVWu2v/e978Xjjz8eBQUFscMOO8RXvvKVePHFF+Phhx+OzTbbLI466qi45ZZbMs85+OCDo0uXLvH3v/+9zmuWlZVFWVlZ5vPS0tLo37+/6eUAAAA0SpuYXh4RMWjQoHjsscdi8eLF8fHHH8d///vfWLlyZWy55ZaxySabRLt27WLYsGHVnjN06NC1di8vLCyMoqKiah8AAADQ0tb70J3WpUuX6NOnT3z55ZfxwAMPxCGHHBIdOnSInXfeOd5+++1qj33nnXdiwIABrTRSAAAAWKVdaw+gIQ888ECkUqnYeuut4913340zzjgjhgwZEscdd1xERJxxxhlxxBFHxMiRI2PvvfeOqVOnxr333huPPvpo6w4cAACADd56X+kuKSmJiRMnxpAhQ+LYY4+NPfbYIx544IFo3759RER885vfjClTpsSll14a2267bVx//fVxxx13xB577NHKIwcAAGBDt943UlsX7NMNAABANtpMIzUAAADIVUI3AAAAJEToBgAAgIQI3QAAAJAQoRsAAAASInQDAABAQoRuAAAASIjQDQAAAAkRugEAACAhQjcAAAAkROgGAACAhAjdAAAAkBChGwAAABIidAMAAEBChG4AAABIiNANAAAACRG6AQAAICFCNwAAACRE6AYAAICECN0AAACQEKEbAAAAEiJ0AwAAQEKEbgAAAEiI0A0AAAAJEboBAAAgIUI3AAAAJEToBgAAgIQI3QAAAJAQoRsAAAASInQDAABAQoRuAAAASIjQDQAAAAkRugEAACAhQjcAAAAkROgGAACAhAjdAAAAkBChGwAAABIidAMAAEBChG4AAABIiNANAAAACRG6AQAAICFCNwAAACRE6AYAAICECN0AAACQEKEbAAAAEiJ0AwAAQEKEbgAAAEiI0A0AAAAJEboBAAAgIUI3AAAAJEToBgAAgIQI3QAAAJAQoRsAAAASInQDAABAQoRuAAAASIjQDQAAAAkRugEAACAhQjcAAAAkROgGAACAhAjdAAAAkBChGwAAABIidAMAAEBChG4AAABIiNANAAAACRG6AQAAICFCNwAAACRE6AYAAICEtGvtAawPUqlURESUlpa28kgAAADIBen8mM6T9RG6I2LRokUREdG/f/9WHgkAAAC5ZNGiRVFcXFzv+bxUQ7F8A1BZWRlz5syJbt26RV5eXmsPp06lpaXRv3//+Pjjj6OoqKi1hwPVuD9Zn7k/WZ+5P1mfuT9Zn60P92cqlYpFixZF3759Iz+//pXbKt0RkZ+fH/369WvtYTRKUVGRf/RYb7k/WZ+5P1mfuT9Zn7k/WZ+19v25tgp3mkZqAAAAkBChGwAAABIidOeIwsLCOO+886KwsLC1hwK1uD9Zn7k/WZ+5P1mfuT9Zn+XS/amRGgAAACREpRsAAAASInQDAABAQoRuAAAASIjQDQAAAAkRugEAACAhQjcAAAAkROgGAACAhAjdAAAAkBChGwAAABIidAMAAEBChG4AAABIiNANAAAACWnX2gNYH1RWVsacOXOiW7dukZeX19rDAQAAYD2XSqVi0aJF0bdv38jPr7+eLXRHxJw5c6J///6tPQwAAAByzMcffxz9+vWr97zQHRHdunWLiFVfrKKiolYeDQAAAOu70tLS6N+/fyZP1kfojshMKS8qKhK6AQAAaLSGlihrpAYAAAAJEboBAAAgIUI3AAAAJEToBgAAgIQI3QAAAJAQoRsAAAASInQDAABAQoRuAAAASIjQDQAAAAkRugEAACAhQjcAAAAkROgGAACAhAjdAAAAkBChGwAAABIidAMAAEBChG4AAABIiNANAAAACRG6AQAAICFCNwAAACRE6AYAAICECN0AAACQEKGbiIgoWbYyzr7ztXju/fmtPRQAAIA2Q+gmIiIuf/Dt+Pt/P44j/vhsaw8FAACgzRC6iYiID75Y0tpDAAAAaHOEbiIioiA/r7WHAAAA0OYI3URERH6e0A0AANDShG4iQugGAABIgtBNREQUuBMAAABanKhFRFjTDQAAkAShm4iIyDO9HAAAoMUJ3UREREGV0J1KpVpxJAAAAG2H0E1EVJ9eXlZe2YojAQAAaDuEbiKievfyspVCNwAAQEsQuomI6lPKl5dXtOJIAAAA2g6hm4iIWFm5JnSrdAMAALQMoZuIiFhZZR23SjcAAEDLaNXQPXny5Nh5552jW7du0bNnzzj00EPj7bffrvW4Z555JvbZZ5/o0qVLFBUVxciRI2PZsmWZ8wsWLIjvfOc7UVRUFN27d4/vf//7sXjx4nX5VnLeyooqoXul0A0AANASWjV0P/bYYzFx4sR49tlnY9q0abFy5coYPXp0LFmyJPOYZ555Jvbff/8YPXp0/Pe//43nn38+Jk2aFPn5a4b+ne98J15//fWYNm1a3HffffH444/HCSec0BpvKWetqBa6TS8HAABoCXmp9WhT5s8//zx69uwZjz32WIwcOTIiInbdddfYb7/94sILL6zzOW+++WYMGzYsnn/++dhpp50iImLq1Klx4IEHxieffBJ9+/Zt8HVLS0ujuLg4SkpKoqioqOXeUA458o/PxLPvL4iIiJu+//X4xlabtvKIAAAA1l+NzZHr1ZrukpKSiIjo0aNHRETMmzcvnnvuuejZs2fstttu0atXr9hzzz3jySefzDznmWeeie7du2cCd0TEqFGjIj8/P5577rl1+wZy2MqKKt3LVboBAABaxHoTuisrK+OUU06J3XffPbbZZpuIiHj//fcjIuL888+P448/PqZOnRo77LBD7LvvvjFz5syIiJg7d2707Nmz2rXatWsXPXr0iLlz59b5WmVlZVFaWlrtY0NnTTcAAEDLW29C98SJE+N///tf3HrrrZljlZWrguCJJ54Yxx13XHzta1+LK664Irbeeuv4//6//6/JrzV58uQoLi7OfPTv37/Z4891K6p0Ly8rV+kGAABoCetF6J40aVLcd999MX369OjXr1/meJ8+fSIiYtiwYdUeP3To0Jg1a1ZERPTu3TvmzZtX7Xx5eXksWLAgevfuXefrnX322VFSUpL5+Pjjj1vy7eSkFSrdAAAALa5VQ3cqlYpJkybFXXfdFY888kgMHDiw2vktttgi+vbtW2sbsXfeeScGDBgQEREjRoyIhQsXxosvvpg5/8gjj0RlZWXssssudb5uYWFhFBUVVfvY0JleDgAA0PLateaLT5w4MW655Zb417/+Fd26dcuswS4uLo5OnTpFXl5enHHGGXHeeefF9ttvH8OHD48bb7wx3nrrrbj99tsjYlXVe//994/jjz8+pkyZEitXroxJkybFkUce2ajO5ayysnxNIzXTywEAAFpGq4bu6667LiIi9tprr2rH//KXv8SECRMiIuKUU06J5cuXx6mnnhoLFiyI7bffPqZNmxaDBg3KPP7mm2+OSZMmxb777hv5+fkxbty4uOqqq9bV22gTqla6y1S6AQAAWsR6tU93a7FPd8S25z8Qi5aXR0TECSO3jJ8fOLSVRwQAALD+ysl9umk9VSvdS1eUt+JIAAAA2g6hm4iIWFmxZsLD4uVCNwAAQEsQuomKylRUVFYJ3WXWdAMAALQEoZtqU8sjIpaUqXQDAAC0BKGbWqF7sdANAADQIoRuqq3njlDpBgAAaClCNyrdAAAACRG6iRXl1nQDAAAkQeimdiO1FRVRWZmq59EAAAA0ltBNZk13lw4FmWNLVqh2AwAANJfQTSxfuWpf7q4d20VBfl5ERCyxVzcAAECzCd3ES7O+jIiILTfpmql2Ly5b2ZpDAgAAaBOEbuLhN+dFRMS+Q3tGt47tIyJisUo3AABAswndG7iVFZXx3AfzIyJi7yE9o0vhqkq3DuYAAADNJ3Rv4MrKKzON1Dbr3im6FLaLiIhFy4VuAACA5hK6N3CpVPWtwbquDt0q3QAAAM0ndG/gqkbuvLyInt06RkTEe58vbp0BAQAAtCFC9wauaqE7L/JixKCNIyLiqffmt9KIAAAA2g6hewNXdXp5Xl7E7oNXhe4ZnyyMkmW2DQMAAGgOoXsDV7XSnZ+XF32KO8Vm3TtFZSri7bmLWm9gAAAAbYDQnWOWr6yIb1/3dPy//7zVItertqZ79Z9FndpnXgsAAICmE7pzzF0vz44XPvoypjz2Xotcr+b08oiIwnarbouy8soWeQ0AAIANldCdY+aWLG/R61XvXr4qdadD9wqhGwAAoFmE7hyzYMmKFr1ejW26IyKisH1BRESUlZteDgAA0BxCd45ZsLSlQ/eq1J2eWh5hejkAAEBLEbpzzJctXele/Wd+ldTdIR26NVIDAABoFqE7x1SdXp6qa254ltKXqFLoVukGAABoIUJ3jqkaussrWyB0R13Ty9NruoVuAACA5hC6c0y10F3RkpXuNalb93IAAICWIXTnkBXlldWq28f/9YV4fU5Js66ZuVrVSnf79PRya7oBAACaQ+jOIRU1ppM/+e4XMe66p5t1zcrV16y2prvAmm4AAICWIHTnkMo6GqctX9kywbhq9/LMPt0tdG0AAIANldCdQ5q/gruOa6bXdNe5T7fp5QAAAM0hdOeQuirdzZXpXl7lWDp0z1tUFsvt1Q0AANBkQncOSSUw23tNpbtq9/JV08uffm9+HPi7J1r+RQEAADYQQncOSSUwwTx9xWqV7vZrbov3v1jS4q8JAACwoRC6c0hlAou6K9ds1J3RocBtAQAA0BKkqxySSmJN9+pLVu9e7rYAAABoCdJVDkmi0p2eYF69e3lB9ddN5oUBAADaPKE7hyRZ6a6re3naykr7dQMAADSF0J1DEtmne/WfdXUvT1tZodINAADQFEJ3Dklkn+66Kt011nSvLFfpBgAAaAqhO4ckkLkzQb5qpbtm9/KVFUI3AABAUwjdOSTRSnfVRmo1Kt0rhG4AAIAmEbpzSBKV7lS6e3mVY9Z0AwAAtAyhO4ckErrrqnTX7F6u0g0AANAkQncOSWJ6eVpeVO1eXmN6uUZqAAAATSJ055AkIveaRmprjrUryI/jdt8i87lKNwAAQNMI3Tmkvkp3eTNCcfqS+VVTd0Scd9BXY8tNu0SENd0AAABNJXTnkFQ9obs5oXhtz0xvHabSDQAA0DRCdw6pb0l3c7b0StUxvTyt/erQbcswAACAphG6c0hlPaG7OZXo9CXrDt2rDq7USA0AAKBJhO4ckqpnMnizQnd6y7ConbrbZ6aXW9MNAADQFEJ3DqmsJ1s3Z0uvtU0v79DOmm4AAIDmELpzSH3dy1tiennN7uUR1nQDAAA0l9DdBqwob0b38sz08toya7qFbgAAgCYRunNIIpXutaTuzJpujdTYwCxdUR7T354X73++uLWHAgBAjmvX2gOg8RLZMmz1n3VVujtopMYG6qw7ZsS9r86JiIiXfrlf9OjSoZVHBABArlLpziH1Vrqb1Uht1Z951nRDxidfLs38/YvFZa04EgAAcp3QnUPq26e7WZXudPfyOs61b2dNNxumqvd8fb/sAgCAxmjV0D158uTYeeedo1u3btGzZ8849NBD4+23367zsalUKg444IDIy8uLu+++u9q5WbNmxdixY6Nz587Rs2fPOOOMM6K8vHwdvIN1rb413c1opLb6z7V2L7emmw1MeZWfqfq26gMAgMZo1dD92GOPxcSJE+PZZ5+NadOmxcqVK2P06NGxZMmSWo+98sor65wCXVFREWPHjo0VK1bE008/HTfeeGPccMMNce65566Lt7BO1VvpbpHp5bXPrVnTLXWwYVmh0g0AQAtp1UZqU6dOrfb5DTfcED179owXX3wxRo4cmTn+yiuvxOWXXx4vvPBC9OnTp9pzHnzwwXjjjTfioYceil69esXw4cPjwgsvjLPOOivOP//86NCh7TRAqu///s3bp7v+QNFeIzU2UFUr3TI3AADNsV6t6S4pKYmIiB49emSOLV26NI4++ui45ppronfv3rWe88wzz8S2224bvXr1yhwbM2ZMlJaWxuuvv17n65SVlUVpaWm1j1xQX8WteWu6V/1Z1yyCDu00UmPDZE03AAAtZb0J3ZWVlXHKKafE7rvvHttss03m+Kmnnhq77bZbHHLIIXU+b+7cudUCd0RkPp87d26dz5k8eXIUFxdnPvr3799C7yJZ6f/8b9qtMA7avm8Urg7FZc2ZXr76zzobqdmnmw1U1dkdQjcAAM2x3oTuiRMnxv/+97+49dZbM8fuueeeeOSRR+LKK69s0dc6++yzo6SkJPPx8ccft+j1E7P6//4bdW4fVx/1tfjWDptFRMQXi5q+pVE6UNS1prt9ge7lbJiqV7pbcSAAAOS89SJ0T5o0Ke67776YPn169OvXL3P8kUceiffeey+6d+8e7dq1i3btVi1BHzduXOy1114REdG7d+/47LPPql0v/Xld09EjIgoLC6OoqKjaRy5I/+c/3Wm8T3GniIj4tGRZ0y9a45pVpaeXW9PNhqa8SuhOqXQDANAMrRq6U6lUTJo0Ke6666545JFHYuDAgdXO/+xnP4vXXnstXnnllcxHRMQVV1wRf/nLXyIiYsSIETFjxoyYN29e5nnTpk2LoqKiGDZs2Dp7L+tCzaZnfYo7RkTEpyXLm33Nuivd1nSzYao+vbwVBwIAQM5r1e7lEydOjFtuuSX+9a9/Rbdu3TJrsIuLi6NTp07Ru3fvOqvVm2++eSagjx49OoYNGxbf/e5349JLL425c+fGOeecExMnTozCwsJ1+n6SVrPSvVn3VZXuOQubXunONFKr41x7W4axAUqlUrYMAwCgxbRqpfu6666LkpKS2GuvvaJPnz6Zj9tuu63R1ygoKIj77rsvCgoKYsSIEXHMMcfEscceGxdccEGCI28dNddf9+menl6+vMlTYDNPq6PUbU03G6KKGqVtoRsAgOZo1Up3U4JiXc8ZMGBA3H///S0xpPVbrTXdq6aXL11REaXLyqO4c/umXrLOSneXDqtujy8Wrcj6upCravYwkLkBAGiO9aKRGo2Trrjlr07IHdsXRI8uHSIiYk4Tm6mtrXv51zbvHhERb3+2KL5Y3PQO6ZBLVlZWn9mh0g0AQHMI3TmkrqngPbutWrc+r4nbhqXW0r18466FMbTPqs7uT783v0nXh1xTc196jdQAAGgOoTuH1Kx0R0R0Xz2lvGTZyiZedXWlu56zI7bcOCIiXvroyyZeH3JLuTXdAAC0IKE7h1TW0Wm8e6dV08tLljZt3XWme3k9qXuj1aG+rLyiSdeHXLOiRqXbPt0AADSH0J1T0pXuNQk5XeleuLRple41jdTqTt35q8vqcgcbilqVbs37AQBoBqE7h9TcpzsiMh3LFzZxennl2jbqrutx0MbV3CLPvQ8AQHMI3TkkVcf+Xunp5U2udDeQudP5Xu5gQ1FzerlGagAANIfQnUPW3kitiWu6V/9ZV/fyqscFDzYUNaeXW9MNAEBzCN05JLOndpW6dPdOzVzTvZZ9ule91urHheDBhqH29PJWGggAAG2C0J2D8qt815q7pjutvtCdqYALHmwgaoZuv3ACAKA5hO4csmZ6edVKd0ut6a47dadfSjMpNhQrK2ru091KAwEAoE0QunNIXbm36prupqw9rWxgennmtbO+MuSm8pqVbr9wAgCgGYTuHFLXlmHp0L2yIhVLV1Rkfc2G8oRGamxobBkGAEBLErpzSF3dyzu1L8hUqZeUlWd9zYa6l6/ZMkzwYMNQa3p5ZT0PBACARhC6c0l6/XWVgJyXlxftVqfwiiYE48Z3L4cNg0o3AAAtSejOIXVVuiMiClYfKK9oQuhe/Wd9S7rzV19bpZsNRc2fI7c+AADNIXTnkDX/968ekdut3kOsSRW5OqrnVWUq3YIHG4gVKt0AALQgoTuHNFjpbkK3s0z38nrOp8O43MGGovb08lYaCAAAbYLQnUPq6l4esSZ0VzQhHWSml9e3pts+3Wxgak4vd+8DANAcQncuqafpWbPWdDc4vXx1pTvrK0Nuqjm9XD8DAACaQ+jOIfVVutPdy5tSkUvF2qeX59syjA1M7Up3Kw0EAIA2QejOIZngWyMhp0N4U9Z0r6l0131+zT7dWV8acpItwwAAaElCdw6pt9JdkF7TXVnzKQ1as2WY6eUQEbGyUiM1AABajtCdQxLZp7uedeJpGqmxoVlZXnOfbvc+AABNJ3TnoJr5OL2mu6Ipa7obnF5uyzA2LOW1Kt1ufgAAmk7oziFrKt3VE3L68yZtGZapdNeduvNVutnA2KcbAICWJHTnkNSaBdjVpNd0N6mRWt2XzKivAg5tVc1fXvmFEwAAzSF055D6GqkV5K/6NlYkuU+33MEGouYvr9z7AAA0h9CdQ+prpNasNd2r/2yo0q3ax4aiVqXb/HIAAJpB6M5BNbf3KmiRNd31vJZGamxgala6ZW4AAJpD6M4h6Ypbfo3vWmbLsCaF7lV/1lvpTj/OTt1sINI/Z2Z5AADQEoTuHLLmv/7VI3K6kVpTpsGmw3TNdeJp6eOqfWwo0r+8ar/6t1v26QYAoDmE7hxS35rulqh011fqzltT6oYNQnqZRvsCv3ACAKD5hO4cUm/38sya7sqaT2nQmsxtn26IqBK6263659G9DwBAcwjduaSepmctsqa73v24VzdSy/rKkJvWVLrTobs1RwMAQK4TunNIfZXu5qzpTlfxGtoyzLpWNhTlq2eMtM9Pd+537wMA0HRCdw6pr4N4weqGT02pdKfVV+nWSI0NTXqVhunlAAC0BKE7h9S/pnvVn83Zp7u+7uX6qLGhyVS6TS8HAKAFCN05pP7u5U2vdDe0pju9J7gptmwo0r+8apefnuXh3gcAoOmE7lxST0BOh4MmVbozf6uv0p1e15r1pSEnpX951aFdep/u1hwNAAC5TujOIZX1TAUvKGh66K6spyN6RrqRmgnmbCBqdy937wMA0HRCdw5ZMxW87n26mzW9vJ7zmUZq2W8BDjnJ9HIAAFqS0J1DKuuZXl6QmV6efTJOx4n6Kt0aqbGhqagxvVwjNQAAmkPoziH1NVJbs6a7CRdtqHu5fbrZwFSkqk8vd+8DANAcQncOyov61nQ3o9Jdz/l0GJc72FCUV9SYXm5pBQAAzSB055B6twxriTXdDe7TLXXTdr35aWk88978iKjSSK2dRmoAADRfu9YeAI1XX0BuzpZhDQWK9GtZ10pblUql4oDfPREREc+evW9menmHAmu6AQBoPpXuHFLf9l4F+au+jc3Zp7veRmrWdNPGLViyIvP3LxaX1epe7t4HAKA5hO4cks7UNZuetWvGPt1rtgxraHo5tE2zFizN/L2svDLKV3ckbGefbgAAWoDQnVNWV7prHM1vzpruqHudeOba+Rqp0bZVDd1LV5Sv2TKswNIKAACaT+jOIekuyvn5LbemOzLrxOs+nal0S920UR9XCd1Lysoza7rTlW53PgAAzSF055B0Vbr2mu7sQvfKisq499U58fmisiprute+T7dqH21V1Ur34rKKNd3LTS8HAKAF6F6eQyrrWX+d7Zrua6e/F1c89E4M2Lhz7De01+pr1i0dxm0ZRltVc3p5eY3p5WZ5AADQHCrdOaS+fbrXrOmubNR17nl1dkREfDR/6Zoo3eD08iwGCjlkcVl55u+Llpdn7vVMI7XG/VgBAECdhO5cUs/662zXdC9fuSZFNNS9PB3ohW7aqqr3dumylZm/m14OAEBLELpzyJpKd/WAnO2a7uUrKzJ/b6h7uX26aeuqhe7lVUO37uUAADSf0J1D6mt6ll7T3dgtw5ZVDd0Ndi9Pr+mGtqnqvV26bM1U8w7tVncv9wsnAACaQejOIWsaqVWXrnw3pdKdVt/08jXdywUP2qaqobqkyvTydvmmlwMA0HxCdw6pr5FaOhw0ttJd9WHpa9Zb6c5ML2/8OCGXVL23S5aZXg4AQMtq1dA9efLk2HnnnaNbt27Rs2fPOPTQQ+Ptt9/OnF+wYEH8+Mc/jq233jo6deoUm2++eZx88slRUlJS7TqzZs2KsWPHRufOnaNnz55xxhlnRHl5ec2Xy32ZqeB1r+mubEI6SNVTPU9LV8AFD9qqqtvhpdd05+WtWbah0g0AQHO0auh+7LHHYuLEifHss8/GtGnTYuXKlTF69OhYsmRJRETMmTMn5syZE5dddln873//ixtuuCGmTp0a3//+9zPXqKioiLFjx8aKFSvi6aefjhtvvDFuuOGGOPfcc1vrbSVixicl8dCbn0VEXZXu7NZ0V5WqryX6avn5ax4JbVHVH5t09/KCvDyd+wEAaBHtWvPFp06dWu3zG264IXr27BkvvvhijBw5MrbZZpu44447MucHDRoUF198cRxzzDFRXl4e7dq1iwcffDDeeOONeOihh6JXr14xfPjwuPDCC+Oss86K888/Pzp06LCu31YiLn3grSgrX7XVV32V7sau6a4qHSjq7V4eggdtW11rugvy8zI/ZyrdAAA0x3q1pjs9bbxHjx5rfUxRUVG0a7fq9wXPPPNMbLvtttGrV6/MY8aMGROlpaXx+uuvJzvgdaiw3ZpvVc2idEETK935eVU6ote7T/eqPwUP2qqqd3b6R6hdfp57HwCAFtGqle6qKisr45RTTondd989ttlmmzof88UXX8SFF14YJ5xwQubY3LlzqwXuiMh8Pnfu3DqvU1ZWFmVlZZnPS0tLmzv8xHWoErpr7tPdrolruju2L2h4y7B0I7Wsrgw5pI6bOz9/zfRy/QwAAGiO9abSPXHixPjf//4Xt956a53nS0tLY+zYsTFs2LA4//zzm/VakydPjuLi4sxH//79m3W9daFDQZVKd41zayrdlVldc1XoTtV5zTWa3qQNckFdleyqlW77dAMA0BzrReieNGlS3HfffTF9+vTo169frfOLFi2K/fffP7p16xZ33XVXtG/fPnOud+/e8dlnn1V7fPrz3r171/l6Z599dpSUlGQ+Pv744xZ8N8kobFeQ+XvNSnc2a7qrPqZTIyrd+SrdtHHpe7vqbJKC/Pwqa7pbYVAAALQZrRq6U6lUTJo0Ke6666545JFHYuDAgbUeU1paGqNHj44OHTrEPffcEx07dqx2fsSIETFjxoyYN29e5ti0adOiqKgohg0bVufrFhYWRlFRUbWP9V2HFlrTvWxlRebvhe3zM93LazZnW/NaUjdtW/oXT9ttVpw5VpAfVaaXu/kBAGi6Vg3dEydOjL/97W9xyy23RLdu3WLu3Lkxd+7cWLZsWUSsCdxLliyJP//5z1FaWpp5TEXFqvA4evToGDZsWHz3u9+NV199NR544IE455xzYuLEiVFYWNiab69FVQ/dNdd0rzrXmCngS8vW7F/eoSC/0ZVuwYO2Kn1vb9tvTehul59f5d5vjVEBANBWtGojteuuuy4iIvbaa69qx//yl7/EhAkT4qWXXornnnsuIiIGDx5c7TEffPBBbLHFFlFQUBD33XdfnHTSSTFixIjo0qVLjB8/Pi644IJ18h7WleqN1Kqfy6bSvWTFmkp3KtVw9/LMlmFZjBVySfr3SX2LO2WOFeRX3afb3Q8AQNO1auhu6D+ze+21V6P+wztgwIC4//77W2pY66WqjdSas6Z7SZVKd0UqlanyNdi9XO6gjevWcc0/h6v26V71d7M8AABojvWikRoNa6k13UurVLorq5S66+teLnjQ1qV/sdet45oGjVUr3VluCgAAANUI3TmicC1rutNV8JUVDaeDqo+pNr283kq36eW0benfVVWtdOeFRmoAALQMoTtHVKt013NuRXnDoXtFldBdUZmqsk93fWu6V7GulbYq3cG/auheXl5RZZ/u1hgVAABthdCdI9a2pjsdussrUw12MC+vWHO+MpVqsNK9pplUlgOGHJG+twvbFWSOLVtRWWWfbjc/AABNJ3TniLV1L29fsObAigammNeaXp7ZMqy+fbpXPzaLsUIuqcz8DKw5VraywnZ5AAC0CKE7R6ytkVrVc9mE7sqq3cvrebxGarR9tTv4L1tZEfn5ZnkAANB8QneOqDq9vGZVun3+mnMrG1jXvbLK9PKKyoanl2f26RY8aKMysz2q/OqpvDKl0g0AQIsQunPE2hqp5efnZaaYN1TpLq9W6Y5GbxkWoZkabVP6rq65bGPNmu51Ox4AANoWoTtHVF/TXTsipyvhDXUwr76mO5Xp3Fzfmu6qryVz0xZllljU+BGwZRgAAC1B6M4RVffpzq/ju9a+XeP26l5RdXp5KpUJ0jWrfGlVD4setEVrMnX1HwJbhgEA0BKE7hzRoWDNdkZ17amdrnSXNVDprja9vHJN6K5vUXfVwyp+tEXpZRM1f/Gk0g0AQEsQunPE2rqXVz2f3fTyaET3ctPLaduqbpvXsX3tnzOhGwCA5hC6c0T10F1/pbtqd/K6rKw5vTxzzbofX62RmgnmtEGZn4GI6NR+zYySfI3UAABoAUJ3jqjeSK3+89lUuiurrOmua8r6qtdS6aZtS1VppLbboE0iYlX4Tt/7uvYDANAc7Vp7ADROtX2661rTnQ7dFRVrvU55lbLdqr/W3bl5zWutIXvQFq3ZMiwvLv7mNrH5xp1j3A6bZe53lW4AAJpD6M4RVSvd5ZW1q9ntM1uGrT0hVK2EpxrTvVwjNdq4qvd1984d4qz9h0RExLvzFtc6DwAA2TK9PEdU3TKsrnXbmX26G9gyrGpgr6issqa7MdPLGztYyCFrGqlVP57+RVSlUjcAAM0gdOeIqtPLy+sI1pl9uhta011efXp55ZpF3Q2ytpW2qOr08qrWrOlexwMCAKBNEbpzRH6V+d91VbMbW+leWWNqemUDmbv69PKGxwm5pmojtars0w0AQEsQunNQeR3Tywsb3b28+nPTU2fr2oYsokb1T/agDaqvg/+afbrX8YAAAGhThO4cVHcjtVUJYWVDle4aoTx9rXor3VX+ruJHW1TfXvXpz5etrIh5i5av0zEBANB2CN05aEVdjdRWV7rLGqh01wzs6U/z67kTNFKjratsYHp5RMS3r3tmXQ4JAIA2ROjOQV/tW1TrWIdGTi+vGdjXVLrrrnVXm12u0k0bVN/08qqhe9aCpetySAAAtCH26c4h00/fKz6avyR22HyjWufS+3Q3NL28Zufzisq6q3xpVdd6W9tKW1bflmEAANAcQncOGbhJlxi4SZc6zzW20l0zlFc0onqdl7eqGpgywZw2pursjZpbhtXXXBAAALJhenkbUdjYLcNqTC9PP3xtASNzRuamjak6e6PmT0D6F1kRERt36bBuBgQAQJsjdLcRjZ1eXqvS3UD38oiq+xU3fXywPqpa6a75e6fiTu3jqK/3j4iIzoUF63JYAAC0IUJ3G9Ho7uW1Kt2rPq85tbaq9CnTy2lrqt7RdTUTPHynVaFbD0EAAJpK6G4j1lS6154Oale6195ILWJNGFHppq2puvd8Xh3/GqaXXQjdAAA0ldDdRqxppFax1sfVXPOdbqS2tunlmUq35EEbk1rLmu76jgEAQDaE7jaisd3La00vr2hEpTsTups+Pljf1dVM0C+cAABoLqG7jejQ1OnlmTCxtu7lptjSNlW9p+val1sTQQAAmkvobiOavE93Zsuw+p+Tr5EabVS1Nd1r+cWTex8AgKYSutuIdKW7LOt9ulc9fu3dy1X7aJuqdS+v40fA0goAAJpL6G4jCtuvDt0r195IrbyyssbnGqmx4Wrons4srVgXgwEAoE0SutuIwnYFEVG7O3lVqVSqVqW7slFbhq1+frNGCOufymprutfWSG0dDQgAgDYn69D90ksvxYwZMzKf/+tf/4pDDz00fv7zn8eKFStadHA0XmG7dKW7/tBdXiVhtC/Iq3Zs7d3L043UJA/amKpbhq2lkZpfOQEA0FRZh+4TTzwx3nnnnYiIeP/99+PII4+Mzp07xz//+c8488wzW3yANE66kVrZWhqpVW2ill4DXpnZp7v+1J2v2kcbVbVBWp37dK8+qJ8BAABNlXXofuedd2L48OEREfHPf/4zRo4cGbfcckvccMMNcccdd7T0+GikTKW7vP413VWnlhe2XzUdvSKdJhpT6W7mGGF9k2poennmce5+AACaJuvQnUqlonJ1M66HHnooDjzwwIiI6N+/f3zxxRctOzoaLR2i17ZlWNVz6enl6cy91u7lq/+sFDxoY6ptGba27uXraDwAALQ9WYfunXbaKS666KK46aab4rHHHouxY8dGRMQHH3wQvXr1avEB0jiZLcPKK+utyi1f3dm8U/uCaJdf/Vu/9u7l6TXdzR8nrE+qbxlW5wTzVY9z7wMA0ERZh+4rr7wyXnrppZg0aVL84he/iMGDB0dExO233x677bZbiw+QxklvGRZRfwfzdOju2D6/VlVv7Y3UVv2p0k1b09Atbbs8AACaq122T9huu+2qdS9P+81vfhMFBQUtMiiyl17THbGq2p3eQqyqZVUq3TWnk2ukxoYoHabz67n98/UzAACgmbIO3WkvvPBCvPnmmxERMXTo0Nhpp51abFBkLz29PKL+dd3LVqyudHcoqBWg175P99omn0PuSv8Y1D21vGojtXUyHAAA2qCsQ/cnn3wSRx11VDz11FPRvXv3iIhYuHBh7LbbbnHrrbdGv379WnqMNEJeXl50aJcfK8or6902bPnq4x3bFcTyGl3O176me9WfppfT1qQaaN5vejkAAM2V9ZruH/zgB7Fy5cp48803Y8GCBbFgwYJ48803o7KyMn7wgx8kMUYaKbNt2Mq6tw1LV7o7dahjevlaSt35GqnRRqX36a6ve396lodbHwCApsq60v3YY4/F008/HVtvvXXm2NZbbx1XX311fOMb32jRwZGdwnYFsSjKG2yk1ql9QSxeXl7t3Nqml6epdNPWVGbml9d9Pk8/AwAAminrSnf//v1j5cqVtY5XVFRE3759W2RQNM2aSndD3csLancvX8t17VVMW5WeNt7g9HJ3PwAATZR16P7Nb34TP/7xj+OFF17IHHvhhRfiJz/5SVx22WUtOjiykwnd9TVSq7JlmOnlsOaernd6+erjle59AACaKOvp5RMmTIilS5fGLrvsEu3arXp6eXl5tGvXLr73ve/F9773vcxjFyxY0HIjpUEdMqG7njXdVbcMq/Hrlsbs062ZFG1NppFafdPLMw9cF6MBAKAtyjp0X3nllQkMg5ZQ2H7V3tz1bRm2vEojtYJa+3TXz17FtFXpaeOmlwMAkJSsQ/f48eOTGActoLBg7dPLM1uGtS+oNZ18bdPL7VVMW7Wm0t1A93L3PgAATZT1mu6IiPfeey/OOeecOOqoo2LevHkREfGf//wnXn/99RYdHNkpbN/A9PIVaxqp5ddspLbWTmqr/tC9nLYm07y8oe7l62Q0AAC0RVmH7sceeyy23XbbeO655+LOO++MxYsXR0TEq6++Guedd16LD5DGa6h7edU13QX5TZheLnnQxlQ2tnu5mx8AgCbKOnT/7Gc/i4suuiimTZsWHTp0yBzfZ5994tlnn23RwZGdwnar13Q3sE93x/b5TZxeLnjQtjR2ernu5QAANFXWoXvGjBnxzW9+s9bxnj17xhdffNEig6JpOjRyn+5OdU0vX8t1NVKj7Vp1V9f8eUhb67ILAABohKxDd/fu3ePTTz+tdfzll1+OzTbbrEUGRdMUNnbLsA61p5en14PXZc0U2xYYJKxHKhusdK9hpgcAAE2Rdeg+8sgj46yzzoq5c+dGXl5eVFZWxlNPPRWnn356HHvssUmMkUZKh+56twxbXQEvbFeQqV6nbdq1sMHra6RGW5OZXl7P+aph3O0PAEBTZB26L7nkkhgyZEj0798/Fi9eHMOGDYuRI0fGbrvtFuecc04SY6SR0vt017dl2LIq+3RXDRMF+XmxUecOdT4nwvRy2q7MPt31pO6qE0Lc/wAANEXW+3R36NAh/vSnP8W5554bM2bMiMWLF8fXvva12GqrrZIYH1no0NA+3fWs6d6ka4fIr29Ra6wJJCrdtDWNbaQWser+L1hr9wMAAKgt60r3BRdcEEuXLo3+/fvHgQceGIcffnhstdVWsWzZsrjggguyutbkyZNj5513jm7dukXPnj3j0EMPjbfffrvaY5YvXx4TJ06MjTfeOLp27Rrjxo2Lzz77rNpjZs2aFWPHjo3OnTtHz54944wzzojy8vJs31rOa2hNd9Xu5QVVQkbPbh3Xet3MQ2Vu2piGtgyresLvnAAAaIqsQ/evfvWrzN7cVS1dujR+9atfZXWtxx57LCZOnBjPPvtsTJs2LVauXBmjR4+OJUuWZB5z6qmnxr333hv//Oc/47HHHos5c+bEt771rcz5ioqKGDt2bKxYsSKefvrpuPHGG+OGG26Ic889N9u3lvPS1eqKevY3WpYJ3dWnl2/abe3ruddML5c6aFvWVLrrPp9XbXq5+x8AgOxlPb08lUrVORXz1VdfjR49emR1ralTp1b7/IYbboiePXvGiy++GCNHjoySkpL485//HLfcckvss88+ERHxl7/8JYYOHRrPPvts7LrrrvHggw/GG2+8EQ899FD06tUrhg8fHhdeeGGcddZZcf7551fbS7ytS4fj+vYUTk8779iu+vTyng2E7vRD/zNjbuy9dc+17ukNuahmY8G06t3L181YAABoWxpd6d5oo42iR48ekZeXF1/5yleiR48emY/i4uLYb7/94vDDD2/WYEpKSiIiMuH9xRdfjJUrV8aoUaMyjxkyZEhsvvnm8cwzz0RExDPPPBPbbrtt9OrVK/OYMWPGRGlpabz++ut1vk5ZWVmUlpZW+2gL0kG6rnCQSqWqTy/Pb3ylOx2y//niJzHtjc/W+ljIJQ1NL/cLJgAAmqvRle4rr7wyUqlUfO9734tf/epXUVxcnDnXoUOH2GKLLWLEiBFNHkhlZWWccsopsfvuu8c222wTERFz586NDh06RPfu3as9tlevXjF37tzMY6oG7vT59Lm6TJ48Oeup8LlgzX7atVN3eWUqUwGvuWVYg5XuKrnjmffnx+iv9m72WGF90FAjtXxrugEAaKZGh+7x48dHRMTAgQNj9913j3btsp6ZvlYTJ06M//3vf/Hkk0+26HXrcvbZZ8dpp52W+by0tDT69++f+OsmbW1be6Wr3BERhe3zqwXpjRvYo7tqHCmvkDxoOxq6m2t2LwcAgGw1OjmXl5dHRUVF7Lnnnpljn332WUyZMiWWLFkSBx98cOyxxx5NGsSkSZPivvvui8cffzz69euXOd67d+9YsWJFLFy4sFq1+7PPPovevXtnHvPf//632vXS3c3Tj6mpsLAwCgvXHjRzUV5mTXftcFB1G7HCdtWnl69tj+6I6utdy+tbMA45KD0rJL+ehTbVG6kBAED2Gr2m+/jjj4+TTz458/miRYti5513jmuuuSYeeOCB2HvvveP+++/P6sVTqVRMmjQp7rrrrnjkkUdi4MCB1c7vuOOO0b59+3j44Yczx95+++2YNWtWZir7iBEjYsaMGTFv3rzMY6ZNmxZFRUUxbNiwrMaT69L5oK5cnA7dHdrlR15eXrUg3aPL2kN31eBRUVn3HuCQi9I/K3mN2H+7rmUbAADQkEaH7qeeeirGjRuX+fyvf/1rVFRUxMyZM+PVV1+N0047LX7zm99k9eITJ06Mv/3tb3HLLbdEt27dYu7cuTF37txYtmxZREQUFxfH97///TjttNNi+vTp8eKLL8Zxxx0XI0aMiF133TUiIkaPHh3Dhg2L7373u/Hqq6/GAw88EOecc05MnDixTVaz1yZ/LWu6M03U2tX+lm/Uuf1ar1s1kFTI3LQpqxupNWrLMAAAyF6jQ/fs2bNjq622ynz+8MMPx7hx4zIN1caPH19vt/D6XHfddVFSUhJ77bVX9OnTJ/Nx2223ZR5zxRVXxP/93//FuHHjYuTIkdG7d++48847M+cLCgrivvvui4KCghgxYkQcc8wxceyxx8YFF1yQ1VjagvQ+3XUV5MpWrkrLhe0LIiJi0fLyzLnuDUwvrzpdXaWbtiR9a9e3ZVjV4wrdAAA0RaPXdHfs2DFTgY6IePbZZ6tVtjt27BiLFy/O6sUbM12zY8eOcc0118Q111xT72MGDBiQ9dT2tmjN9PK61nSvqnQXrq50lyxbkTnXoY7qd1Urq5S3remmLVkzvbxu1ffpdu8DAJC9Rle6hw8fHjfddFNERDzxxBPx2WefxT777JM5/95770Xfvn1bfoQ0WrqRWl3ZYPnqSnfH1ZXukmUrG33dqk3YdC+nLckE6Xqnl6t0AwDQPI2udJ977rlxwAEHxD/+8Y/49NNPY8KECdGnT5/M+bvuuit23333RAZJ46TzQWMq3QuXNj50r6gSupdW2XoMcl36J6VRle6ExwIAQNvU6NC95557xosvvhgPPvhg9O7dOw477LBq54cPHx5f//rXW3yANF5+Zsuw2ufS1eo108ubVuleUla+lkdCbmloTXe1RmpK3QAANEGjQ3dExNChQ2Po0KF1njvhhBNaZEA03Zqtt6s2PktFXlTpXr56ennVIN2QdJU8QuimbUkH6fq7l1eZXr4uBgQAQJuTVehm/Zbe2itd6f6sdHnse/ljceC2vWOnLXpExJpKdzaqVbpXCN20HWuml9e/T3de3qqKuEI3AABNkX0CY72VV2Of7idmfhGLy8rjHy98Ep8uXB4RayrdN37v69G/R6f4+/G7Nnjd6tPLremm7cj0Uas/c2fiuOnlAAA0hUp3G1JzTXfXwoLMuX+9Mjsi1lS69/zKpvHEmftEY1RtpLbY9HLakMrM9PK1VbpXlbpFbgAAmkKluw2p2b28aoX6/S+WREREYbuCWs/Lxoryymr7dkMua6h7edVzCt0AADRFk0L3woUL4/rrr4+zzz47FixYEBERL730UsyePbtFB0d2anZgXlFHs7SO7Zv/e5alppjTRjTUSK3qObVuAACaIuvp5a+99lqMGjUqiouL48MPP4zjjz8+evToEXfeeWfMmjUr/vrXvyYxThqhZqV7ZUXtkFDYvnmV7oiIhctWRHHn9s2+DrS2hrYMi0hPPU+pdAMA0CRZlz1PO+20mDBhQsycOTM6duyYOX7ggQfG448/3qKDIzvpdamVqwvcK8prV6Sb0r28po8XLGv2NWB9kK5eN6aRWqXUDQBAE2SdwJ5//vk48cQTax3fbLPNYu7cuS0yKJomv8Y02Loq3R1boNL94fwlzb4GrA8y3cvX8pg1uwIkPhwAANqgrEN3YWFhlJaW1jr+zjvvxKabbtoig6Jpau7TvaKOhmctUen+SOimjVizZdhappevNZIDAMDaZZ3ADj744Ljgggti5cqVEbHqP6uzZs2Ks846K8aNG9fiA6Tx8mvs011WRyO1lljT/dH8pc2+BqwPKrNppKbSDQBAE2Qdui+//PJYvHhx9OzZM5YtWxZ77rlnDB48OLp16xYXX3xxEmOkkdLVunQ4qGtrr5apdAvdtA1ZbRmmezkAAE2Qdffy4uLimDZtWjz55JPx2muvxeLFi2OHHXaIUaNGJTE+slCze3l6y7DdBm0cT783PyLqDuLZ+vhLoZu2oTHTy/Nr/DILAACykXXoTttjjz1ijz32aMmx0EyZcLD683To3mnARhER8fR782OHzTdq8vV7diuMeYvKYumKili+sqJFmrJBa0ovxchvRKlb93IAAJoi69B91VVX1Xk8Ly8vOnbsGIMHD46RI0dGQYFAtq7lZ8LBqj/TVe0O7fLjxu99Pb5YXBZ9ijtlfd1rv7ND/HbaO3H1UV+LsVc9EZWpiNJlK4Vuct6a6eVra6RW/bEAAJCNrEP3FVdcEZ9//nksXbo0NtpoVdX0yy+/jM6dO0fXrl1j3rx5seWWW8b06dOjf//+LT5g6pdXo5FautLdviA/2hfkNylwR0QcuG2fOHDbPhERUdypfXy5dGUsXLYyehZ1bOCZsH5LNWJRd81eCQAAkI2su2pdcsklsfPOO8fMmTNj/vz5MX/+/HjnnXdil112id/97ncxa9as6N27d5x66qlJjJe1qBkOVlSpdLeU7p07REREybKVLXZNaC3p5mhrm16+Zrm31A0AQPayrnSfc845cccdd8SgQYMyxwYPHhyXXXZZjBv3/7N33vFxVOf6f2arumRZtuXeMC7YGGOa6S10COWGFEIKpAMpBO6FkHJTSUgghcAvJHAhCS2FEiAJhA4GTDHGGHDvXVbv2ja/P2bOmTNnp+7OSrvr9/v58MGSVruj3dnZ857neZ/3ImzatAk33XQTjQ8bAVhPtxykFmTRXV8ZBQB09lPRTZQ+rBXDyV5OQWoEQRAEQRBEPviuxnbv3o1UKpX1/VQqhT179gAAJkyYgJ6envyPjvAFKxsyktIdDQepdLOiOxHYfRLESKF6mdOt/z9DRTdBEARBEASRA76rsZNOOglf/OIXsWLFCv69FStW4Mtf/jJOPvlkAMCqVaswffr04I6S8IShyGnVAQtSC2I2N6NBV7rJXk6UE45FN8tKIHs5QRAEQRAEkQO+q7G77roLjY2NWLx4MeLxOOLxOA477DA0NjbirrvuAgDU1NTg5ptvDvxgCWeMIDXt/9xeHqjSrfV0k72cKAcyfGSY+8wwspcTBEEQBEEQueC7p7u5uRlPP/001qxZg3Xr1gEAZs+ejdmzZ/PbnHTSScEdIeEZRZonnEhr/w/SXs57ugfIXk6UPl4KaXkziyAIgiAIgiD84LvoZsyZMwdz5swJ8liIPOH2cv1rClIjCGdYIa04KN3GnG6qugmCIAiCIAj/5FR079ixA4899hi2bduGRMKseN5yyy2BHBjhHyPwiaWXpwEEPTKMerqJ8oGV0U4jwyi9nCAIgiAIgsgH30X3s88+i/POOw8zZszAmjVrMH/+fGzZsgWqquLQQw8txDESHgmFeOITACBZQHs5Fd1EOcA2qBw7usleThAEQRAEQeSB72rs+uuvxzXXXINVq1ahoqICDz30ELZv344TTjgBH/nIRwpxjIRHQnJPdyr49PJ4JGy6b4IoacheThAEQRAEQRQY39XY6tWr8alPfQoAEIlEMDAwgJqaGvzgBz/Az372s8APkPCDVh7Ic7qDtJdHw4rpvgmilGGFtJO9XCF7OUEQBEEQBJEHvqux6upq3sc9fvx4bNy4kf+stbU1uCMjfBOS5gkndTU6SHt5VC/gk1R0E2VAhhfSTgZzDaq5CYIgCIIgiFzw3dN91FFHYenSpZg7dy7OOussfPOb38SqVavw8MMP46ijjirEMRIeYYFPGb0eHiqA0s1mfidTVIIQpY+RXm5/m1CI3ZbOeYIgCIIgCMI/vovuW265Bb29vQCA73//++jt7cVf/vIXzJo1i5LLRxgj8EmFqqpcjY4FqHSzAp7s5UQ5wFwhjkFqUtsGQRAEQRAEQfjBV9GdTqexY8cOHHzwwQA0q/nvfve7ghwY4R9xTncqo3IVL8iiO8qVbiq6idKHvUdCTkFq/EdUdRMEQRAEQRD+8VWNhcNhnHbaaejo6CjU8RABkFFVU7o4BakRhDXMMu5kL+fp5VRzEwRBEARBEDnguxqbP38+Nm3aVIhjIfKE93Sr5qCzQHu6BXs59bgSpQ47gx2LbsFBQhAEQRAEQRB+8V2N/ehHP8I111yDJ554Art370Z3d7fpP2LkMAKfjDnaIQUIO81D8gmzqqsqkKYmV6LEUf3M6abTnSAIgiAIgsgB30FqZ511FgDgvPPOMy1UVVWFoihIp9PBHR3hCxb4pKoq+hLa61AZDQf6GOL4sWRaRSTYuyeIYSWjeghSEwIKCYIgCIIgCMIvvovu559/vhDHQQSAMacb2Ns9CAAYV1cR6GOIVvVEOoNKUNVNlC6elG6F0ssJgiAIgiCI3PFddJ9wwgmFOA4iAIziQOVF99i6eKCPERGs6glKMCdKHN7T7XAbbi+nrm6CIAiCIAgiB3JK2Hr55ZfxyU9+EkcffTR27twJAPjzn/+MpUuXBnpwhD+YWJfJqAVTuhVF4X3dSUowJ0ocZhl3ij1QjKqbIAiCIAiCIHzju+h+6KGHcPrpp6OyshJvv/02hoaGAABdXV34yU9+EvgBEt4R53Tv7dZel+aAi27AGBtGRTeQohT3ksZbkBqllxMEQRAEQRC5k1N6+e9+9zv84Q9/QDQa5d8/5phj8Pbbbwd6cIQ/eE+3Cuzh9vLgi24+Nmw/t5cPpdI46eYX8Ik/vD7Sh0LkCLOMewtSK/zxEARBEARBEOWH757utWvX4vjjj8/6fn19PTo7O4M4JiJHmCKXUVXs7dKK7sIo3cas7v2Zldu7sL19ANvbB3h6P1Fa+AtSo6qbIAiCIAiC8I9vpbu5uRkbNmzI+v7SpUsxY8aMQA6KyA1Rkdvbw3q6gw1SA4yiO5nev4uQqpiR3D6Y3L83IEqVDC+67W9DLd0EQRAEQRBEPvguuj//+c/ja1/7Gl5//XUoioJdu3bhvvvuwzXXXIMvf/nLhThGwiM8SE1V0dabAAA01QRfdMcjFKQGABXCDPTeodQIHgmRK/7s5VR2EwRBEARBEP7xbS+/7rrrkMlkcMopp6C/vx/HH3884vE4rrnmGlx11VWFOEbCIzxITQVSuoQXCQdveeb28v28p1vUPnuHUhhTG/wGB1FYVC9KNyu6C384BEEQBEEQRBniu+hWFAU33HADrr32WmzYsAG9vb2YN28eampqCnF8hA+M9HKVVwihAvQZRyPafe7vPd2i8Nk7SEp3KeP0PmFZCVR1EwRBEARBELng215+7733or+/H7FYDPPmzcMRRxxBBXeRYNjLjYKwEEU3n9O9nyvdGbHoJnt5SZLRX0SntwmfCkBVN0EQBEEQBJEDvovub3zjGxg7diw+8YlP4F//+hfS6XQhjovIAbH3lCUthwoQqE3p5RpimjUV3aWJ8Qq6+8sz+/fpThAEQRAEQeSI76J79+7dePDBB6EoCi6++GKMHz8eV1xxBV599dVCHB/hgxAfbSSmMhdA6daD1Fbt6Nqvw6XMRXdyBI+EyBXDEWJ/G0ovJwiCIAiCIPLBd9EdiURwzjnn4L777kNLSwt++ctfYsuWLTjppJMwc+bMQhwj4RGruqGQSvcdL23C3a9sCf4BSgTq6S592MaJpyC1/XiDiSAIgiAIgsgd30W3SFVVFU4//XSceeaZmDVrFrZs2RLQYRG5YNW/XciebgC4/YWNlre55m8rcf3D75Z1oWJWuqnNohRhr6DiYC8npZsgCIIgCILIh5yK7v7+ftx3330466yzMHHiRPzqV7/CBRdcgPfffz/o4yN8MFxFdzRinDY18XDWz7v6k/j78h144I3t2NDSG/jjFwvmIDWyl5cknpRuRbwpQRAEQRAEQfjC98iwj33sY3jiiSdQVVWFiy++GN/5znewZMmSQhwb4ReLwkHJy8tgTVSY/V1TkX0KDaYM1fe1TW2YNa42+IMoAkxKN9nLSxL2CjptThktGlR1EwRBEARBEP7xXXSHw2H89a9/xemnn45w2Kxyvvfee5g/f35gB0f4w6p/uxBKd1xQuqtj2afQUNKIeV66vhWfWjIt8GMoBlSyl5c8GQ/yNbOeZ6jmJgiCIAiCIHLAd9F93333mb7u6enBAw88gDvvvBPLly+nEWIjiFVSeQFy1EzUWijdQ4LS/d7OrgIfwchB9vLSR+Up/w434kFqBT8cgiAIgiAIogzJ2Xz80ksv4dOf/jTGjx+PX/ziFzj55JOxbNmyII+N8MlwKd3dgpU6Hsnu6R5KGUp3R3/5FqOZDM3pLnW82MuNIDWqukuZh9/egSff2xPofaqqijtf3oQ3NrcHer8EQRAEQZQXvpTuPXv24J577sFdd92F7u5uXHzxxRgaGsKjjz6KefPmFeoYCY9YFQ4FqLnRIxTdiXQm6+ei0j2QTGMwmUZFNLs4L3UyNDKs5OEjwxxuo5DSXfL88dUt+N5jWtDnhh+fiUg4mLCL+17fhh/9czUAYMtPzw7kPgmCIAiCKD88rzzOPfdczJ49G++++y5+9atfYdeuXbj11lvzevCXXnoJ5557LiZMmABFUfDoo4+aft7b24srr7wSkyZNQmVlJebNm4ff/e53ptsMDg7iiiuuwOjRo1FTU4OLLroIe/fuzeu4yolCKN09g4Z6PZjMbicYTJoL8Y7+RODHUAyYe7qp6C5JPNjL2XuIau7S5dbnNvB/W20U5srfl+8I7L4IgiAIgihfPBfd//73v3H55Zfj+9//Ps4+++ysELVc6Ovrw8KFC3HbbbdZ/vzqq6/Gk08+iXvvvRerV6/G17/+dVx55ZV47LHH+G2+8Y1v4PHHH8ff/vY3vPjii9i1axcuvPDCvI+tFLEeGRb843QPGEW3aCU3vmcuxDv6ytNibu7ppqK7FOFzup3s5VzpprK7VEkI16SExTUrF1LpDN7Z3hnIfREEQRAEUd54LrqXLl2Knp4eLF68GEceeSR++9vforW1Na8HP/PMM/GjH/0IF1xwgeXPX331VXz605/GiSeeiGnTpuELX/gCFi5ciDfeeAMA0NXVhbvuugu33HILTj75ZCxevBh33303Xn311f2yv3y4errnjK/j/7YsuiWl+9nVe3HdQ++is8wUbxoZVvqoXuZ0g+Z0lxNy0a2qKpZtakOXz/yJy//4Fv93ZRm2zxAEQRAEERyei+6jjjoKf/jDH7B792588YtfxIMPPogJEyYgk8ng6aefRk9PT+AHd/TRR+Oxxx7Dzp07oaoqnn/+eaxbtw6nnXYaAGD58uVIJpM49dRT+e/MmTMHU6ZMwWuvvRb48RQ7lunlBVC6v3/eQZg1tgYAMGRhL5cL8ZufXocH39yOQ37wNC76f69iIFEeCfdi0d2XSJuC1YjSgL1kikNXN1e6S8Rg3jeUKuupAbkgvjXl69PDb+/Ex36/DBfc/orn+1NVFa9tbONfN1RF8z5GgiAIgiDKF99pMtXV1bjsssuwdOlSrFq1Ct/85jfx05/+FGPHjsV5550X6MHdeuutmDdvHiZNmoRYLIYzzjgDt912G44//ngAWrBbLBZDQ0OD6ffGjRuHPXvsU2qHhobQ3d1t+q8ckJVuRXG2zeZKU00cP7lwAQBv9nKR5Vs78Ktn1gV+TCOBrHz2JUjtLjU8jQyTblvsXHn/2zjn1qX4z/vBJnWXMmmh6pZ7uh99ZycAYFNrn+f76xpImu4nTRtuBEEQBEE4kFeE6+zZs3HTTTdhx44deOCBB4I6Js6tt96KZcuW4bHHHsPy5ctx880344orrsAzzzyT1/3eeOONqK+v5/9Nnjw5oCMeWeQCuxDWckY8op06XpRumcdW7irIMQ03GakKo77u0oOp107ZB+x9VaxF92sb2/Doip386+fX7gMA/Pb5DXa/kheDyTT+9NoWbG/vL8j9FwJT0S1dn9yuV1a09AyZvpavBQRBEARBECK+RobZEQ6Hcf755+P8888P4u4AAAMDA/jWt76FRx55BGefrY1iOfjgg/HOO+/gF7/4BU499VQ0NzcjkUigs7PTpHbv3bsXzc3Ntvd9/fXX4+qrr+Zfd3d3l1HhbRQHhQhRY7D53FYLVqtEc5FyKU5lcat3MAXUj8yxELmherCXh7i9vDj5+B+0/IoFk+oxc0wN/36hiuLP/+ktvLy+FWcvaMdtlxxakMcImrRQFCclpTuXYLV9WUV3bsdFEARBEMT+QTDDSgtAMplEMplEKGQ+xHA4jExGWyQtXrwY0WgUzz77LP/52rVrsW3bNixZssT2vuPxOOrq6kz/lQuiul0IazmjIqor3R6C1LJ+HlB68EhDSnfp4y1ITaMY1Uxxg6ujzxxU2NGfDDxxfVtbP15erwVo/nPV7kDvu5A4Kd25FN0tPYMAgOa6iqz7JwiCIAiCkAlE6c6V3t5ebNhgWCA3b96Md955B42NjZgyZQpOOOEEXHvttaisrMTUqVPx4osv4k9/+hNuueUWAEB9fT0uv/xyXH311WhsbERdXR2uuuoqLFmyBEcdddRI/VkjimLz76BhSreVqs2K6spoGAMWP0+kMlBVtaCbAsOBXNBQ0V168JFhDrdRlOKVujuFxO2KaDhLxW3pGcI4vTAMgnd3dvJ/z59YGpuVcsDh6t3duP/1bbjy5AMwY0xNTnO7mdI9rr4Ce7oHKUSRIAiCIAhHRrTofuutt3DSSSfxr5nl+9Of/jTuuecePPjgg7j++utxySWXoL29HVOnTsWPf/xjfOlLX+K/88tf/hKhUAgXXXQRhoaGcPrpp+P2228f9r+lWNCUblX4d2FgPd2pjIpUOoNI2HAksCC1xuoYdnYOWP7+UCqDihIfs2NpLydKCiNIzSG9nN22CKvuDmEMXzKdQfeAeexVe18i0KJ7a5thWS+V8z0tbY595x/vAwBW7ujEs988MS97+fi6Cqy0eAyCIAiCIAiRES26TzzxREf7Y3NzM+6++27H+6ioqMBtt92G2267LejDK0nE2qGgPd1Ro8hOZBXd2iJ2VHW0zItuUrpLnYwXezkTugOuqza09GLq6CpEw7l3+YiW8kQqgy6p6A7a9rxFSPjuKZWi2+Y52LhP+1tys5drRXdzPdnLCYIgCIJwp2h7uoncMBfdhVS6jYJZ7uFmlvNRVTHb33caK1YqZCndVHSXHIa93GlOt2K6bRA8vnIXTr3lRVx1/4q87qfdpHSr6JSK7qA3CkSlu6dEzne3Xvxc7OWtvVrRPbYuDqB4k+0JgiAIgigOqOguM8xBaoV7nHBIQTSsPcCgVEBzpdup6HYJWysFsnq6S0T5Iwy8JP0XIkjt/17ZDAB4Ms9Z2h1CT3cinUZXv6R0B1wNbmkzlO5EKlMSm2duKnQuSvegfv2qiWtmMbKXEwRBEAThBBXdZYZYdIcK6S+HoXb3SYoXK6gbq52U7tIvusleXvp4Si8vgL3caUPKD2Z7uYrOAXOCeZAbBQOJdNZ86lLYaMo4XGpUVc2p6GbPK2sNIHs5QRAEQRBOUNFdZoi1QyHt5YARpnbqLS+hP2Esvpn6la+9vKV7EFf/5R28va0jzyMtDPJinoru0sNbkFrw9vLAim7BXp5IZ7KU7iBTtXuGtPtWFKA6pm24lUJfd8qh6u4dSuVkL2fPq9iPTwnmBEEQBEHYQUV3mTFcQWoA0J8wCuc1e3r4v5mK3Vgdtf1dL0r3DY++h4dX7MSFt7+ax1EWDlK6Sx8vieT8PRWgajyqynhvyE4RP4hKdzKVyerpDrIOTKX1QjMUQk2FZqsuhXPeyfotK/deYc8ra7HRvkdFN0EQBEEQ1lDRXWaIlvJCz8EWZ3CLKg8rqBskNe8fVxyDWWNrtNt46OneLCQlFyPyGttqZjlR3GR4T7dTkJr2/yBLqmjEuPTmWvgBQLuppztjmtsNBGt7ZjPAI2EFtRXapkH3YNLpV4oCJ3v53u7BnO6TPa+RkPE6Ul83QRAEQRB2UNFdZpjt5cP3uH2C6s2KT6aGMRZObuCjxrzYyyM5/gHJdAaf+MMy/PCJD3L6fa/IytZAGYTD7W8Y9nL72/D08gBrqqTg9GjJsfADYBoRlkxnjwxzGsnol2TasFSzALFS6Ol2KoZ3d+b23LP3fkRUuuntTxAEQRCEDVR0lxmmILUCK90iokWWKd0Vkew53Cx8zYu9POyz6H5+bQsuuXMZHnhjG17d2Ia7lm4OtOiQkUXEwQQp3aUGs5c7nWmFSC9PCn3Ee/NQuocEd0UilUFnvzlILUj1lfVGR8MKavUNtVLo6XbqtV65ozO3+9Sf11iYlG6CIAiCINyhorvMEOvsQpfcN110MP+3uejWCoGKaPbpxcLXvBTdfpXurz/4Dl7Z0Ibv/uN947gKWAjLC295dBpR3Awk0nhtYxsAN3t58Ep3Im3cWT5Kt/g+Slgo3YXo6Y6EQrzoLomebosnYZw+X/uNze153aeodFOCOUEQBEEQdlDRXWYoyvD1dF98+GScvWA8AHOoGuvXjlsq3XrR7aH/2a/SbfXntvcmsr8ZEExFr4prf+eARYFPicbFyyMrdmJ3l1bwnjh7jO3teI5agI8tKt1rhRBCv2Qp3XLRHeD5lxB7uuNaT3dPCfR0WynQp8wdBwBYuze3557dpdjTXUhXDUEQBEEQpQ0V3WWGWKeGhuHVrdJHB/Va2Mvjlkq3d3t5xOcfcPCkhqzvtfblbt11g9Uz1TFN9RuQNhLufmUzFv7gP3hp3b6CHQORO+36uXHOweMxa1yt7e2MOd2FsZe/sqE15/sW30dJYWQYc5kEaYlnSncsbKSX95SA0m218TBzTA1mjqk2uRf8OGvSfE43Kd0EQRAEQbhDRXeZoWB4e7qr9UAlqznd8YhF0R31bi/3q3SnLZKM2gqodLOChm08iOnl7+7oxPcf/wA9gyk8/cHegh0DkTusRqqrtB9tBxTmfSQW3bu6BrGlrT+n+zHZywWlu1GfHBBkIZgypZeXTk+3ldJdVxHBhYdOMn0vZnG9soO990MhhW/KUE83QRAEQRB2UNFdZpiU7mEpurWCs29IKzhVVcWgF3u5l/RyQUXyogQm09m3aestvNJdpW88DCTS+O4/3sPFv3sNr2xo47eTU9yJ4oAXTi5vk0IEqSVS5vt6N8dAL3Gjp6M/yYvsxhqt6A5SfOX28lBppZenLK4LdZVRfOQwc9EdDfsouvW9jrCiIKxfZym9nCAIgiAIO6gaKDPMPd2FfzymdLMgtYSg4FkHqen2cg/jtUSleyiVQUU0u4gXSaUtlO6+wvd0V+tKd18ijT+9thWA2dpPfd3FiZcZ3QB41R3oyDDpXPXyfpBJpTNICecWm/cdi4RQFdXel4Wwl0cjIdRVlE5Pt9VzUFcRxdjaCoyri2Nvt/+NOXaf4ZCCUEgBMmqgzzVBEARBEOUFKd1lhjLcSrfez9yn28tFu6uz0u0vvbzfQwq5lZW2taBKt9leLrKnazDrdkRxoXKl2/l9wlo2ChWkBgBDFhtGbiSk39mnF90NlVG+6RNo0c1GhoWUkk8vr6vUjv+wqY38e36eK3afimI4JainmyAIgiAIO0jpLjPMc7oL/3iG0q0VxUyxUxRzyBDD6Om2L6L/b+lmbG3rM9nF5ZAyK6zs5e0FVLrZGrsylv02Ent0c6iniGGAFVmuQrekdKuqClXV+nlzJaFvOsXCISTSGf61H2R1fF+PttHTUBXl14EgC0H2/oqEFSNIrQTs5XZKNwCcuaAZ/1y1W7udj+eK3TQcEuzltLlGEARBEIQNpHSXGcPe082s1UNM6TZC1BRFwWePmQYA+MzR0/Tvu6eX/+CJD/DH17biRSH1eyDhvrhPWTRVDofSHQuHHEPfaDFenHi1lxsjw1RkMirOv/1VXPD/Xs0rzZwp3SwTIaeiW/qdVj00sKEyxs/HQljio+EQarm9vPiLbqtNLxaed/aC8bj0qKkA/PW/ZwSXBNt8IaWbIAiCIAg7SOkuM4ZzTjdghIj16fZvOUTthrPm4tyFE7BgYr3+fTan21+RMZBwvz3rb/3VRw9BS88gfvKvNVyBLwQqV7uAymjY1mpLi/HixGuQGivKVRXY3T2Ilds7AQDdgynUuySf25HQVePqeAQd/ckci27rc7u+KsoL8mDTy9mYLCNIrRR6uq2eg1r9+BVFwRdPmIE/L9vqK32c3WdIUfgGB22uEQRBEARhByndZYa5p7vwj1ejK3X9CbPSzULUIuEQDp0yiicDy+nlV//1HXzyztd5CJqdxbPfi9KtFwWTG6twoD53We6dDZKMsPB2CnmjxXhxonpVuoU53X0B9TCz85IVr4m0/80hO7dIfWUUrLMjyHMvmWHp5QrqhJ7uIOeXFwKr50BsDTBcAX7s5caGTYjby/M5SoIgCIIgyhkqussMsXwYjvTyqpg5vZwVAlYhagAQjxr2clVV8fDbO7F0QyuWbWoHkB0OxfDS083Up2hYQUwv8gtadOuLbEVRLJPajdvRarwYyfAwLK9Ftzk4LJ9U+qyiOwele9DmPdFQGRUKwQIp3RUsHd1byOFI4qb251I0s9c+HFIK0j9PEARBEER5QfbyMsMcpFb4qrvGJkiNKdoyYnq5qNRtbuvDsbOa7ItuDwt7VsiEQwqi+uNYzegNChWG2uW04KbFeHFi9HS73dJILxfnUqfyKbpTrKc796LbTuluqIpyJTfIU4+9vyJhBZXRMMIhBemMit6hFP87ihFmGz9wXA2mNFbhuFljTD/PpWgW8wDYeG96nxMEQRAEYQcp3WVGaJh7uhurY4iEFAwk09i4r9cIUrNRfpky3juYNCl129r6ANgXH16U7lTGUOLYuDG7Ij4IxIV394B9byullxcnGa8jwwSlu1voYc5HRWY93Ya9PP/0ckZ9VawgY6ySgtKtKAo/9i/8eTlvDylG0vpxV8YiuPPTh+PTeqgjQ9x08WoxZ4V8iNLLCYIgCILwABXdZcZw93RXxyM4dlYTAOCJlbtd7eWN1TEAQEd/0qTUbdrnXHR7sbCyhX8kpPAe8kLay1Whr7NPOD45XKvYe173V1TPQWr67aGiS9hcyUvpltLLvcytl7ELUmuojObUp+xGiqeXK6b/r9zeiTe3dAT2OEHDCmSLCYba94UTwOsmhcrvUymIq4AgCIIgiPKCiu4yQxlmezkAnHPwBADAs2v2cvXarseZFd2tvUMmpXvt3h4ADkq3l6JbX/VGQiHEIqzoLtxK2JjzbH6ex9bGTV/7SUUmhg+xJ98JBUZR1T0QbE93EPZy+fDrK6P8bwpU6RbeX4AxogwobpVX7L+2Qnz9vT5dRnp5bvZ0giAIgiD2L6joLjNCw6x0A8CcZi0pfG/3oKvSPVovunsGU6YZvy3dQ1BV1dZm2+cjvTwSFpTuHIoZr9jNeR5dEzN9TYvx4sSvvRyqWenO53UNIkiNKd01Uj91Q1VUsDznfIhZiHO6ZYo5TC3t8jqL10mvmwf8vR+ikWEEQRAEQbhDRXeZIa4rh6OnGwBGMct4XxJDunptF6RWL1hfd3cN8u8n0lqwml3x0d6XsPy+SCoj2su1x2BjjgqB1ZxnRQFG18Qtb0cUF16D1HjNDXNPd6728kxG5Q6M6gB6uusqzO0MDZVGT3ew6eVme/nXTpnFf+ZlpN9IkXZRusXve3m+RIdDSFEK0j9PEARBEER5QUV3mWFOLx+ex2ys0oruRDqD9j6tKLErukMhBaOqtCJhV+eA6Wfdg0nb4mNv96Dl9xmZjMqLqEg4JPR0FzC9XFC7rj19NgDg5o8s5Gq+cWwFOwQiD1QhDMsJtnmlqjAp3bkWtOJGUD72ctaeUVthVrrrTenlwQepRfSi+xsfOhAnzdaSwNn0gmKEPQd2RXfIp71cbBcJK6R0EwRBEAThDhXdZcZI9HRXxsK8h3uPXhzb2csBo687q+geSNkWHy09Q47HIBYyor08nVELpkAZc56Br5w4E299+1RceOgkfOLIKabbUU93cWL05Hu7vQrVlFKf6zg6cSOoNoCebrHoVhTtPo0+45wO0RI+MixkfGyw0MCRULp3dPTj2dV7XW/HngN7e7m/IDWxuFZCwpxv2lwjCIIgCMIGKrrLDHFZOVxFNwCM0tXuPV1aIW0XpAYYRfdOK6Xbrujudi66xcWyaC8HCpdgLvZ0K4qCJt1WPqe5Dq9cdzK+f95B2u3IdlqU2PXky4QEpbs7CKU7ZaF052Iv1++nocpwVqhq4cZYsU2GmOBiqdKPf7iV7p2dAzj2Z8/j8j++hfd3dTne1i1Ize/IMLG4DiuKscFBm2sEQRAEQdhARXeZIfcXDxes6GZ92vGovdI9ulorTmWlu2fQXune1zPkuCAW1cNIKGQKeypc0W0/cmpiQyUqY2HT7Yjiwun1E2Hvo4wKdAvhf7k6KNj5GA4pfHMqnyC1iQ2VqI6Z329MjA5ywycpZCYw2OMOt9L9q6fX8X+39TrnPaRci+7cle6QaC+nzTWCIAiCIGygorvMEBeQwxWkBhjqtWEvd1e6d3Wa+7S7B+x7uhPpDDr7k5Y/A4yQJ8A8p1v7WWEWw6prKjJTwAry8ESeqB6VbiNITUXfkFFc5hqklhACyWLhPIpuPUgtHg3hH1ceg3F1cVx+7HQAKIj6akwHEJTumK50D3PR/c72Tv5vt00tY063zfvUFKTm/tjicxoKGb9PQWoEQRAEQdgRcb8JUUqI68rhClIDtDFFAHhh7FR0j6vTlO49Ujha92AS1bHsU7KxOob2vgT29gzypHQZ09xc/Q8PhxSkM2rB7eV2mxvhAqiNRHB4dSAYI8PMromc7eV68RoNG/Pkh/Lo6Y5HwjhgbC2WXX8KPxd5n3FBRoYJSndcV7qH0V4+mExjU2sf/9rtZXCzlwPadSOjerOXq5K9PMydEPQ+J4j9iaFUGp+66w0cPq0R1+hhqgRBEHaQ0l1mjESQGmCo1wynILUlM0dbfr9nMGWpdI+t1Yp0p77uZCZbhWM22Fz6Zb3gZk82wqxoMV6MeO3p5unlMCvS+drLY0LRncs52tJjdpWI7/1CWJ7FzQLGSCjd6/f2mp57V6Wbbcg5FN3s+fLiDEhL9vJQAfrnCYIofh57Zxde39yO3z6/YaQPhSCIEoCK7jLDHKQ2fI87qkoquh2C1A6ZPCqrSAd0e7mF4sdu29Fv37vJZwgLf3SswGPD3Io2GiVU3Pjt6VZV1VQc51p0s3M8Gg7xgtmvvfy9nV34zwdacrfVJpZSAPU1ZdXTzZTuxPAp3at3d5u+dnsZ+Mgwh9dZ8eEMMPV0hxTBXu7+uwRBlA9OaxKCIAgZKrrLjJHq6WaztxkVDkp3OKRYFgp2QWpeChOrsKRohBXdhVkNqx6Vbiq6ixRhzroTCoyiStzAybnoZhtEEQWxsPY+8Vt0L93QClUFTp4zFodOGZX183ABe7otle6h4VO6t7T1mb4OROlWvDsDMhnz+97quX59Uxu2t/e73hdBEKXLcE9tIAiitKGe7jJDGKE7rEo3Gx3EiDn0dAPA+LoK/u9oWEEyrWojwywKZC99r1YFAes9zSWkygvGnGeXIDWylxclbq8fg/04kTYvsHK2lwtKd672cvbYrPVChhWYQe73sGOMiD3detE9nEp354A5UNGtD9stSA0wrpVeNsj4/QnZEeJxrNzeiY/+fhkAYMtPz3a9P4IgSpPh3GwkCKL0IaW7zFAEg/lw9nRXSSOL5K9lmoRiYYw+39rOXh6LuKuBzPpqUrr1AjzXlGk3vNrLKb28OPFsL9f/L59/ufd06/OuhaI7nVF93R+77XBu+PAWDlHp1u3lw9nT3SUV3a72ck9Ban7s5dr/2XPPXgL2XD+/tsX9TgiCKHn6hnGzkSCI0oeK7jLDnF4+kkW3s4mCFdoAMEZXvbuEonvSqEr+87gHNdBa6S6svdytaGOH4iURmRh+vAepaf+XnRa5WreT6WylG/DnyOB9yjZXcD/KrVfY5lXUSukeRptlV79cdLvZy7X/O9nL/Yz94kW8Yla60xkVv3hqLX71zHrX+yAIovQhpZsgCD+QvbzMMPd0D9/jVkbNpxJTwOwQle4ZTdVYub0T2zsGeGF95vxmLJ7aiLnja3HHS5sAGHOJreAhT2FR6db+nSyQvdxtzrNC9vKixq0nn8HcI/L5l8vr+oun1vKk21gkxMP+AK3ornRxiDDkwk+mkOnlkZDY0z38SnfngDm8yO1PTFsEwMmwH3nZIJM329hr0NmfpBRjgtiP6Beue5mM6poPQhDE/g0p3WVG8SjdzsWDqHTPHV8LANjXM4T2Pm1BHYuEcMb8ZkwdXS0o3fZqGlO6rezlhR4ZZvc0h6noLmrc5qwz2Ck1mMqvpzuZzpiKsmhYManGQw7nt0zapR/dTxq3V5JW9nL9fT6YzAzbed6pK901eo6E28ZCmhfJAY0Mk4LZ2P/JVk4Q+xe9gtJdqDY2giDKByq6ywxxYTmsQWpSkV3tYi9vqjVGho2qiqFJL8LX7e0BAJ7qDAhBao5Kt259DVnZy0e2p5vc5cVJxkMxBoDvquSrdG+VUrej4RAURTHC1HzZy7X/2/UpFya9nBXdxmOKyvxgcngs5sxePqpam5jg1V7u1NPNNyk8vATyc8+e61c3trn/MkEQZYMYIJnycvEgCGK/horuMkNcVw7nyDDZFutmkx1dbSjdA8k0ZjRVAwDW7NGLbqHXNc6C1BwU66RFsjKz7qZGqKc7VIDChwgOY9PE+XY8SC2dX9G9ocVcdLPzMx7OoejOuJ172v+DzBPg9nJB6Rat5qlhSAxMpjPo0dWlxipt487znG4vI8N82cuZ0u36KwRBlCGkdBME4QdaLpQdI9PTLQenuSnd4gK4ZzCF6XrRzQoPc9HtrnSzAkjs24xG9JFhhZ7TbbOY52FW9GFclKgelW4jSE2yl/ssaDfu6zV9zZwYuYwNy7idez6CwbzCN7aExxT/PRxKT7eQXF7Pi26Pc7oDGhmWVXQP54WWIIiiQQyQHI5NR4IgShsKUiszxDX4SPZ0V0S97+ccPKk+63sxQbH2kl7upMIVzF6uH46do8BPnygx/Lj15DOCClLb0CIV3RGp6PahdHPLtFuQWoCnnpFebrzHQiEFIUV7nOFQetiM7tqKCKLSfGw72Otkl/QO+OuBT0suAycFnSCI8qVn0NgELJSjjiCI8oGU7jLDHKQ2fI8bj5hPJS/W9peuPQl/+NRhOPaAJjRURU0/E5Vu3tOdcghSs0goHumRYSFe+FDRXYywTRO3zakQV7rzK7q3t/ebvma90bn1dDurt0G7LFRVRe+gZqWsqTDv1bKNrmEpuvV+7oaqqOdCmdvLvQSpeRoZZv6dsJSj8eFDJgBwTksnCKK02dczZJrTTfZygiDcoKK7zDAHqQ3foi+X/vEpo6vwoXnjoCgKaiscim4PPa/cXi72dOv28sIV3dr/bYPUfIQzEcOP1yA1W3u5z0WW7NRg57WX81vG1V7uo0fZC4PJDD/++krze5UVl4VWelLpDD5z9xsAgIbKmGdLuJw2bkVuI8O0XxKvfWNq47jhrLna49JmG0GULTf/Z63pa7KXEwThBhXdZYZ5TnfpKC21knomppfHo0zp9mAvH8b0crc5zzxIjXbAixJjzrrz7dj7KF+lWy6q5Z7uIR9FK7dM2yrdrLXB1yHa0qXbusMhBdVSKwkrugv1PmOs2dODHl1tP3BcrbCx4Px7Vr3oMn564PkIMv1SI9rWm+sq+H2parBBdgRBFA9Pvb/H9DWllxME4QYV3eXGCNnL86U2LhXdJqVbW+Q7Fd0pi4X1cNnL7TY32KKcFK/ixO31k5F7uv2qyLL9MJZHT7dba0M44NYGVnTXV0azni9mLy/05hIruAHgJxfO5+8vp8L2kRU78Ne3dgBwU7q993TLAXxiT/fYugrTRghtuBFE+dHVn0SH3urCrt9kLycIwg0qusuMkbKX54tsLxd7vOMeipKUhb2c9cwmfRQzfvA+p5s+jIsRt8KVYWcv97vIku3X0Xzs5awffZiS88WiW8ZQugur9LDxPAsnNyAeCXtyknzjLyv5v516uv3Yy+UQO/H9LyrdAG24EUQ5srVdG/84pjbOr4lkLycIwg0qusuMUKkq3ZK9vLE6xv9tBKl5ULrDVvbykQlSC5O9vKhx2zRhsPRy+WX0+7rK9utYHkFqabcgtQIp3XUWRXd0mJTu3iE9uVx3xfhRpwHnlHHDju+jpzuUXXQ3VsdMj0OOU4IoP7a0aaGY00ZXGZkW9GYnCMIFKrrLDHFZWco93U3Vcf5vQ+l2Si+3mNPNFMSC9XRr/7crfBQquosaVerNtcOuVvP7usqLMnZ+ehmJJ8MTuW2O3VCBfR2iLbzorsieMhkepkUnT0/nRbf2fa9OEuc53d4LeOYesBoZVhOPmO3lpHQTRNmxtVVTuqeOrubuOrKXEwThBhXdZYZobSwle3l1zLyYr6s0vvakdPOiO1vpLlSqstuc50LMSiaCg70ubptTdj/OV+nOZ063Ufg5J+cH1drgaC9ni84C2yt7hswjy/wmtDsp3fy96mVkmLTZJr4GVfGwaROHxgUSRPlhVrrZOoPe6wRBOENFd5kRE6SvEqq5s3pTxUIoHtGC1Bx7uvXCOiqODGM93QW3l7uMDKOFd1HieWQYrH/u93WVz8N8errTri4Ldrvh6+kutNIjK91e53QzvIwM8/KaytZ+0W0gK91B9dQTBFE8tPQMAgDG11eSvZwgCM9Q0V1mRIUVYCn1dDvhR+kOW9rLRyZIjRc+tPDOGVVVceO/VuPeZVsDv2/j9XO+nV1N7j9IzaWnOyd7uXOIX1CnXrdj0V3Y7ARGn65011aY7eWelW4ne7nL8yU6BjLStUYs5qtjEdP1gN77BFF+DCS0VrfqeJjnyJDSTRCEG1R0lxnmont4q+4LF00EAFx+7PRA79dLejlbkFdGjRnCUf57IzOnO+ixTfsjr21qwx0vbcK3H30v8PuWRz/5xa+KadfT7WVTye6x3WbED0d6OXOXFHxk2JDc023Mw/aCk+rvlIR+83/W4sifPIu93Zq6JQcoiudPdTxC6eUEUeYMJLWiuyIaJqWbIAjPUNFdZkQjxoJvuIPUfnLhAtz/uSPxP2fMyet+IlIl4SVoamfnAABg4qjKrN+TRz0FhdETbP1z6unOn437+vi/mboQFG49+Qy7otyP0q2qanZPN7eXu7dPyLDi0HZkWAHndMuw81z++4KG28uZ0q1/etltLKiqanptmVpvhVMo263PbUBLzxB++9wGANnPfdhUdGuvpdEj7vgnEQRRgrCiuzIaHrZMC4IgSh8qusuMaGjk7OUV0TCOPqCJK3e5EpUimdn9pTOqbSjajg6t6J40qop/j/WC+1EQ/WAUbXazkim9PF926q8rALT1DeGPr27Beb9dirbeobzv2/PIsACC1KxuKwep3f3KZizf2u7p/tjd2Vmm2Xs/qHOvsz8BwC5IrfAjw37z7Hr854O9ALKVbruHHUxmTCq4c9HtPjKM2eezgtRCZqUbEMYFktJNEGUH2wCuikWGLdOCIIjSh4ruMmMk7eVBMbmx0vQ1K54Be7V7e7uWJjrJUukemZ5ucdODApVyY/3eHv7vjr4kvvfY+3h3RxdueXpd3vftPUjNTC5tA1YqcMzCXn7R/3vN0/15DfELquZr69OK7qbaeNbPhsNeKb7ectFtV9j2DJmL7C4PRbfT25S9hryfnqeXG7fhx+aiwhMEUbpwpTsWMtLL6b1OEIQLVHSXGaK9vNSC1P542RFYNKUBv/3Eoabvi8r5UDJ7Yd87lEJHv7agNtnLo+7zvfPBa083QIpXrqwViu52XW0FgM2tfVY39wV7Sdz2pmQnA8sN8LPISloUpDH9vRrPwRnCi26bk08JWGlt7dGcBaOrY1k/G+4gIa9zuvuGzO/7A8bW2N6nl5FhbFPBeO717wt/d1VMt5fT5IKiQVVV/HvVbr4xSxD5wpTuCpO9nHpJCIJwJuJ+E6KUMI8MK62q+4QDx+CEA8dkfT8cUhAJKUhlVEulm1mQ6yujqKsw7K/DZS+3VbqFgogW3/7p6k/ytgEAaO8zLOX7eoKwlztvmjB69QAv+Ws/KqZVQSqPDPMDs3Lb/WqQIX4DiTT69EWmldIdLbDSLffys55uxaWwZT3gAPA/Z8zBpUum2j4Gews7PV/sNeQ93fovMdUL0NLLAeO9T60lI8+/39uDr9z3NgBgy0/PHuGjIUqddEblawqylxME4YcRVbpfeuklnHvuuZgwYQIURcGjjz6adZvVq1fjvPPOQ319Paqrq3H44Ydj27Zt/OeDg4O44oorMHr0aNTU1OCiiy7C3r17h/GvKC7KwV5uBbfgWijdOzqyreVuvxMErMawDVIzzevN7TG6+pN4Z3tnbr9c4ryzo9P0dXufYQ/eF0hPt3NPPoOlVsv4KaisVBA5vdyJ93Z24TuPvsd72V03fFgRGcBCsFV/zFgkhNp49j5toYPUOgSHAwDUxrWNNTdLOLOXHzC2Bl8+caapTUXGS+gh6+lmdTn7HbHo5uFqNLmgaHhzi7ecBILwwqDwfq+MhhEODa/ThyCI0mVEi+6+vj4sXLgQt912m+XPN27ciGOPPRZz5szBCy+8gHfffRff+c53UFFRwW/zjW98A48//jj+9re/4cUXX8SuXbtw4YUXDtefUHSU45xuwDmJfJeeXD6hQe4FL3R6uVvhk7+9/Jzfvozzb3sFz69tyen3S5kV2zpMX4tKd2e/fX+uV9hL4rY5FUTRnbQKUrMpuq3u95xbl+LPy7biZ0+uAWBs4ri5LIIQX1jR3VQds9ygiBY4SK29z1x013ic083s5TUWGwUyXkasMSWL/Z3suRi0SNU3QhRdH5ooMFYbRQSRK+ImWzwSEkYm0pudIAhnRvTT6Mwzz8SZZ55p+/MbbrgBZ511Fm666Sb+vZkzZ/J/d3V14a677sL999+Pk08+GQBw9913Y+7cuVi2bBmOOuqowh18kcL6iwD7fs9SpLYiio7+pGUY0h69KJpQX2H6vpcgtUxGxd2vbsEhk+uxeGpjTsdmX/gY/861INnerm0oPP7OLpw0e2xO91GqrNjWCQBorI6hvS+B3Z3m4verD6zAbz6+KOf7N4pu59tVC4v2CxZNxKRRlbj1uQ2+NlKslO6Yjb18KJVGVcz60szOh7THDZ8gCuG2XvsQNUBUuv0tOlPpDPb1DmF8faXj7cQNlqtOPgCNel85u77ZvQy9utLtrejW/u+kTBvp5SxITfu+uAg37o/s5cUC26QBtNdQno5BEH5g7S6V0TBCIYVnWhR6ZCJBEKVP0X76ZDIZ/POf/8SBBx6I008/HWPHjsWRRx5psqAvX74cyWQSp556Kv/enDlzMGXKFLz2mn0K8NDQELq7u03/lQu59IeWAs11WkG9uytbdWTfG5dVdLv3dL+2qQ0/fOIDXPT/XrMNZLLDrSdYtJf7vW8Zq4V9ucNmrx85XdsMEfu7AeDxd3fl9bx6TS//79Pn4JQ5Y/HHy47ALz96CC/6fAWpWdnL9SA1WekelNohxL9xXJ1W+GbceroDDPLiSneNddEdCedWYH7uT29hyY3PuY5JY/byI6Y34punzebf533YNo/bqyvdbHa2E15GhqWk9HKrnm4Ge13IXj7ysOBDILtVgSD8YiSXa+dVhPIbCILwSNFWaC0tLejt7cVPf/pTnHHGGfjPf/6DCy64ABdeeCFefPFFAMCePXsQi8XQ0NBg+t1x48Zhz549tvd94403or6+nv83efLkQv4pw0q59nQ36wW1ldV3j150j5eLbp5ebl90i9bVTT4TsdlnrF1PsCm9PM8PZLGPbHA/KcDZ38naBrZ3mNOHVTW/8Bqjp9v5ds31FbjrM4fzkD8vSdcyViqInb1cbofoHjACwcbqm0+ee7oDLLqtkssBIJrjyJwX1u4DAPzfK1scb8dmhI+qMs8ID7v0dHf0sd+zPm4RLyPDWFAc2z9hSrsc9CYeGy3ERx7xvJRbFQjCL6LSDRhFt9WECoIgCJGiLboz+gXswx/+ML7xjW/gkEMOwXXXXYdzzjkHv/vd7/K67+uvvx5dXV38v+3btwdxyEVBVLSXl0/NzQtqK6WbFd3NdVKQWti9p1tMXH5i5W5fx+SmdCsB9HQzmPr5wBvbcND3nsJT79tvKpULzKHA1F322rOxTIDzhoobbnPW7QjnoGxYhezIc7oZstItbjZwVcVtRnygPd0u9vJwbvZyhpgybgUbBygXzyEXNZ8VWI02mwUiYW5Vd7KXy0q3+XdNxxZyV86J4UG8RlDRTeRLf0JSuvnIMHqvEwThTNEW3U1NTYhEIpg3b57p+3PnzuXp5c3NzUgkEujs7DTdZu/evWhubra973g8jrq6OtN/5UI0Up5K9zhd4dsjFd2qqvKebjuleyiVsV1M9wgL/l8+s85XMct7gh12N8IufadeYZa26x9ehXRGxVUPrMjvDkuAIf1vlm3NYvGVT9GterSXy+SiYlqpIEzpjoflotu8SSTa6tnfy47dquADvAWDeaVPH5FWW2HdGx3N014pj2STYZbghqyiW/u/XdHd5qPoZqeA098gz+lmz/33zzsIkxsrcdNFB/PbeiniieFBvEZ09OUfwEjsv2QyKl5cpzl0DKU7N6cPQRD7H0VbdMdiMRx++OFYu3at6fvr1q3D1KnavNXFixcjGo3i2Wef5T9fu3Yttm3bhiVLlgzr8RYLUSG9q4xqbkHpNvf1dg+m+M5zs01Pt6rah5z0SCrbm5u9j5fxMuc5KJupXIjlU2yWCoP63ygX3bUVEf6cW81t94qhdPv7vVxUTEulO2KtdP/f0s1YcuOz2NDSA8AYiQcYarKRoG39eEH2FLPHtMuLYCNzcg0S6nMpuju50m22lzvN6d7VOYD1e7Xnb3SNd6Xb0V7OlG5pTvescbV4+b9PxsWHG21KYUovLxpEB4Y4AYEg/PLYyl343YsbARhFN7t2WIVlEgRBiIxoenlvby82bNjAv968eTPeeecdNDY2YsqUKbj22mvx0Y9+FMcffzxOOukkPPnkk3j88cfxwgsvAADq6+tx+eWX4+qrr0ZjYyPq6upw1VVXYcmSJftlcjkg28vLp+pmBbWsdLMe7/rKKCqi5sCkuFDMDKXSlvOQ5aK7L+FcAIh4mfMcCgFIB1N0i6pZGb20lqiqyjcWxki25up4BLFICIPJTJ72cm9zumVyCc6xDFILWwep/W35DgDAtx99Dw9+YYkpvTuRNo+tslO6FQ/BYF5hxbRd6nO+I3Pk96AMe8+PljZf7Pqw23qHcOzPnuPfb6y2tsVb3pfDa8o3PDy0JYQoXGnEeHtbB67920p8+5x5OGn2WAyZim5SuonceXzlLv5vZi+P5hgkSRDE/seIKt1vvfUWFi1ahEWLtLE/V199NRYtWoTvfve7AIALLrgAv/vd73DTTTdhwYIFuPPOO/HQQw/h2GOP5ffxy1/+Eueccw4uuugiHH/88WhubsbDDz88In9PMWC2l4/ggQQMGyu0t2fIpPr2DGqLqPrKaNbvmItu64KAjRVis1zZbF8vuM1KFn+Wr+I4mMxgX49ZpSnnD3nx9ZKV7qpYmKuueSndGXenghW59HRbF93WSjeDbSiIfyO7H3Y6he1C/HgR6fkQbWGvhd1xGiPDgreXq6qK93d1AQDmjq81/Yy9brKF+6X1+0yFuF0AnAhPQnd4n7LX283aLx4bpZcPP199YAU27uvDZ+9+E4Dc001KN5E7E0cZuTGs2M7X6UMQxP7DiCrdJ554omvP22WXXYbLLrvM9ucVFRW47bbbcNtttwV9eCWJaAEtpzndY2vjGF9fgd1dg3hx3T6cfpDWs8+s5WK4FkNRFMQiISRS9oooU9nG1Vegp6XX1eoqovqwl+dSH4vvjYFkGlva+oWfAV+5bzlmj6vF1cIYpXJhSAgTq6uMIBJSeM9cdSyCWCQMIJVnT7f2/+EMUouFQ7yIjtrM6WYwtXpI2GQy1FZnlT6ozR7xMe2UbjanNtdNIKf33Lb2fnQPphCLhHDgOKnoDllvLLy+ydwi4ilIzcP7NCm5DJxOG5rTPXLIrTjihlfXACndRO40CJv7bBM8X6cPQRD7D0Xb003khrgw9mubLWZCIQVnLRgPAHjiXSNlXE4SlYmHjTA1K1hyMusZdwt1EvGSfp2PzVQMZhlMprFFGmn21Pt78ZvnNvjaKCgVWOK8omhFqRjiVRUPcxdDEPby4QhSYyFcM8ZUG/cTsraXy4hKN/t75TAvGRbtEETRZxTd1o/FgtRSPhad4oaSUwDRqp26yt1cm1X0W6nTqqri5fWtptt5Kbq9bFIYQWra13YuA0DYmCGle9iRXU/iNaLPYrwbQXhFXEfs7NTaXliQWpI22AiCcIGK7jIjUqYjwwDgzPmauv3aRmNRPeCgdANigrn1Yosr3Xo6em493fa3ycdmKio0Q6kMXtnYanm7lp7ys0yyxU1FJAxFUVBbYSykNaXb2l6+YlsHWixmuVthzFn3d2y5BKkxlbShKoo3vnUK3vnuh/jP7IpudljiQo+dExne021zjHk4LGRY0RK3s5fzkWG5zy23mj2fzqj4y5vaOMf5E+uzfm71N67a2YWdneawRTnrwQpDNXcourNGhrkX3UGkxxP+EFPuMxnVdK6V4wYlMXwMCNcptqZga6402csJgnCBiu4yw2QvLyOlGwDmTdBGu7X2Jvi8Va50R607JViC+VDSxl6uL8Ka9aK7309PtwelOxcrMkNWcZ/+YC8A4OaPLDSpOXKvdznAijC2aSIr3bynW3iO3t3RiQtufxVH/ORZeIEXTz53p3IJUmMqaTQcwti6ClNhIE4csEL8G3mQmou93EjjLry9nB2/n+dD3iyxmp/8wtoWvLy+FRXRED61ZFrWz5nSLKrmogvGD+wUcNpIkZPjnc4bspePHHXCtaKtL0FKNxEYA8L585uPaVlE7PPAaiwkQRCECBXdZYbJXj6Cx1EIqmIRTG7UgkzW6eOA2M6zrdIdcbaXsyC2cbq9fFNrH+58eZOn+bpe5jzn01srFyb9iTQaqqI4f9FEPHHVsdzuW45F95CkropFt0npFl7X1za2+XoMo6fb37Hl0jKQTGm3jVg82KjqGK493b4vX/wbk8xern/LzuIcZJBXwiW93AhS877oHJKU7S1tfVm3ae3VzutjZjZhdnNt1s+t7OWvb9LOgRvOmou6igguOXKKp+Px4gxgNnhPowI9jCAjCoOobO/uGjBd+0npJvKBrTe+e848nDRnLID8NtYJgti/oKK7zBD7LstM6AYAzNbDlHjRrdvB7YruWMTZXt4rKd0A8KN/rsYyKYzJCl+L7xw2wa3sugdNqEM4pGByYxU+NG8cAKClx5udupRgrxezBov28qaauPC6Gk+s3/M9157unEaG6SdAxKZwveKkA/DxI6wLRKv0crdjD1JpTeivhZ0NPpeROfKG0urdPVm3YTexU5T53yg8bGuvppgfPr0Rb3/nQ/jxBQs8HY+dHdzUey7N6XZKLw/nsdlG5Ee/0CK0q3PQtBnUT0U3kQeDFpv8bDMyRfZygiBcoKK7zBBHhuUxTalomaUX3Wv3aIt0Zi+369t0CtxSVZX3dItFNwB09GfbXWWMnmAPNtNcerotjnlCvTGyZGytdszlqHQPJiWlO24o3fMn1lmODFN8eju89ORbkVOQmpBebkdF1DoozGwvly3O1vcV4tZrz4doi9ucbraR4KenW34/rt7dnXUb9p7xo+a36SOhRlfHbDc4rLAbGSZ+mZSC1JwDFLX/k/o1/PQLFuC93YNkLycCgyndYnBrLk4fgiD2T6joLjPERX05jrCY2lgFANjdpam7TiPDAKGn26KAHUxm+KK4ud5cdIt2Zju8KN35LL6tPsTFOaFjarX51eUZpKb3dOuvX/egMepndnOtpb3cv9Kt/d+v0p1bkBpTuu0fy27jyFR0p8yFn53aGqTl0TVIjT+WD3u5h6LbTVFmrwNTo/sTKb5Z4yWxXMRuZJj4GrN/pgvcVkLkhxh21TOYNG3M9Q2lPLUOEYQVAxab/Lk4fQiC2D+horvMEHtGnUbxlCosWIsVMfmkl/cMaYWcomjKmIjVczeYTONbj6zCy+v3AfA259kq7MkrsgUXACY0ZBfdbkr3/a9vw23PbyipxSYLvmPqrzijPB4JWxbdIm6qg/hcDIe9nJ1PEYfQtIqITdGdi718OIPUwmxkWO5K9/q9vVnnp1tgGXOYsFq/TbeWxyMh2+uBHXbPl9VrbIxrs78/6vMcOUSlu2coZboWpDKq5XWVILwwoH8uVUZFpZtGhhEE4Q0qussMURUqxwVfVEqt7ud2L7v0cr3otkgvH+DJ5+Gshb1VMff4yl24//VtuPSuN9CfSHnqCc5nTrfVMUyyKLqdlO5MRsW3HlmFnz+1Fi+s2+f7GEaKQUnpZv3OJ+vhNUZ6ubHAFm3+dsF5DPHlGJ4gNedZ14CxQSRjClLzOLYq2CA152Pni04/QWr63zSqKsofQ7an8+LW5imT/8Y2PQF9dHXMseXD+r7serrNt+vqT+KOFzeZfsfp/srxGlzsiAnTvYOprOton48JFQQhMmhhL6+Ja/9u7ys/xxlBEMFCRXeZIS42y3HBFw3LSrdzkJqTvdzJmm5VQLD+bwD465vbPc7pzqOn26JHVlS62diwHsF6LdMvWC3/8sZ2JNMZrN/bE6jq/djKXTjh58/jg13ZFuFckZXuTy+Zivs+dyRu/bg2psVuTjfDau5zKp3Bxn29AMzFqN8CjSndfoJzmAriaC+X7NssNdxcdEs93TZ3Z2eXzgX2+LZBajlsQjDniTj6bkB6zdyUbjlxnC16G2v8Wcu1+9IfU1a6pa+//pcVxu84BakF6DQgvKOqqilIrXcolXWN+Om/V1P/LZET4kY9Y/7EegDAmt09lp87BEEQDCq6y5hytJfLxZZrT7detMkLevF3Ky1+10plFvuKH393N7e1OtVs4TwCrawWhuMbjN5zq1nVMuIC9IV1LfjuP97Dh375Eh59Z6f/A7Lhqw+swNa2flz795WB3acxMkx7bSLhEI45oAnVeqBazCK8S3y+rDZZrrx/BU65+UU8smKHqRjyq3SzY/JjU02xnm4ne7nU083Gag1Z9XS79DsrktK6cV8v7nx5U06LQva82oXA5ROkVh2P8OdfHiPG+9btZpFLrRvMXt5YHfd8HAzZQWMcg/lven6t4RZxzHIIcNOD8M5QKmN6znsslO6/vrUDDy3f4et+V2zrwJm/fhmvbmgN4jCJEoWtI8Rr9cSGSjTVxJHKqHhvZ9dIHRpBECUAFd1lTDkq3bzYSrHwpOydZxGWet07mD0qhhWkVdFsa7pVIdvZbxTdy7d28KJLHGclk5e9XL//gybU4eLDJuGqkw/gBR8gJLM7FH+i1XIwmcEDb2wHANz05Frfx+N6vC6Wbj+w4tAuvMtqZJjYQmBVXD75/h4AwJ0vbzZtgvjt6XZKxLeDbYDZqcVA9nnE7t9UdPOebu1ru2MXi/HBZBqn/fIl/Oifq/Hb5zZ4PmZAO2/ZY9mml+dwjovqOXvvyhtjbhZ6OXG8XbCX+6VJb9Vgs8H5MTj8TU42ZfZUleM1uJiR3/dW9nIA2NEx4Ot+L77jNaze3Y3P3vNmXsdHlDZW6eWKomDRlAYAwIptnSNwVARBlApUdJcx5ah0y/ZyY26mdU93nW5f7bawYHOVPG6hdFsUslZjxMIhBXUOSed88Z3HyLB4JISb/mshvnnabNPP3cLEAPvCwEvQ1J6uQTzx7i7PhYNTQekXrnTbbKZY/e1iWJ6TohsNhySl22fR7RDOZwdPL3eQR1l/M4M9B2LfOreXs4LU5v7EzYor7nubv4Yv+1TqxOfX7vVllvlcerrjkRBXjQal3AX/9nKmdPsvusfqRffe7iHT3+x06q9vyZ4tziB7+cjQL40Ek4PUGKN8nCPPr23hLg63rAiifElnVH5tkDf55zZro0y3tPUN+3ERBFE6UNFdxpTjyDAW5iTbyytj1qdyXQXre85WuuXk859euID/zKqQ7dCVbrH4GFUV9TSn20kxs8NtPrKXonsgmf13O92nyEd//xquvH8F7n9jm+ttxePxyhub27Gz01pxMkaGef/bzQW4/XMSDSumYsrvqDHmNkimVc8bEqz/22l2dEOVuRBgGwfm9HI9SI3Zy20OvjoewSeP0sLn1uwxikPZwu2G+Nh250wuSd2G0h0Wim7rnm67p4w59VlhyzbF8im639neicN+9DRaugdNx2CF6HzJOjYKUhsRXpTCInuHkpbXR68bRBtaevDZuw11O5dz67WNbdhlc50jSgfx+iQX3XWV9usMgiAIBhXdZUw5Kt1ysWXYy63VZjZvu3vAXulmv/uxI6bgI4snafdvsSjr1Bf1s8fV8u+NqnJehOXT25lIa8dnV8wyq30qo5qK+qFUGjf+azXe2d7J/8bx0hzyvXpR4cRWfUzXEyt3eTpeuwKZsWlfL7796Crs7BzA5tY+XHzHa/jKfW9b3papnnaJ3ryfPW0shMRC20npjoRCUpCa42FnP7bwd3q1mPOxWw5Kd4Ot0m08Rjqj6pZvpgLbP+anlkwDYO7r99vTnTQV3dbHHhXOQ68khD7xCpvcBSO93E3p1m43yMP3/I0LA4Bxdcb7o3swhaf0VgSrwMEpjVUYWxvH/553kO390ciw4efm/6zF9Q+vMn2vdzA7SI19f9WOLu6OsGNPl7ndgG3OeGX51nZ8/A/LcOIvXvD1e0TxIV6f5M86trnfO0RFN0EQ9lDRXcakfQQblQoxn+nlzvby7N91Uo+Zknagj6I7n8U361t3U7oB8ybBb5/bgDte2oTzb3uF28vH11dgelM1v01Hf9JxgSAWG14tsjGbOdOML927HPcu24bL73kT29q1gn63i9JtN7vaVem2GBHHiIQVqMKPc+3pFo/TDaaAsCA4K+RzaSiVRiqdydqwSQrfczp2q15p2cLtBlekwyFbRwezzPuyl7Oe/WjIVen2OqfbLWXdiaaauOXXVm0hV558AN644VQcPq3R9v7kDQGisCTTGdxqkVfQM5iyDPh7fXMbzv3tUpxy8wuO9yuf8qN9JuO/vF5r5wgy74IYGZgzriIayrom1eib+06TREqFlu5BnPjz5/H/Xtg40odCEGUHFd1lTDkq3VEhKVlVVT4Sy7bo9mEvF+9fXiR959H3sL1dKxAPHFfDvz+q2j5EDRBHN+UepGanMIrFxerd3fh/L2xEW+8QX+gBwOZWrcesKhbBvZ87Eh8+ZAL/2U6HMKEOwTqbcNi8EYtOu3Rrxrq92riuNXt60KU7D6xS5QGjaLZTuq3CzMRjcSqGY3n2dEdCCk+u9rqY3qcHdI1xUMoqpL81mVYxaHH/4oaCnQoMGGE/YqE96KMPXTsG9/nizE3SZeEmsYOd2/GwQ9HtqnRr/2evpaGe+7QuILtQZ49tdQmdIWxe2WHYy30fCpEDdg4O8TPw4En1/N9vbukAoF3nUg4vkny/fhL6ZUTHCVF68BndFk6aWl50l/5r/NvnN2BLWz9+9uSakT6U/ZZ7l23F2b95GS097o5EorSgorsMOWn2GADAJ46cMsJHEjxRIbG7rS/BU6hrbMLMHO3lFkmkVsVcIpXBn5dt5V+LSrdbjx+rF3JSunnBYzcf2fj+Bbe/ip89uQaLf/QM3tneyb/PPjgrY2FMbKjErz+2CAv1xefrm9tsH1vsQWxxsKKLqfCxiHOxUyOovF26a8BusTzIg7ZclG6bMWFOim4krOQ1MkxRFMf571a09mhFt6yoyvcrY6WciIWzF6Xb9Ls52sud1OPmem12fM9gyrO9km0ciOnl8mvmNhaN/e3spcxH6ZZhx2KVxTBjTE3W92TYWzZIpfuRFTtoJJENVu93+a1xx6WLecq0CNuYtELeFPSbidAnvB/2dg853JIodgYcim722VYORbeTS4wYHr796Ht4f1d3QabMECMLFd1lyF2fPhwrv3eaqTgsF0R7+asbtaJxTnOth/Ryb0o3W7CLVllZoThgrLHolsOvZFj6tlXyuRtuBU8opDgqkCLVwt944aFa3/odL26yVXnEkTp7ugdtlWNxkeGmAo2tMwrONr2XMplWLY9hKGlY+aywmlFup3rLP4uGQxCP1CkIzw6/CeZelG4r2POrKIaNWyycnXq6rYtufwsqtqngFLxXE4/wzS27dgEZrnRHjJ7ubHu59n/39HJZ6fbf0y3DXldxs6wiGsKUxqqslHkreHp5QG6jl9fvwzf+shLn3Lo0kPsrN8T34WFTR+HmjyxEjfSZMK62ApceNTXrdz/Y3W17v/L7xW96uVho7+ki1aqUMXJGrJTu/Hu693YP4sn3dlvmSAwnKsrPIVmqtPTQRl25QUV3GRIKKaivdF8YliKs2FJV4MW1WlLtMQc02d6ejfPqHUplqc1MhRALdiOgSyy6zcWAWDy6qaRLZowGADz53h7nG1og9tPa4WbpZlQKf+NHD5+MimgIOzsHeG+1jKh0q6r9XFux6HZbkIrn5Pu7jIWupYXao9JtmtPtoHSL88rFkWF+VW7++GFWKLovwgeTaf48jXFQuq1gDo1YOMT/5hseeY//3E4FBrTrgFu4nRtuCfqMiQ2a2r3LY2EhqtIVrnO6re8jy16ep9L9648dwv/NlW79vmsrInjlf07Gv752nKdNGm4vD2gBvXxrRyD3U66w935dRQR///LRuGjxJJP7KRJSEAoplpkKq3fbj35j52ST3svt1ykiBlaSVbS0YZvDVmMfa4V1Rq5F89ceXIEv3fs27nhpU+4HGQBl2JVYsvRTMF/ZQUU3UVJEBQvzaxu13uWjZ462vT3bgQbMVmhAsJcLO9dRi2JOVLrv+ezhpkIw5aLunnPweADAsk1t2Odz1zLhoeDxWmCIan5FNMxtzhtaenHDI6uybKvyiJu3trRbHr9of064qL5i4Sta4AcS2b/HFrd+RoY5zenuE17DjKpyS7Lffm4GU7qtkpFlWnWVOxYOoa7SPkjNClasxyIhfh4sFWZtux1/pUXWwe9f8h6Qk0gZirQTLB3fq9Itbqqwovv7j3+AlcJ54T293HysuRbdHz5kIp9ewM4lfgwhBaNr4qYWCSeCVrrlaxdhZshChawVim52TtRavH5bHWYrM8cN2zD06xQRi25SukubZMZ+7CM719IZ1TanxI1lm9oBALf8Z12OR0iUG5SGX35Q0U2UFGIByizKzdI4LJGYYF+VE8wt7eUWtmVx7NaJs8ea7kMcNWTF5MYqzB1fh4yqFd5+cOvpBqwLjNMPGoc3bjgFv/roIfx71VLxxcZTfeW+t3Hf69uybKvMDs34n4dW4fAfP2PaxU+mM/jBEx/wr92UbtExIBbwVuoRuy+78U9WjgSnOd3iY2vp30xFzbHoZj3dHhbh+3g/d8y3lZ2ds/FI2HIx53b8VRbP30/+tcZzyq6XcxAAxvtQult7h3DPq1sAmN+fAPDh217h/3ZLL2fWelWyl3ttubCiQuovZzWzU2Cd5bEFrHSXQ69oIWE5B+K5NKXRCLxjqeNWSrfTc8uuTWyygJ8gQlVVTfZy6ukubVIO15fKaJhvtOXyXhXb2RLpDN+oHQlo4ELx0Efhi2UHFd1ESSFau9wsyAyWYC4X3UzBrnQZGcbneQu3+90nD8VHFk/yFFZ35HRttNCbW9pdbyuSZP20DgFlVkX34qmjMLa2whTaVSn1NzZUaotIu4R7O1V+5Q5DEX985S6s2WNYM90KULv0Xqei25/SbT+nW3zsRErlxVSONTc/Li893a292uaQl37uM+c3m75mWQTxSMgyKd3JXg4AFTap/l4XhryQdQnJm+BD6X747R383wsm1lv2ngPuSjfbwEikVdzx4kZsaNHS8fOx1PPXNWnu6fa7WWKMCsz5UEz0DJX+KKJCwpVu4bNgdrORvTG3uQ6AXdFt/9yyjS62SeknZKp7IGXaKNtL9vKShrXaWNnLFUXJK0xNniTyiuBmsuOGR1bhs3e/4TlXhCg92MhXonygopsoKRRFySo03RbZRoK5+cOQKd3VsWwborjzbHW7M+aPx88/stBWiRVh83zf2Oyz6ObBUP56ug+e1ADAPFNWHqlW7xIGZbfT/sTKXfzfcmHuZrWWe+MZVgquEaTmrHQnc1W6M/kq3eY2hJfX78NT71v37e/zkFzO+PlHFuLXHzuE35YVBLZhei6Hb1fQeh1f5CVXAADG6wnmuz0o3bs6tducf8gEnDRnrO1r7Kp066/d6t3duPHfxnibfILU2LGw19Wwl/u7H7nfPF9I6XaGFR7iZ4EYJDp3vFZ011pMuXBWurXzoL7SULq99uyu3WvuFaf+zNImldF7um0uBkbR7X+DbIvU4rB0vXPR3dGXwH2vb8Pza/fh8ZW7fT+eE2KQmtM4PaIwiNeXPrpmlB1UdBMlh1wAuBXdo6u1Aka2TPc72cuFD5s+C0XcD4dNGwVAW4T52ZVmPd2ORbek8v/qo4fgKD28TSzy5OeowSVojxWKsjr7rt77fdOTa0yFDuCs+mYyqm3RbdUnWUilO5nOCD3dtofsiDgybCCRxqV3vYEv/nm55azqf7+nLYpmCqn3dtTEI/jwIRN5cBPbKLI7B9wUWLv59V530L3ay1mbhZewKHabhZMbANhvrLC3oP3IMOv7z2dkmJyknmEJ6n7t5VzpDqboFqcvjHS6cTFidb2wKrotlW6Hha2sdKuqtxwHQNsMEvH6e0RxkuIZK9bXAjFMzS9b27RAU9YGtnRDq+P7/L1dhuPs78u3+348r9A5O/zYjUElygMquomSQ/7Qc7OXT2rUVLjtUlK3lW3cyV5uV8C4MaYmDkXRFmxd/d53wZna61REiD9bNKUB5y+ayL8WZ4jLF+8GG6X7xn+vxq+fWY8O/Tgnjao0/by1dwirdnTh9heyw7icrJdO4TJWP+NBanYjw1yD1Izvv7qx1ZRQHERPt/j44sghWUFet7cHL69vRTikWI4rsoMVD6LSff2Zc3CIXqh6xa6g9bqD7mVON2Ak+nsZcdKi97aOra1wPEavQWoy+RTd8vz1XM+TsDTOLF9E9SyoQr6cGLRwxswYY/R0z27WCvCqaDirpcRJmWT3K25Sel0Is6J7pn4cNP94eAn6fZLk6eXW1xdWdOfiSmFTRC48dBIURXMMscwaK1YJwadvb+v0/XiOCE8bnbPDz2BCdumR2l1OUNFNlByy6mZXmDGmNFYB0Irulp5BXPfQu1i1o4tfzCxHhgkLKyt7uR9CIYX3lVspoXbs0ZNvnXqB48JzMUqaGS4qhLLKzHq6RXZ1DuCOFzfhl8+s47/fLAXF7esZwgNvbjN9jxWTTrviToEgVunlPEjNZWSY3a4wK8A/2NWNT/zhdfz8qbX8Z4mUUXQH0dMtJr/Lqj1TMOZPqMNk/Tz0dv/a3y2ml3/xhJl49IpjfB2naC+PR0K8aO+zcR3IeLWXj9XP0c7+pOtYJdbbOk4v1MXwK/F43ezldq9dfkFqZqWbBaE5zUO3Iuj0cnEhb5fDsD9jpXTHI2Hc+anDcMvFCzG9SSt8QyEl6zo+mMyY2lRM96u/n+sqo/x88zo2jBXdzNFRrqrhg29sw+0vbCgqB0ZHXwJH3fgsrv7LO4HdJ3vf2Svd2md8LvZy1s41dXQVxumbkXZjOgGYPnMSqYxl3keukNI6sshhjW299psvROlBRTdRcshKlltBwIruB9/cjhN//gIefHM7zv3tUq7mir3PRjEn9NXkaS8HjJEzfopu1h/L+mWtEJ8LK/Wa1StHTB9lPh6L28q2uNHVsay/uWcwhf9IvcssPd5pV7zfwc5sHaTmonSHned0s02GlTs6s343kTaC1OwKOjfYaKKhZMakOsgW+5RHpTj7/rXbszFDVrZYL1RJLo7quPa1V6Xby9g6QDu/2d/oNBpve3s/trdri0mmdItOlVHCeek2p9tOAQ9C6WbnpOqittsRfHq5MJqvTIu3fLAL1Tx13jhceOgk0/fYe0DETp0c4Ap6iG8AelX/turq5Tzd2h5kYVQsZDIqrnt4FW56ci1O+PkLvnNLCsU/V+3Gvp4hPLxiZ2AbXykXpbtSmnzgB1ZYja6JcXfZjo5+29uv39tr+jrI3l+rzWti+JDXQ5TnUV5Q0U2UHGKRHQuHXAunKYLCKCu+4ZCCRkEhjnKl27id1Wgxv/gtulVV5bOyJzQ4j0RjyEo3ALx2/Sn4yxeOwuKpjabvW/V0y8c2pjZuaf1tlXZeJ+rjonJVuuUPmXRG5Umxdq0Dsr1cVVXTopY9d1b2Xq2nO7ggtQ92GfZyecHF1BG3lPHs+9f+7uXbOgAAc8fXOt3cFnHTJB4JcZXP6ygS9pxGXQpZRVG42m3X193Rl8BxNz3Pv2aWdPF1qxfOYXFGthV27/t4HkFqbLODLTzZKe3bXh5genkynTGdV6l08SiKxcKQSzuKiNWsdTt1UrSts/v+5dPrsK3NviBisM8NtqlbjqqhqMpta+/HD554fwSPRuNHT3yAbz/6Hv96V5f7RAUv8PRyG6XbquXJK0zpHl0dF4pu++OWZ74HOc9Z/Dwux42iYkdeQ+Q6950oTqjoJkoOUXXzMh5oioOtt6kmZlq8W9mWjZ7u3NRGwH/R3dGf5Is0xznkJnt5diE9rq4CR+rBaiINFgV6h9RD1lQTt7V3i4zXjy+dUW3TTu1C1IDsDxVxd73CradbL6Dlgn9LWx9U1VC0RbSebu3fuQepGfbyzn7jeRuS/haWeOumFGfdv/53M4v1gon1/Gd+7NPipkksHOKKuZPzQIS9FhUe3me86LaZRyy7DtixiQWP+DSlXRLmCxOkZla6udru80QJ0l4uvy/trND7M27tKCLWRbeb0h3m9/3wip04/ufPW7bFMFRV5cfEWovKsYCRr+vv7ewe8b/zzqWbTV+zUYL54nYtZ9flXJworH+7sTqGSaO09Yqd0t0zmOThf0xdD3Kes517jBge5PWQ13YWojSgopsoOcSZwV4W2E490fLP4ha71fkGqQH+i26m1DbVxByD4sz28uxC2u14RPZ2m3fP54yv9aQcifZ3uw9pJ/tbVtEt7PTatQ4wNVNVNTVZ3h0eTGbQ0jNkWfQkTT3duSrd2uMnUhn0C8c/KP39TrNdne/f/Hebi27vl207e7lXZYQ9r15G4xkJ5tZFt93il/XbAmYV1y293O61y6enWx4FZ4yW83c/QdrL5eezHIruHz7xAb76wArThlU++FK6LcaGddsq3cb5L8+qtxurCJivg3X6tbYcCxhx46FOf17Xt/TY3bzgWPWVB1V0u13LoxZjLL2Qyah8Y62pxl3pZp/TtRUR7hbqDdCCLG4ck718+JGL7MFkGl39STz4xrac8gJKCVW1F27KBSq6iZIj5lPpVhQFd37qMMufsb5Sft9S0Z1KZ4TAtdyL7jqfRbeXfm7A3V5uhxyQJj4m47gDxnhSjsbVGxsXdiqHk9KdVTDrH/SRkGI7E1X8uxOpjGkhwhYtW9v6LRcNWk93bsWU/PhDqYxJNc5Surkl0afSLTzvNfGIya3hp+gWg8ligr3cayLqkNDT6gZTuuXNG8Y6YW7xj86fz//9oXnjcPbB4wGYQ8JyTS/3+1yLyEp3Osc2BD6nOwClWy7ukgW2ly9d34qN+4IpVKzY0dGPu5ZuxmMrd+HL974dyH26jRgUsQrEtFO62flfGQ2js8987bYr1LXfE1omeNFtXBteWrcPmwr4HA8XbMO0sTqGgyZoG4PPrW7BKxuc50wXCquAyI37+ixu6R+3a3mu9vLuwSS/7pmVbuui21gbVHDXRpD2cpPSTenlw45cdA8k0/j+4+/juodX4esPvjMyBzVM/PHVLZj17X/jtY1tI30oBYOKbqLkMNnLPShwAHDMAU2W3x9TY1a6o0JA143/Xo1533sK/3hnF4BggtTY3GU32Hiz8Q7WckAuup1nb5uOpyrKR5ww5D6xw6aNsi22Dp3SwP8dj4T57r+b0i2OMWPIHzJDHtRVueje0qotrKaNrubK6Za2PvRa2KgTqbQwpzu/nu7+RMpkJ5SVbsOS6O9xxOe9sTpmUnX9FN0VctHNF2neFAyrUUx2sPReuw2WtXr4z+2XHIpPCuPTFEXBZ46eBsA85sctvTzXDRMn2OvKNoLY4fjtyec93Xko3Zv29eLav63Eqh1dpu8XUglYtaMLn7zrdZxy84sFe4xXNxgLqje3tAeyMWEXpGaFldJtpxSKQWryPG8ndZFtHIZDCt/4YsXYW1va8an/ewMnF/A5Hi742M1oGAdN0ALjbn56HS6583U8v6Zl2I/HalN7Z2cwPd1u1/JYjko3y0ipq4ggFgnxDIBOm/Gi7HN6XF0Fv573ebyee2HQpHRT0T3cZBXdiTT+sVJbgz47Au+p4eR/H/8Aqgpc87eVI30oBYOKbqLkEAsuL8oGYB9+ItvLY7yYSuOOFzeZdq1zHRkG+LOXZzIq/vLmdgDAoimjHG8rqv5+7OUA8LHDJ5u+FpXuzx07XQsPEp5fUR2//NgZuPpDB+L2Sw4FYO5xtoItXkdbFN1yb6QX1SocUnhhk0hn+GiuqaOrMHW0phRsbeuzXBgnTUp3runl2rF1SOqXrHQblsTclW7ZYRHzUcDL4/DYfXlVugd8FN3sPWa36Nyk2zxnja3J/l39tWQLW0AMUrN+vFxfOyfY35ltL88tvTyfevLSu97A35bvwM1PrzN9v5BK97JNhVcYlgoqaCqjOs4j9sqgD0eG2NPNNirdgtQqLc5/J3WRH08kxK8VLH/iPx/sdT3GUoFduytjYRw0sc70syfe3T3sx9Nt8fk6EFC/s9u13LCX+3t/tvex5HJtLVIpuW1k9lgo3UGml4vOs3K2l/cnUvjxPz/AuxYTTkaSbOdfxvSZWa72a7E1pJw3e6joJkoOv0FqgH0flp/xY8M1MuytrR1Yu7cHNfEIPnHkFMfbin//qGrvSjcAXHP6bHz+uOn86916yuv/nDEH3z5nHgBzsfX542fgjksX4+7PHI4z5jfjq6fMwlkLNFswex4vuP1VywUAU69HCUU3e02sepjkv80Kcab6ljZD6Z42mind/ZbHIgap5T6nW3teOqSe1Cylm4+Zyb2nWx4X5pYkLiI6C0Slmykjz69twW3Pb7BVG9kCwMv7zK2nsVdf/FqNq2ML2XTaQum2eZEKUHMbm0cjHKSWyai2Cl0he7pFy3ShFnfv7TIr93btCH7wo3SL7ye26bqp1dqC7LTp5Fx068cTDZvyJ5JplbuYiom93YPo6EvguTV7ceLPn/e8+TKQNAK95o2vN/3Mz3jMoLB6zKBUYPZ+sFW6pTwIL3T1J3HxHa8BMK7VlcLGqFWP+m79/dJcX8nPZdmFkQ9ioV2qxc+Ojn788ul1ji0gT6zcjT+8vBnn/faVouqVzloPJdKY0GC0Ga7bW/ptKVaI7RTpTGmed16gopsoOUwjwzwWIIqiWBY+YkCV2/0FEaRmtRMvw1JLD5ncYBl4JhI1pZf7U7rjkTBuOHse5jRr46iY0i0+prjYrKuI4PSDmnHSnLFZdls287y9L4FfPWNW5gDjg1wcz8aCt7LTy72Fd4mLHLPSrRXdW9v6LBfGqYzKL+r52stfl+bSZqeXO4+Zsb1/QbGTzzs/9vJxdYaTIxYJG4u0wSR+9cw6fPbuN/Hzp9bibX00mYwfezlXqy2UnnRG5ZZ+q42tMFe6LXq6bQpev5ZvL8hKdzrXIDU+Miy3onv1nm7bn6UKuCARi5ZCzYeVk/ODKbpzGxl2zsETAAAPvLEtqxhWVVUonrPv1+n5EVP/Ta0w6Qy2C6nUVkXVcNMzmMSpN7+I829/BTc88h62tPXjY79f5unYBhLa81MZC2PmmGrTz1bvtj+HCwX7fF04uQEPfuEoAMGNXHK7lucSpCZOdGBZJKzozqjWSegsZHVYlO4S7em+9m/v4tfPrsdXH1hhextxw/zL975dNCnhVj3d4vfkTcty4QPhetHRn8Tm1j6cf9sr+MNLm0bwqIKHim6i5Iia7OXeC2Hxw/LSo6bi1o8vwomzx5huUxM3grgOm2q2dospy35hhewbW9qxtc052IXZzUZZWLFl2IKuMhr2VBhZwYo6VmiIRbeocHpV+v/61o6s77H7Fv8mlryabS/XvnbbUGE/T6YNpXvq6GpMY/by1n5bBYAdT74jw+zulxFEkJrc1uCn6B4rtATEwgqq9dfw9c3t+NUz6/nP5NnrjEGPGyDicSWtEuOFxaPV8bP3plikstrSfmRY4YruRDqDdMYYOWcX5mZHOM/08re3Wm+CAEAiVbhCTWwxcVKJ8oH1O7M2kL02I+b8MOTDkSEW3WfMb8aCifVIplWs2N5pvk/hvVwZDePuzx6OYw9owhJ9BKNT0S2mnsv5E+KM72JQElfv7kHPUApb2/pNSfnyhqIVrE2lMhrOusbt7Bzgn2XDBds0aqiM8tfZayuNG0nuWnIOUnMrupdv7cAn73wdG1p6TYXfdWfOAWBuZbAaS8cUwcmjqlCjT6MIqujWRt2Vfnr5a7pT44W1+2xH+4kbvEs3tOKp9/cMy7G5MZA1iSVtykkpRqdMEGySAg8vu+dNvLO9Ez/+1+oROqLCQEU3UXKI9i6v9nIAiAofllMaq3DuwglZY4fCIQX/+cbx+PfXjsN9nz+SBzx955x5PFU0F6Y1Gb9747/WON6WBag0eghGYx/0fkLUZOT543ZKt5ckc0BTTmSVhPXG18TD/PWbrD+f8uLV65gq0V7OFnfj6uKY3FgFRdEsdztsPqCMoju3wm3JzOzZ54DDnG6f1b3Ym1oVz72nWwwK7BtKm15r8THsRybZ97TKsNfVypaccCm6mWotLljdksMLGaQGaAvOXHv/2Z/I7OUf7OrGO1JR58QeB/W3kEq3aPHzGvroF3ZOsTYQp7/V832mvDsyxKI7HgmhQb92ipbGTEbl7wlF0Ta+Tpo9Fvd+7kjM1p1BvUP2mxK8RSYaRjhkuKy6B5LoFq53TlMdhgtxxJe46bXFxnIvwv5OtnH79VNnmX6+xWWDOWjYc1tfGTXyKwKzl2vPjX2Qmp4x4rKR8s2/voOlG1px1m9e5p9bZx88nk8qiYZD/DEGkmnT/amqyp1wk0YFby/X8k6Mr4thU8gv8rSHW59bjw92Zbsu5M2YfTajLocbK6Vb3DjYVqZFt5y9sFm4/hRqA3gkoKKbKDlMI8M82AkZotLtZPetikUwd3ydbr+ei5f/+yRcfux029t7YdKoKlxz2oEAgL09zotMtvvtJRiNPRd+Q9REZPuySemOelO6j5tlpMNb2eLEnktWwDFFunMgId3WY0+3EHrHFq91FVFURMMYryu8dr2arDjOVSydNKoKN/3XwVnft53TPUJKt6iydfQnTD3ej195LM5bqNlr7doe/IwMizgECSVTYtGd/aRHLOzY7N9+53Tng3jODSYzQk+3v/sR53SnMyrO+s3LOP+2VzyPiWrR1d8rTzoAo6tjiIVDvFWgUD3dQ6k01gi29kIsdETLNlO6W4Kwl/tQuqtNRXc4KwDryff2YO53n8Rf9TDLmljE1NPPxzQ5Kt3m9w17H7b1mRf2QVmf82G9TY9ou4cZ6mJ6OaCdr0v/5yQcNaMRwPCrckzprquM8A3G/mQ6EBt/Ut+UsbuWe7WXs8kRiVSGO0sapc9vtnn043+uxvzvPYU7X9Ystm19CQwmM1AUYHxD8PZyWdkuxaL7nW2dpq9vf2EjzvrNy+iS0uDlXv8gE+DzYVB6DQaTafQJBWm5Ft1D+vvmM0dP42tDht01qhShopsoOWI528tDlv92IhoOYXJj7gq3yOHTtIWIfPGXYUW3F/WaK90+Q9RE5KJ74igjtMOkdDsUXrdfcigev/JY/rWcwCkW0pMbqxBSgAWTGgAY/eD8tkIIkRNsw0Hc2WZj0Fhftx35Kt2AdQ999pxutlDLPUhNVrr9FN0iXQNJHDiuBtedOQd3fuowzBpXi7pK1uNtvWjz6joArBPIGUbyr2JZLDv3dFs/XiHs5ZFwSNjMSbmGudnB8waSGdOC6UG9kHNjr666TB1dhWe/eQKevvp4TNTDdAqRXq6qKr5y79sQaxMv+RN+ERfxUwNSuvsTKbyl2/E9jQyTlG45i+CXT6/DUCqDX/xHy6aQRyuykWNO6iJvy9CPh72f26VpB3bW1+FkQ4v1gtZuZJUI2zRgG7KRcAiTRlVhiv6ZKVrphwN2ztZXRvl1M51RLXuj/ZISrmFWeA1Smy643h5ZsRNAdisZ+0x+4t3dSKQz+NE/V2Pd3h7uRBlXW4F4JBx40Z31uV0Em0J+WbHdujXnkRXmtjf5vdcXUBtCvsgbiIPJjOlYy9Vezv7u6ngYJ84ea/rZur09Vr9SklDRTZQcuaSXA2aF3K/dNwiYGt2pLwz29QxZhgixMVReeroXTWlAbUUEJx441vW2bscFABMbKk1qaNzjBkdtRRTzJ9Zxy6/8Yc2V7mgId336cPzjimMxT5/r2tmfMCkRg0IIkRNskcOK7uqY0Vco9+rLfw+7wOdXdGdvdMiLFlZERn2PDBPSy2Wl28c5L9LZn4SiKPjSCTNx6rxxAIzZ2rb28pR3pdtJ6Uny5F9nlchyTvcw2ssBoFZIeGenpd/QNrYh09GfMC2In3zPW98gU3/H1lWgoSqGqaOrcwpq8sq/Vu3Bs2taEIuE+HWyEEq3uHicrG/utdnkCXjlh098wP/tqadbKKLj0RDfEPvWI6vwuT++iSmSysLeI/z3fSjdcUnp7pB6nIuh6Bbt5SJe+rEHJKWbwYrum59eh/tf32bZG/zHV7fguJue82Rj9woruusqoqhy6Y32izGnOz+lWwwrZJZm+bPEqp1n/d5ek7UcMFwbTkn6figLpVtv45Ezef7+trnoZkV2Ax8bWBxFN7O9s3XYQMLc093amwgsp6CYYBtj8UgYh00zv3ZUdBPECCKPQfKK2V4+/Kc+u7h39ieQSGVw+I+fwZE/eTarh8dQut2L7oMm1GPld0/D54+fkfNxHS30J8+XZq2KCqdbkJqiKPz22Uq39nUsHEJzfQUWTKrnC41kWjV9qHhWuqWiW7TFX7R4kqUiIc8Tz0cstbL0y4sWtlDzW7SJf3s+c7pFrKysdazotunfNca3+ZnTna3EJlzG7YhKN9uAcbOXF0LpBsSFbJIfg18rO7tGdfQnTMVZi0triXE77ZweW2v05LNFvVU6fL4wFehzx07H2QdrYwAL0dPNNnGiYQVN+t8mj93zy9tbO/m/m4Tny44awTkSC4dMwVjPrG4xWeyBbKWbfe2YXs7ndDOlW/u/PJN8pO3lXQPJrCC7CfVaa468QSCTyajYqLdLyNco0R32rUdW4edPrkV/ImVyJX3vsfexvX0AP//PWty7bCve8BDc5gbbKKqvjGquFf090xdA0W20Crmllzu/P63GmjVKG+yV0kYroKX8sw0KVnSzDaSgim7Z3VBqRXc6o2Lldi3d+8OLJpp+9t7ObrQJ5x/biGHX2CAT4POB2dzZOMO+RCrrOrG93XqcZCnD1n2xSAgnzR7LN+4AKroJYkSZ0GAkMvtRusUizG7xX0hYUZhRzWMf5OAPP0U34H+GsMyxQj+2PKLMbC93L7x40S3vmFsU0pXRMF8UiQtvo//b25zu1h7td+uEY2+qiePLJ87M/h3JAphPX7AnpdslfMcOUeWX53Tbpefa8V195vrPLlqQ9TOjgLBWNdnCxE96uVWQGlN/7DbJxPcmE7u5vXwY53QDgpI5lOZhbn4vF8yl0t6XMM3bHkxmXOdfm4MBjWsdO4eCsMrKsAJ7/sR61OnnRCGUbt6uEAnz65sXG7MTKrTX6FNLpuLAcbWutx9dHUdI0UYgRsKhrCJKXtDaFd2vbWozhf2IGGMPJaW7v7iKbitr+YwxNQDce7p/9uQaPLO6BQBQ4VB0A9oIvIvveA3H/ex5dEr3+/T7e/HtR9/j86rzgZ3HzJ3ANorlkKZc4HO6ba6/7PPKLUjN6nyXP+srLZxFe7sH8fzafQCAQ3UVt87DBpAfZOddqaWXr97djd6hFKpiYZymu7lEXhPmzzOle2ytdo3tG0rhsZW7fAVeFgLWttKkh6CK5wvL9bDauCl1DKU7hOp4BM9fcyIe+crRAMprNjkV3UTJwVI+AX893aItzG/hEgQV0TBfhL0p7OqLipKqqrzHOZ8+bT9UxSK48NCJCIcUfGrJNNPPTDPRPbgDKngfkvUYMLGQVhRFUP+NDxFDXfVnL6+TbKDfPG02/vCpw0xKKTsH8h0ZBmRvUGj3a/67cw5Sc1C6/drLLzt2OlZ850P46OFTsn7GNirs7eXm4sEJnl5uNTIsxTYfrO9HfI2YO4Dby4dxTjdgtg+rLrPC7WDBSMm0is/c/abpZ26BPfv08zkaVkwbO5ECKt1sAVoZCxvnRAEWdmwTJx4N87+tdyjlWqg4wa6XHz18sqfbj6qO4fZLFuN3ly4GYN+jy8i2lxtff+fR9yx/R55vz66dsmU7iGIwHzZYWMtn6PO23TZD7hDm51ZJm3IHjqs1nbsrt3fhvZ3dGEimsaWt36QqBrmJxBRftjHCRiQGkRIfxJzuwWSaf/aI9me56BanTBwwVtsEWbWzC29v0/qVT5vXDEB0KgXzXpVdD71FEi7mhUxGxbceWQUAWDJjtMkldMR0LU/nlQ1G0S0r3R/s7sZXH1iBL9+7fLgO2ZJe/bO4qcbYuAW0TebR1Yb6XW4wdxBb14VDCt9E3dcz5Oq8KRWo6CZKDhYoBORjLx9+pRsAGiq1C6lopRPTu/sTxogQr0p3ENx44QK8dt3JmD+x3vR9sdjyUnjZ2cvZ3yS/XlZF94A0hsaOrKK7MtuS96F543CS0N/NFkZsUZyPRdmqkM6a080Sb/3ayx16ur1sfsjY5QNwVdPCSpzJqPx18xakZm+vTLj0dIubYKyoZP3UwzmnGxAtm0mw9bNfR0RlLGw7Zq3XZcHE+rnH1MRNj8te90KMDGML0Kpo2FjIF6DHkTlgKmMh1FVE+aaXPMHAC+mMiufW7BX6Yr1fL8+Y34yjZ2oOH7cNMVnprhbs6Us3tFr+jhxAyHq7s3q6R1jptkoFnsmUbh+LXLn1qEZXqp64SgvXFO3Pyza14aDvPWV5P/n2XrPHYe9hdlxBJFPzOd22Pd3uThSmUIYU4KQ5Rg6LvMEuXm/ZiLpXN7ZBVYFDJjegWW8BqBXeq0EktLNQQ3btKqVCZ82eHry7owtVsTC+/+GDoCgK/njZEfjJBQtw8WHahpzoOmItB2N09Zglye/uGizY352x2JCWYecws5ez9U1lNMzP66DG4BUTYk83ozoe4a0U5WIxp6KbKDlEy6WfPhxxYT8S9nLAKDLf2CIq3UbByeyHsXDItegMkngkjLHC88qorYjipv86GDf918FZio/l/fCi2yZITXImGOFy4sYDU92yi2gRo+jW7eU2x8cWkYBRyDOVJmixVP67DXt57kFqcnr5OXrP7fj67NfLL+w17bGYOSxuIHgquh3mdCc99nQDhqKUdkkvt6qDrSyFfqkW7OVuFncn5D5Nhts1i1l+ZYtuxOMc4FxgSmBVLMI3rwphYRwUep1DIYW7RXKxmP/tre247J63+NcNHqY9WOEWqilf92aNNSzsM5qspyRkBakxpVu2lydGtmd2vX6u1QkbC0zp7hpI2rZCyI4eq2t1Q1UMs8bVZH3/Fj0V3oodHf1YmYe9lxfd+nuYKcYDyRSS6QxueXod/rVqd073za/lLunlTu/PLiFdfaE+vQPIvlaIn/1zm80tE6cf1Mz/zd6r6YwayAYO2/CbO157TD8bLyPNBj1fYN74OkwapV07TzhwDD5x5BQ06psaYjHNNnjG1GTnQGz0ONrRD3e/shmH/OA/eHdHp+Pt2AYRs5ezz+GqWJg7N8pT6bYWZmbrajcV3QQxQohvSjmYxglxwT8S9nLAsCSLPVjigrNTsJYXYg5xLlx82GS+U+wGU8PlBYBdn3aD/nyIY8NYAVDtsukQ1xey+7jSbb3ovuqUWfjUkqn46xeX4CsnHQDAWBgF/RzLSncy15FhQpErK90nzh6DR75yNJ782vE5HqVBfaW90i1uILglyQPO9kq3zQfRCcBs5e7p5ebv/+SCBfjNxxe5Hqcbor081zndgP1rLocedfQlcOO/VmNDSy++9uAKXPv3dwEAc8ebQw15z7wHtcQv/aK9PGDLqsigNGKKqdOvbmj1rdT9fbmRRhwLh2ydBW74VbpjkRAeu/IYANmbbIxB3k4T5r8DZCuHI51CvLtLU/7mNBvn2jRh3KLdxsuuTnPvr52CF4+EuU2Wf8/BMfXfD72LD9/2Cu57favzgdvAQguNolt7/r/32Pv44RMf4DfPrsdX7ns7p/tOZpxbhbzYy9nne31lFEdMb8Tc8XU4cnpj1rlbaVK6zdeB0w8aZ7od27DMJfgwkdI2Ilbt0DJm9vCiW3tMMYNAVVV8sKvbNZPCC//z93fx0Ttes33/5ALbrBQ32Rlsc1/cROA93RZig90YvXz4/uMfoHswhZ8/tdbxdixfZYwUClkVi6AqzpTu8iu6xZ5ukQP1TSfmRCh1qOgmSpo2KYTMCbHQHjF7uYUaIy5s2IfCcFrLg4Sl9cofpgmLnm7A+Dv3dg3y15JbXT3ay1kBXVdhrYzXxCP4wYfn44jpjThv4QRTr1dQSjfblc5SutlCzecDiUWu/DwoioJFU0ahPkdlT4SpeF0DSfxIGL0EGFa8aFjx1JPO/kYre7lbkFoopHDlmtmnMz7Ty485YLQnRd4Nlm7dl0jxY8jFym4XbiSPmvraX97BHS9twqm3vIh/vLOLf39eVtGtP78FULrFlg63Pv98EIPUAON6+L+Pf4B/+lQgxdekvir3TUq396bVdYUVRXbqomEv1853VnzLymGQRUcusM1OUWkdUxvnm8N2yfJsdBUj7bARJGawAM6hXyu2dQIAbnjEulfeiaFUmi/cmQ2XXTu3tw/gT68ZhXwuVuyUywYq+2xzSi8Xle5YJIR/ffVYPPiFo7LOXfE6OUdQug+Z3MCD7gDtsyCf4MN/v7cbv3l2Pc797VIkUhns6TIX3e19xjjP/3ywF2f95mV89p438yq8h1Jp/OWt7Xh9c3vOrgMrmDrNeuBFGnloo7mVDzBPiJDvqxA4rWtUVTXs5TVy0S0q3eVnL2fuGXmN8IXjZmDV/56G/z5jzkgcVuBQ0U2UJJ88SguF+uopszz/TqQIlG7W0y3SaWEvL9miO2qeg80YsunpbqrV/s7fPr8BS258Dnu7B/kHild7OcNO6ZYR2xOqXB7DjT9edgROnTsWPzp/PgDt73zq/T34r//3Kra29QlFd+5BaoUcbyda8u9cupn/u3cohXNuXaodi8ewwqhDz7FbTzdgpAKnJXu51zndfvIdnGBBWT2DKaRd+sqdsEuEl+3lL63bZ3k7WenmPfMBK93JdIYXClWxMC+4CjEyzAhS0/4W8Tp3+/Mbc77ffPbO3DZgra5DFS5FN08v5yPD2Oxz83M6kj3dqqryIkScXV4RNc4BO6V7R4fRG3vWgmY+Zs6KXNtg5KkeIi3dg7j7lc2m95i4mcXcQXbXrlxGbBn2cmel28lezp7vev28VxTFcrNIVMvH11fgypMOwMWHTcKfLj8i67Z13D3nv+gWnRePrdzFRxUye/lQKsOLU7Yh8vL6Vvz8P85qrROiSyKIMXGMjS32RTfLNOlLpDGYTCOVzvDXyaroDlrpFt9H4yyUdcZAMs2nd8jjD7sGkny9UizjzYKET7iRhZnqmKfWxlKBim6iJPnhh+dj5fdOw6Ipo9xvrGNKLx8hpVu2DAHmC3LnMCeXB43tyDCbnm6Wwgpohdn6vb080ddV6ZYKOLuebhnxQ1YM5cuFEw4cgzs/fTimjtZ6yHZ0DOCLf16Ot7Z24IE3truqI3aIHzxWKelBURENWRarW4RRSF4XqE5zpN16ugFhVnea2cvN35eRF6t+++btEGff5ppeDthvnHl9PuV+WC/21VwQk51N9vJCKN0sSE2/TogbCCwwxyutfUZRls8i1G1DzErVY/b4wWTG0lqdlV4uvcfYtSuIVO1c6Uuk+WZLjTSW0K3o3qW7YD551BTcfsliR4fJUTNGAzArtgDw0wsX4NS59hkM6/bY93Be9sc38f3HP8DNQn84e19VxwzL9a4u63nGuWQIJDPO13I2VcIpSI09boPLNV10QETCIVxz+mzc9F8LLT/jah3CMN0QW1X+vWo3d2LMHFPDz1n2ve3thrvhjhc35TxaS3RJPL+2Jaf7kElnVGzSP7Os7OV1FRF+TnT2J9EvPL9Wa7INASvd4uep0wYu2zgKKdr1UFyr7O4a5O/TkbxuFAo7e3m5Ud5/HVG2KIriuxgxzekeIaX7U0dPxXkLJ2DBxHpcfux0AEBXf7a9vKFklW6bIDUpWIhx8KR6kxKSUVX+gSIn4spkK93eVGvxQ3ZCnkU3Y9bYGhwoFUnpTCbnILVoOITfX7oYv/3EIttAriBQFAVPfV3rDRcLy1xCtNhi1KooTHpQutn7kyndGZeCt3BKN0s8TvFjycW5fPslh1oeu1ggWhWLHz9iCv502RFZhUzUIaguH5j6HA4piIVD/H3Un0gHXuDLqd4bBUXJj+qrqiov/ID87JZu9nKr117suZVzHACx6Gb2csnho/c5j6S9nKmcsUgI/7V4EgBgph6ixs4BuTh9f1cX3tvZxbNU2AgjJz5z9DS8ccMpWSPd6iqjjmPenAqL93Z2AwCeeNdox2C2dVG1X2tTuOdyfTOu5TZBasKmmJ19nc3BZvOW7bA6p+zIZ5NM3AB8dk2Lfn8RNFTFMFr/3GHuu2160c2us6t2dvl+PMDsktjbPRTI5t6Ojn4kUhnEIyFMtNi8UxRj/GJ7X4Knf4dDCmrikawN/B0dA4G+NzcLRbdTjgOb0V0djyAeCeO5a07ECQdq01emja7ioarlrXQPX4DwSEBFN7HfUAxK99jaCvzm44vw+FXHYv5EzT5qVrq1D7jGki+6pZFhNruYiqLg/z5zOP96KJUxjS9yIqvozkXp9qmu2REJh/DXLy7Bl06Yyb/XM5gy1JEclNLTDmrGOQdPCOT4nGCbV+mMylU7PwGFjKjDyDA2p9tp3FmYz/mW5nR7DFLLZZSaFcxergWp6ceWQ9V92LRGvPGtU7K+Ly50P9jdnfXz8xZOwPEHjsn6vqF0B2svZ4FCVdEwFEUxqZ5O/be5wIPU9Pf254+bzn/GChIvtPclsq4xuWLXvvH546Zj9rhay/eguCFitTiXF5DytYpt/I2kYsUK6saqGOZPrMfz15yIx/URX+yacPVfV+L6h7Vgvzc2t+Ps3yzFxXe8hp164TTKQ65EKKRgbG1F1iZ5fWUUJ88Zi1OE0VkidgnNYtEithTJyeUAcIUenCmTi9LNXUs2G/bs+qOq9mGHu/VzvLne+XPHT8HHle4c3qtWxdsU3bXFnDptfQlsaOnlRfaiyQ0AgD02LgI3RMXc6utcYHbwGWNqbDdpRwl93f2Cm05RFNMYQEB7DTft68u6j1wRi26nDUKmdNfq53BNPILbLjkUXzphJn536WLeNlHOSndQm+fFSnn/dQQhIO5Qj9TIMBE+LkcYl9XO7GcBhGSNBEzZERcNqqra9nQDWu/qEdMbAWj9cOL4Iidy7ek2K935j91iNFTFcN2Zc/DDDx8EQFMIclW6hxPxvcA2CdqFfspZFj1yVkSkolnES08325hISUq3E87A3gAAWC5JREFU3X6FvLgKquhmC7DeISG9PMeQLrHQYIfbK8xY/bMQ7sSIRawfiz2/97y6JZCFKoNvcul/dyQc4oVL0AnmsgJ86ZJpuPHCBQDAQ5y8IKdns4yPXLD7LLjh7Hl46hvH8xFyIswVAFgr9PImo7whyIIX851LnQ9MwWSfNdObqvk1VzzeB97YDgD4oR602J9I40195OUoHy4c+Tmoq4giHFJw12cOx3fOmZd1e/Y5MJhM472dXVw9Xi1sVInZITy5XHicr5w4E9/80IFZ953LXHgjvdzOXi5cR20cIuwcd+tzP2uB1iNvN5JOJJ9pA1YF4GR93BZzWO1o78ept7zIf84+q3NNkxaVbkALucsXI7nc/vliRffGfb14bVMbAKOFrcYiLPGRFTsCmX0OGBNWAOf3vDxnHtAK7+vOnIM5zXX8eHPJJCh2uBuSim6CKA9ERWOkgtRE6vVQNSulu3SD1LKV7mRaBfvssrMOxXk/XNo0vsiJXHu6m4RU0EkNVQ63zA0+e7w/mfPIsOFELISZisraHGaMqcZ9nzvS1/1or7d5scLt5Q4fqHJPt1t6uVwHhwKKoq8Verq52p7jfYvXHLZJxNSljr6EyR7LsNuYEL9/zd9W5nQ8VlhtcuWTiOyE3OscDim8wOgeTHkuQlk/98wx1bj7s4fj22dnF21eycWFApjHIybTGXziD8vw33/XXhcW0sTOd9lOzK5BI6lYMeXS6rPGqnVLtPOz4/bTBiVPWxAfY6LF5id7n3zzbytxzq1L8dhK7b3ywW7DMr67a4A/14bSbQ6hPGO+kRvCyEfptnt/ip9HzNkjw4ruZpei+6JDJ+HPlx+Bh758tOtx5TNtwFLpbjQX3XKo2HR9I8CPM0WE2dTZ205Ows+FDQ4hagyWk/Odf7zP0/EXTKwHYE4KZ9eDP7y8Gf9+b0/exwYArT3Z+ROqqmJza5+pXcjKrSFi9HTbF91PvrcHZ/zqJZz326X43B/fdJwsECQ7Owd4AnkukNJNEGWGaU53ERRB7ALaJyhfTH0oZB9vIeEjw4SLrzgmx24Xk31/KJkxjS9yQlagvPZ0i4WQ2+InF5hy1Nmf5B94uQRxDRemoltfwDJ7+TkHT7CcY2p9P9mztvn9sgWrw/MQ8Z1eXpjnlNnLuwaSrmq7Hw6fpilEbNG1p3sQVushuw1B8fldvrUj/wPS4ZtcgmWaLeTP++0rfHRcEAxIRTegFfjssb0u5Fk/8oSGSpw0e2xeo+LCOTok2KbgQCKNZZva8OrGNvz1rR1IpTNZwYFyYvF4vcgcKcXqX6t283nBVqGdVq6hHotj9WIv5/cpK93C9frUuePwnXPm4bR543DYVC0ctT+RRkdfAv98Vxsrdc+rWwCYFd2MCpzx65eQSmdsC5bRNc7hpYDW+/3h217BizaTBABjM9BukyYsjD0cSmcXH5mMys/vZpdraiik4LhZYzw5CdgmYS6tIOxaJI4nZCo82xQRVdoLFk3kn5m5KN3pjMr77E/RQ/SCcO04hagx5DVVY3UMP75Ac9mMF7JdxE2abQE5isR2LXYNfPzd3TjpFy/gB8KoTiu3hkiVxZpR5pq/rcSaPT14d0cXnlndgi1twdnk7Xh/VxeO+elz+Pjvl+X0+5mMyjf8qaebIMoEcTFbDHZfec40AHT0lYm9XLAGHv3TZ/nP7SzAXAUUEnXdiu7R0oeo3e6wzOHTRiGkaLvihdhV5b1jAwlulS6G882OcEjhmwKsWGjnQUneN3/EzQy5pzHpwWZv2NOZ0m0cnxVi0e0WTOSHKY1VCIcUdA0kuRUyn02TJ79+HH710UNw+kHaYu7hFTvRNZC0DXOytZcL16+gsggAwV4eEwth4/rzvX/4n5lshxykBmi5Duz12+Ox6GbnZxCOIKeNICcqhdBIUQXuEBwu7Honb+4dNEFT2HIZ8xQEP/33Gv5vK4eQXHQPpdKWo7D8PP/ypqg4BigSDuHyY6fj9586DIfoPcNvbe3Aoh8+zW8zo6lGPxbzcWza14c1e3qEott87FZJ4Z3S/PHL//gmVm7vxKf/7w3b42etN3bXMEVRHHMXWvuGkMqoCCnWidm5UpuPvVwv3j5zzDT+vQP1lHlWdLd0D+mPE8EtFy/kc9f3dA36tl9v2teLgWQaVbEwz63Y3pH/pl5Lj3bdcGoXO1SYdHPQhDq8et3JfDNsgvD+PP+Qibj4MC1YMKjAMnH8HbvP37+kjUgU58dbuTVE2JxuJ6Vbdgjm4urwy1/e1FpQ3tbHyvlFTPwnpZsgygRx4ZyrpTBIYtKIEVVVA11MjgTyyLANLb0mNc/OpssWp13CYsjNXi7OsayJRzzPs26oimH5tz+EJ/TQoKBhixVx8V0M55sTTJFj5yLbmffjuBD/Rrmn0bCXu48Mk5VuL+nlrIgJgspYGAdN0JSft/Te1XxU9TnNdTh/0USuSAFaIWu3SLZb1O/tMQrSMRbqXa5YTQsQg4XkHsx8GOBBaua/kamRHR4D/NpzOD/t8HrdkBFndW8UQpfa+oZ40RXj9nJjUd9YHeNqYj5BdfnYRsXT2Sowqk7qcWWbwTJ+NofH1poLIrv3NVPz5Pn1zHXCLKzi4rylZ9AIoZKOXfzMYZs7ciHi5RznSreDSy7uMKubWcvH1MYD3YSt5epnDkq3Xrw11cTwz68eixsvXIAl+og3tknClO5RVTEoisJV+v5E2tL94ARraThoQh2m6jb2IJTuTi5W2F8PxFnyh0xuMG38jReC7arjEX49CsqJItrL2SbngokN/Hst+mYjD7W0ybPhc7od2lLkMajyBlMhyDdwU8xmoJ5ugihDcl1oBUlM+IBWVRXr9mq7wBXRELcflhpGkJp2Ed0nfNg4wSxFHfpiKCIEFdnRJIyrkRdaboyqjuVlSXW7b0B7XdmHUTEr3UB28nguSrdVb7jxtZ8gNW/p5eKcblYkBwVTRba0sf7D/DdNFk81lJZ/rdqDrfp9yw4Nu+eorTfbopgPTKXqt2jnEO2QdknMudCu/w21FdZqZKdHtS7INhzRtn/crCZ89ZRZfIyeE7zoTqRNo6naehNcjWWvpZgjUVcRMeUG5MKKbR1Y8L9P4a6lm33/bs9g0mSbvXTJ1KzbyD3ddtdxr+4iQCuyT51rnVQuUm2z2cpec1bQfu7Y6ThptqaWtnQP8UkA8oYBAHz5xJmY01yLy47R0vK9nmcMVVX5+8ApD4b18FsFqe3VFWO51SBfqj1Yju1g519VLIKDJtTj40dM4ddV5oBgrz0rhipjYX5++Ak/BIxRb/Mn1mOyXnTv6BjIK7Asmc7w4t9JrKiKRfDdc+bhgLE1pikjgFkhr4lHjN7pHJ5TmYFE2lQk91k4i17dqAW7DVr8TMQ4LvN14+5XNuO3z60HkH1N6RgGpTvfwE3WjqEoxS9Q5EtxrwQJokAUQ3p5TCpSlm5oBQAcMX10yfa1yHO6d3kcK8JUC7awqtRHeTjRVGt8wBZTz3R1LMw/OJiFuBgyBJyQF4tcSazxXtSIPY3yLGmu/DmNDNMXs6m0MbqM3a8bQRfdYoGsHUP+9zlpVBW2/PRszJ9Yh0Q6gwff3AYAGC09x3ZF9xePn8H/nW/A2X//fSVOveVF9CdSGLBQV9iGAABsae2zVO5yYX2LVpzKgUcsZMvr/GTuCAqg6BbPr4aqGK7+0IGYrVtsnagUlO71e4Wiuy+RtckkPkZlLMI3HfoT6Zzmrr+6sQ39iTRe29jq+3dX7eyCqmqq72NXHoOzF4zPuo1sL2f22CbJYeF2jZb5yQULcMDYGtO5LFMlFfIskZqp0+IkDFbAPvHubjy3pgXhkIKzLUa8/c8Zc/Dk14/nhV5br/Umgp3KJm48Oa0d2PVt7Z4ek6UYANr18D8/G5leYEnXflVnwFDHrTZP2DnAnDBxwZ3C7PFeN9UZzAY+eVQVJjRUQFG0909rb+5qLDsvFMU6AFDksmOn45mrT+DnAcOsdIf5xk+vg43bK/J5wJRu8Zq6Rt+0G5BGKsrwOd2JNP+MHEym8f3HP8Av/rMOe7oG+Ws6g79vSkfpjkdCvq8ppcaIFt0vvfQSzj33XEyYMAGKouDRRx+1ve2XvvQlKIqCX/3qV6bvt7e345JLLkFdXR0aGhpw+eWXo7e31/pOCEKnGNLLRXtcMp3BK3rRfewBo0fqkPKGbRawont3p7edcLbYYYtut35u7TbGQiGooiAIFEXJsrkVw/nmBLeX664L9jr4bXPgPY2SOppIeVe60xmVW8sB5xnZR81oxPSmapw4211B84NcdAcZ2nbMzCYA4JZkuZCx25iYNa4Wz1ytKbDdA/ktcv761g5s3NeHZ1a3cIVMtJd/9PDJ/N+pjBpIGE9b7xBfXM8aZy66GyqNxH8v5OLEsMOU9eFj8449X0PJDFoFZ0Bb7xAvuq2KuFpBSQM0hfKJd3fhb29t9/zYrNjJRd18d4dm8T10yigcPKnBsuVHLl4+e8+bALReU7cQMCfG1lXgmatPwPVnzbW9jax0f0Ev0DslpTseCWOsXvyxDesPHzLBccOEFelMdQZgUlnl9yIjJTh3nFxyrH3mqgdW4LAfPWP6mdGyE1xrCGD0/+ZiL2dKrtVYPPkcEK9LrL1FLijdYA64ylgY8YhxLm3PI8GcnRdsBF0uiCPcKqJhwT0QXNHNzutEOoNEKmNK+t7Wrl1frYImRcTrBtsQEDc+BpNp3mbBUug7hqHoFjeBMzk4oxJSBkY5M6J/YV9fHxYuXIjbbrvN8XaPPPIIli1bhgkTsncwL7nkErz//vt4+umn8cQTT+Cll17CF77whUIdMlEmFIXSLSzI+hIpvK7PjjxaX5SXIuxDgVmc/CrdbNHtNqNbxm4u6kghp/oWu2WKFcOpjIqBZJpbu/3a9lnRkq10uxfdfGRYRuX9mwDgtF9x/+eOwrNXnxB4q8CEhkrTQiyocWSAMXKHIReOTn3vTH3qGUzmtLgBzK/NX9/cbsysFZ7Db5x6IG77xKGYOlpbuAWRYL5ur7YZPqWxKuv93cCVbn893YEEqeU41YKpUf2JlGnTr603YbnJ9MXjZyASUvDtc+YiFgnxgryjP4Er71+Ba//+Lu/tdIOlYDsFKtnx7o5OAMDBkxpsb1Ntc/2tqYhg8bRRlj8LCvHciIVD/DiZJVxUusdIGwBHzXDesGbv6b3dg/z9I9pvrZLcASNEDXC+ljtd31hrhexsyRcWHOfUqjCYTONjv3+NW5ABbbOB9RBbWfrlgD3RfdeUo9LNCk3WhsZmgufT181ePz9J+jJNNXHMGFONqaOrMLY2zovuXO3lG/f14s6XN2EoleZtQaK6PpAwBxNuae3Xv29sSlhRETVUeHa/Yrp871CKF+PsuR0Oe7modN/63Abf6jpXugvU8ldM+FtRBcyZZ56JM8880/E2O3fuxFVXXYWnnnoKZ599tulnq1evxpNPPok333wThx12GADg1ltvxVlnnYVf/OIXlkU6QQD+bXGFIBxSEFK0sSdvbu5AXyKNUVVR0/iOUoMtntmFXlS6T5s3zvb3suzlPi++VmmxI8niqaOwXphvWuz28hhP3TX60MMhxffroKlAafueboeC0lC6MxDWuI4qc5DFsMyhU0fxcUVBPkxW0S2pa04Ld7YQzqjaRp3cG+2FbmGBxBRCAJgjXHcqY2GcffB4PPDGNmxt6w/EorhOt2AfKKncgHnMnhfY9SWInm5TwKYPpYVt9MjWytbeIWFqgXHf1505B189ZRZf0NdWRDHUO2QK8eoaSHoa0deiFzu5zPleuV1TuhdOsg8fbK6vwCVHTsF9r28zfb86FsH/nnsQuvqT+PgRU3w/thfEEL+po6t4kdo1oI1gTOiFWzwS4ko349ApDY73PaY2jpCibey19g1hbG2FKXnerrWYKZ5uWSPyz1RV5WuNIMP/RJi9vNfB4vvIip1Ytqkdyza148qTZwHQVFW2b2eldMtp8yZ7uX7N2udb6daLbr2An9RYiTe25BfWyNYMfmbGy4RCCp76+vHIqCoi4RB/PnLNXPj475ehpWcI3QNJfj2fMaYaG/f1IplW0Z9MmRK7t7b1QVVV/vw4fe6Oromjr70fbb1DmN5Ubdr4aOkZ5OfwJH3CxXDYy8W2oF8+sw4f7O7CHZce5vn3eTgiKd0jSyaTwaWXXoprr70WBx10UNbPX3vtNTQ0NPCCGwBOPfVUhEIhvP7668N5qASRE6zYfH5tCwDg6AOaClpIFBrWY9nZn4Cqqlwh+90nF+P2Sw61/T22i97V791eLlJM9nIA+Ji0IC36IDVWdAvhbzXxiO/NqSgf+5VLT7ehdJvs5SP0fhCLEieLu19mSLNkm2rkVgT7x6qIhvk1w2v/s4zdIoyFUonwTTSb9Go/bHaYpcusrF6K7nRGDThIzTgn/Sz6mFon99eLY89EN5OiKKbihrlIRGut175cQ+n2V3S39yX4NXm+Q9ENAD++YAHmSFbt2ooIxtTGce/njjSlQQeJqHRPaazirQeqCrywtoUr3fFIyLRBUVsR4WPF7IiGQ7wfmYWAifOm7T5HWGHTVBN3/HyWlTqxsMplIoQXavTnK5E2W5ZFrGzSrKBUFOsiT7aXi60SLE9lX/cQnnp/j+epA/LIwCCUbnY9y0fpBrRzg61DuGU/x55utin2+Lu7+cbm0TOb+PPcN5Q2JXb3JbS+dreebsD4vGC29Rah6N7Tpf07ElL4mMIgrt1O/ObZ9VmbE0+9v9fXffCWkWhxr5OCoKj/wp/97GeIRCL46le/avnzPXv2YOxYcy9fJBJBY2Mj9uzZY3u/Q0ND6O7uNv1HECMBW+S9s70TAHD41MJa9woN++BLplX83ytbsK29H+GQgsVTRzmqSGxxyhadcpiOHawn7PDpxfW8LZxUb1pcFb29PGKMDGOzg/0kEzNYf2wyJfV0e7CXs5+lM6ppHFKQ/dR+mDraUKSDdMY01cT4mB/AbC+PhhXXx6rjc3lzWxBaFetNNbEsxR0w7NtB9AWyImeCNNIGMFQqL6nSPYNJrua4BSd5QXxv+nmfsoWx/HyK7h6n850X3ULB4WUjRVVVvtD2WxSw16CpJm45n1tGbtuwUkSDRtxwndxYZdq4uPyPb/HnNxYJmeYrX3zYZE8b1uwzgxXbolKYsGlTYrOq3eZrx6XXezBh3F+QOQQiojPArsdfVPCZrZ5Zp6uiYcvnTb7+i/ZypnQ/vGInvvjn5fjkXd5ELqbksgKe2dTzub4Y9vLgnlc+misHpVvMCNjZMYC3t3UAAI49oIm/fwYS6axzbUtbHw9ZcxqX2sRdBrq9XCy69c24mopI3tfuO17ciN88u97xNql0Br92uY0XhoSchnKnaIvu5cuX49e//jXuueeewK3AN954I+rr6/l/kydPdv8louQpAkd5FjH9IsMWQ16shcVMpaDE/f6ljQCAr50yy3WxEpMCh2ri3i6+D37hKFx+7HTccvEh/g+2gCiKYrI+FsOIOiciwsgwtmvtt58bMGz0SVnp9hCkxpXuHNLLC8EUoQcvyGNQFMUUJCYWu142GJjtM9cEc6vC7o+XHWF521G8XSSAoltfEFqNTGIjw7o8PA4buROLhLKuG7kgvjf9vE/Zwlh+PsVxa56KbsFa62X0TtdAkitDfntOOweYFdfbZoWsuA1H0V0tKd0ya/U2hXgkjLF1FfjOOfPwww8fhBscwtlEmoW+bsBI1AbM84JFmI1atrPLyOfjoKA8F8peHgmHuOvCrkhUYVxP2YhA9n60s2VHwiFT4W1Wus3Pw/u7uvnf5wR7PpgjoDZPGzcQjL1cpiaPMWxiD3UinUEyraKpJo6po6v4hlLvUIoXmoy93YP8tXGzlwNGAv8+4fzdq68ja+JG0d3el8A72zv5hocXBhJp3PjvNbjl6XXY0moformjY4BvkOezvk4IOQ3lTtH+hS+//DJaWlowZcoURCIRRCIRbN26Fd/85jcxbdo0AEBzczNaWlpMv5dKpdDe3o7m5mbb+77++uvR1dXF/9u+3XtqKEEESUwvUtiHTtC74MONoiho1C/2LCFWToK2Qk75tQvykZnWVI3vnDMv8NmnQSBuNBRDcJ8TVj3duRTd7H5StnO63Xu6U5mMyV4+UiYBMfgmV5uhHR+aZ3w+icFKXhYuhtIdTNH9jVMPxEETrK3GDVwtyd+iyDYWxYA643G8z+l2GnOUC6K6HfPxPmVqmGzfZK9LOKQ4btaw4/erdLdIyqyfEEl2bF4dArLilhyGNp4qYcN1ot6XevTM7IA09plx+bHTcemSaZ7bsth4qF2d3pVudhtXpVsuuoVCp42PDAs2vRwwwtRYMr2MaJtnvd/MXXewQ5uBOPNcLIjGWLhinnrf3l3KMOzl2n0ZvdPZBeG9y7Zi/veewjE/fQ7H3fQcvv/4+5b32dEXjL1chCvSQqioV3ZZhE421cS0tVG1UQgPpcyfie19CT6n20npHqN/Xjz1/l6c+PPn8dDbO/nPuNIdj/DNpZaeIZx/2yv41TPeFel2YfNTzKaRYS1Dc5prMV1whvnF2Gwo2pI0MIr2L7z00kvx7rvv4p133uH/TZgwAddeey2eeuopAMCSJUvQ2dmJ5cuX89977rnnkMlkcOSRR9redzweR11dnek/ghgJ5J29oJNNRwJZRfGSLJ1VdA+DolJoxAVa0Y8M0+3lyXSGL8pyCeliSrftnG6HnWxTerm+0AkpIxd6KBZ1O/MI+rHiHKEf1m5MkR0swTzXnm7598R59zIszTnfMJ5UOsMVRcuiW+/b7U+kbftSGWyDstqjG8aNiCm93Pv7lPVWysnuXsffsPfXDqGnu8vD5kZLtzm8yk9fN+uZb/BadEvX7lyC2/wibriy98bNFy/MCuCzm6ntBrsuG0qhUHTbbCqwc9e30q0Xmf2JFP93YwE+45kz7Ir73+bFtIgY9tc7lMTyrR346b/XAAAOmdxge7/ivHbR+mv1PGx0KM4Yg9JILD7txMK18+/3dqN3KIWdnQPY3j6Au1/ZYnmf7fqm16gABQuTZd/nhqtVKBzb5DLmmw/yc41tArUJPd1OayamdK/e3Y0tbf2mc5a5N1j2woWLJvKfPb/GLFA60S7MTV+z2779dpNedM8YU53XBJGgN1KLmRFdCfb29vKCGgA2b96Md955B9u2bcPo0aMxf/5803/RaBTNzc2YPXs2AGDu3Lk444wz8PnPfx5vvPEGXnnlFVx55ZX42Mc+RsnlRBYKik9tlD+kg57hORLIvVVeQtHkBWo5XHzNRXfxnXsiPEgtrXLbcl493Rk7pdthTnc4e073SFnLZYIYmSUyubEK3/zQgfjM0dNMwWJerlHjpZ5Uv8hhZVaqFYMp3a9saMMtT6/L6fEAoLU3gYyqvQ+sesdrKyJc5XcrPNkCzasbxo2oyV7u/XxjrUB254abu4W9v1qFBa6XjZS90lgxP2PDunwq3fL7L5cRZX6piIYwTbfiHjRBE0TG11fiY4ebwylztaLKm1aic8Buw8er0p1ddGv3xwqximjIcjxXvoib1A+/vSPr5+LEgp7BFG54ZBX/etEUeyeaqegW08tr4zhgrHkTxM4lIDIkBak52bitrP5W5x8LFPO7eelELBzin9l+WzislG72fmvi880TPIWfKdLtfV6D1Oz/zr2C0g0AN5xttFz42aQUle7Ve+yL7s2t2kbL9KZqR3XeDX5NL4N1nxsjWnS/9dZbWLRoERYtWgQAuPrqq7Fo0SJ897vf9Xwf9913H+bMmYNTTjkFZ511Fo499lj8/ve/L9QhE0SgiAu+kOJdgShm5FmnXopuObWyHC6+Y2sLM+e5EEQFe3k+Pd1RW6XbS0+3YU1Pc6V7ZJ83tkCeb2O/zoerTpmF/z3vIN8bC5MbNWVkW46Jv9lKt/0iTtxAcwvVcWJ3l7YQHVdXYfn3hkIKt922uMz+ZQv0oK4RYZO93PuSSN6sqJCuYW5F4bTR2f3Kfu3lgKY+J9MZT3PbmX2/zuPnjDyFwGm2d1AoioJ/f+14vPXtU03q2VTp+co1dKleKrpFpTsp5UkwWnjR7dzGZGUvV1UVL63bB0BrtSqEc0dMD7faUOkRlOS+oTTW7OnhXy+Y6GQvF5VucxL//Z8/EsfNauKfE3b98Ix0RuWFeYV+X3zcmUVPt9zzDBgtKiLMtj/GwbHjF3HSgN9+8w37shV/ueje1zPE/77xQtHN7OVOayYnNyRrA2LHPromjr984SjTz7wgnk+rd/fY3o7Zy6c31WRd//y0vfTuR0X3iP6FJ554oinpz40tW7Zkfa+xsRH3339/gEdFEMOHuDAbVRUr+uLMC3KgiZcd0FjYfBuvQWrFjJsqUkxY9XTX5BSkZtyPCLOXOymJxpxulaftjnTR/e+vHY/HVu7EpUumjehxiLBe81yLbj9Kt9wnKc4d9gNbLI+rs3+s8fUVaO0dwp6uQcx3KASCVkWiQuuHH0eK/P6ur4xiMGkUcG4F/EcPn4I/vLzZpJTnonT3DKZw1q9fRjik4J9fPc5xE4fdv9cgtYQwheDa02fjsmOme/q9fLH6zJCL7lyVbrHoVlXVVHQDmmJbETI/fs5KdyqD7z32Pv702lYAwDEHNOV0zG7sEopRq0Az2V4+tjaOlp4h3Hv5kY6fz/U29nJA21T+8+VH4s6XN+FH/1zt2hYi/jxL6U6kkMmopvWP1f3t6RrMGrnY2qP9vUEq3ezYugaSvhLMVVXFC7qNe3x9BXcjyfby1t6hbHt535ChdDsV3ZKNftbYGpy7cILJiSRuqrE+cq9hmJ39CTwkuCW2tfcjlc5Ytt4wa79Vu0Fnf9LzGojs5QRRhkwalT2qZqQRF2ZBp5qOFPJC3ckqxZAXK+Ww43lYCY1/Ywp1ImX0dHsZKWR3P0kpSM3LGLKI0NPNlO6RtpdPGV2FK0+eFchoqqBgic65zrbNUro92MsZufb0skRvp8diNsvd3c62edZjGZRNN9ee7sbqmOn8lDMQoi5FYWUsjMuONRexXopuuUjc3NqL9S29WLOnB6sd+i8Bw7rv9XwWN8+uOOmAvCyk+SIHkOXa083cZN0DSXQPpLJs0bLCOpRKc8uw2xpC3jy+9m8recENAMfMLEzRLWLlFBEnHfQMppDSr69jHTbBAGNSAmD/fLPvWynTIoOCEi4X3aqqhWlt2tfLr2usKBU3iPZYtFawQtWqbSUfmNrsp+hes6cHu7oGUREN4awFRm5Htr18iJ93TOlu6R7ir4tTf3S9tL767zPmZBW3x88aw//Net27BpJZDjQrrrj/bby8vpV/nc6oePSdXZabIMzuXx0PZ4Wndg14zwFhQXpBtQwVM1R0E/sNnzxqKj5z9DTc/dnDR/pQOGKxWT5Ft9zT7X4hLccgtQkNlXjy68fhletOHulDcUXs6e4Z0hZoudnLdYu4ZEv1Mi7H6Ok20svLwPgROKzo3tM96GsMDIOFoi2c3IAffPggx0KqTjoHcplbCxjFpNMsXbb43NPl3D8ftL1cLLr92MvDIcWkOlVEQyaLpVMrBeNsYWEOmHtv7ZCV7q1txubLm1vaHX/Xr9LtxyJaaGRLvNyS5BVWAHUOJPkoMPFaJ4epbW3rR0bVxlu5BanJxyQWwGctaHZMCs+HX3xkoeVjMsxKd4q/rm7ODnHj1c5ZwBRw96Jbe99Gw0aqf0U0xP+9p2sAJ9/8Io676Xmoqsrv797Lj8R/LZ4EIDvHgqncheiVZ+ebn9GMLMTu8GmNpsBIVigbQWpD3I7PNht3CI4XJ6GChU4yaisipmvhmNo4Tpk7Vri99tiq6m1T75UNbVnfu+ZvKy0zPdi1uCoW4RsGDD929r6AwzGLGSq6if2GWCSE/z3vIJw0e6z7jYcJcZFXDsnlgHlxFIuEPKmV2XO6S7/oBoA5zXWY2FB8DgsZpvBtbu1F90DuVi+2iBOV7mQ6w4uJRoeiKyz8bqZIlO5ipLE6hupYGKpqnZTrBgse+taZc/ApF9u81l97HP+6L0elmxX6skojMs5jQFzwI8OEXA2f55uoFMbCIdMGo5eiu7m+AhcICcNe5pSzoop9dmzzUXSzOd25KN0jTTikmAorPxskIuxv70+kuWugsTomKLbmc3yDnso9Y2yNa2uF0zH98MPzCzaJ4b8WT8IjXzkaANBi4RQRe7p7B1NclXSbqmFOL7cpuqPWz5vIA29sw7Or9wIAKgSbuqIYr6nYOzyYzPCiOxYJCRtyg/rP07jsnjfxPw+9C0BzQQT93NZzR4T3jUY2km9MTdzkEjKUbu17rb0J/nxN0O3lbLMnHFIcQxhjEfMGQ21FxNSOd/pB46RwyBDfPM1n9OMdL27CnS9vMmUeMKW7KhbOUtE7PMxtZ5C9nCCIYUHu6S4HxN1xL9ZyoDyV7lKCzSf+61s7sHSDZi3L5QOQfdiLahHrJQspzot9VpDv7BwouvTy4cLLulFRFEwapanduaSqt+lp2U4BaiJzx9fxXuxcle5OD7ZmtrCWlVyZQo4M83u2if3w0XDIdL3z2nN880cW4q1vn4qQovXmbrQIYmKoqsqfnyl6j/NWoc3gnW2djo9lpJd7+6xhPcjFshiuFDY14jmOKBILyW3tWhBUQ2WUv16y0s2K7gOkXmIr7F5zRclu1Qgappju6xnKCoOTlW7WvuOW1m/q6bZ5vvlmhU2Q2vq9Pbj+4VX4zj/et7wf1pYh9hwPJNMY0pXxeCTEN+SYzf+p9/fguTUteG2Tpsp6vZb5gRWqfkYzsmtTTUXE1GZXJ9nLE+kM2Es0rt587JXRsOsGgngu1VVETbbsYw8Yk3V7v33djFrpff+jf67GX9/aDgDIZFRu7bdSuuXsECf2pyA1KroJYgQRP6S9Wv6KHfGD2ktyOWCldJe/zaiYsFLlWMCLH9iH5oCgiHb0GdZiJyWRja55e2tH0aSXFyvjG3RV2GfRPZhMo0df4DT5GE+Ya5Ivo9ODrZn3dDso3aqqcjXJS9uKF6Iuap8T4ojHWCRkut7FPI4fC4UUNNXEcfIczYH1lze32962ezDFFUCWfi7ay3d1DZreewxVVbFsUxu343pVur90wkz8+IL5JrfDSCLa93NVusMhhdvJ2XPXUGUo3XKPNy+6x7oX3XZq8KiqWME3EJtq4lAULRODuVkAza0gZjH0DKWQ1Nt/3Ipusb3EvqdbO+ftRobJRauccs02dMT3fX/COM/jkTBmN9cCAFbu6ISqqti0r890H2MK4BKsz8FezkNI4xFLpbsiGs4SImri5gLdS26C2A5RWxExZVEsmTE66/bsWLyoz+Jr3msxou0Z3bEwmErzwFOrnm4/BT7L6SiWzb1CQkU3QYwgYrFTTGFN+SCGr3gN3inHILVSwir0afqYat/3w8NnhA9rNtJllEtmwaF60b2ptY/PLt7flG6vsA2RXT5nde8TrMni+9QNpqTkOqeZBXjJ/Ygi7G/a3TloO9Xkh0+sxsMrdgIIboEmnmN+93jEzcG4VHR7sZeLXHSo1rf64tp9trdhSl9jdYyrV2KBBQBb2vqyfu+3z23Ax36/DAPJNCqiIb7B4UZFNIxLjpzKE/NHGrFgcZuD7gT7rDWK7qjRmywptiy8y0sQq11hOhyf7dFwCDOatGv2T59cw7/fK+UEaKnt+u8EYS93UboHpe/LIWHMsSLOt+4dMsLe4pEQDp5Uj1gkhNbeBO55dQtW6L3TjKCTywGhpzsHpbu2ImoqpMXXX85KiYVDPKcD8OYOFDdLquMRLJhYjyOnN+Kzx0yzbOHxqnSrqspbiMbXV+C+y4/Mus0mfUwY6+dWFK1lQM5x6fTxvPUHnNNRzFDRTRAjiFhslkvRLf4d3u3l8gdx+V98iwm5QKivjOZU1PARMIIiypRut6DA+qooZulq0nP6bvr+pnR7/Wsn1PtXupdv7cBxNz0PQMuP8NMDyRbGLGXWL6yX2EnpHl9fAUXRrKVtForMtrZ+/N8rm4VjCkjpDudedIvHEA2HTJuMfkdaLZzcAADYuK/Xtj+W9fBPGlVpO12Azc4VeVYfY3TWgmY8cdWxJasoiQVbPj287DOKbVCMqooZ9vKscYdGb7EbdrfxMxo3H354/nwAwKMrdnK3UI9UdHcKhZcfe7ltkJpLT7fsjslSuvXzWCy6RWtyPBpCPBLm88S///gHfO454xD9vRMk7P3ly14ujNsUnzvR/i0W3eGQgkg4ZNrU8rJmCgvnfjQcQiwSwl++uATfO/cgy9uz1kWr66rIQDLNz5unrz4BRx/QxM8pxhb9+sI2YCujYYRCSlY+SKcPpTvolqFihopughhBYmWpdBt/h1elMqunez8YHVFMyFbYXBVmZvkVA7fa9Q9fpxA1xmJ9zNof9TE7TnOd92fG6+F8bqFjIjcJ6pff0EarzRQ/eOnproiG0az3blrNIP/LW9ukYwpmgZZP8SYW3bFISFJi/S2vxtdXoL4yilRGxfq91n3dOzq052XSqEp+DjDYNfQr972N59e28O+nMyrW7tFCqr552mwcMLbW13EVE143cd2Qle76yij/LJZ7uplt1oudXd485veRGZ6im10/M6pRAMr2aNZiALifo2al266n2zm9PKvojsj2aqZ0G9cysehmz/uJB2b3KjMKMf/csJd7v+ZxpTseQWN1DEdMa8SiKQ2m1PsaMRFe/9tEpZu1Djnh9/O5qVa73q/f28uLZivYBk1IMUYyzhtvvl5kVKCtd4i3LLDP/EuPmoqHvrwE1585B4Cx2e4FClIjCGJYEItNeSRKqVIjFMzyAsYOuegmW/Hw4rdAsIPtVIvFWbtuFXezlwPAocJs87qKCH58wYJAjqvcYEr3LpvxWqt2dOHkm1/Ak+/txv/8/V2cf9srpkLW7+KmOs+i2+uoqskOM8h3SkntQfV0iyg+o9TkNG3xmPz2HCuKgrn6Atdu3vb2dqZ0V2GitDhn7RkA8Le3jL7wLW193FY+bbT/lpFiItcxYTKsoGJF0qiqqK1iy8drebCz26nBcrBZoYhHwvyzlBXbctG9T2hHcB8Zlv+c7t5Buac7u6cZMIdCMpWUKcEA8PnjZ+DOTx2GH54/Hx8/YjI+c/Q0fvtCtD+w9hs/SjdLia+JR6AoCv7yxaPw8JePNmWZiM8pO1/E4z98WqPr4/hdH7HAx0dW7MSJv3gBLT3Wm7Xy8QPWjqJNrX2mGd2Alk2xeGoj3wz02tOdyRiW9v3B4Vj+fyFBFDHl2NMtfsB4HTmjKAp+euECXPfwqpIYsVVuyEX3hcIYIz8YxZkQpKZ/+I72UHQvForuM+Y3Y+74upyOo9zhSrfe/yyrtVf/9R1s2teHL937tuXv7+3OnuXrBHcw5GAvb+0d4gtytwTnKY1VeGNzu2XR3S6l4co9hEGQl708EoJ45fJrLwe0EYPLNrVjzZ4ey5+LSvfEBnOhcclRU3iasziyjxXws8fVlvxmplyw5Yq8+dNQFbNVuo2Z1l6UbuvbDJfSDWj9xEO9Q7zYZuplbTyCnqEU2gWLsdv5UB2LIKRo6qZsC2cYvfDW1wZ5zCBrNWFYrXvYaCvx+ayIhnHqvHHG/Q6l0NaXwLkHj8/6/SDIpae7h/d0a9cFKxdNrcVGxuRRhS26x9aZN+jW7O7B2NpsRZ2p+rUV1tZ4xsrtnZipt4LJ7hPWy+51s6JfOG/2B6W7/P9CgihiyrGnW8TPYuOjh09GQ1XMU2ANESyiivO1U2bhiyfMyOl+eJCa2NPd797Py2BBQABw5oLCLKbKATZei/U/y0FCVvZsEa9TBRjMAtrnM0itoy+Bw370DP+62uVxmc3S6vjFHsFoWMHCSQ2+jqUQiIvEWDhkUg5zCfpi1z4WeCcj9nRPEJTuymgYZy8Yj8GPZHDN31bycLUX1rbgr2/tAADMm1D6G1hB2ctlV1lDlTEy7Ev3vo2V3zuNfx6zz7BYJHel+7vnzsvncH1RVxlBa+8QL7bZ/yc3VuEDwUERDSuurRWhkIJ5E+qwtbUfzTbTLGIuSrfcUy7O4wasN+JYYW63iQFoG163fnyR/cHnSb493XbUxrP75KeONorugyfVuz7OkhlNWLap3fNxjZE+HzI2GQO9g+ZNA8D6s+JH/1zNi2tZnWb94629CSTTGVcXHVsrhEOK4+tdLlDRTRAjiHjxK8eiO+nRXg5ou8JnzG8u4NEQdohjhq446YCcVDrAKELElOtBfSfbS5K9oih46MtLsK29HyfNHpvTMewPVETDmDq6Clvb+rFmdw+OneVtUQVoipdf236uI8Nufnqt6Wu3Rb5T0c0Uuvs/fyRmj6vF6AIkFvsdk1cVl1Qr4c/LpWWDFSBWyb+ZjMqfl0mjqkzBhCwYb3qT9vy19g5he3s/PnP3m/w2heh7HW7s1Fa/yJ+1DUKQGqAFkX1aty+znm4vSrdVS8Gy60/xnBYfBEylZMUuU2onN1aaim6vaunfv3Q0BpNpWxWSFUqpjIpUOmMaXwVkt6SkpY34URZFdxdXukcuWCuXkWFGT7f9Wq7Wxl7+u08uRn1l1JOb40snzkB1PIwTPX5GjpWyUew2Etg5I4Y02lm+mRtBLsrZ89baO4Szf/MyHrvyWMe/iZ2foqW9nCn/bQWCKGIGytxak0gPn62OyB2xmMq14AYMG7J4f0wB8drjunhqIy5YNCnnYyhl/Cw65jZryqXc/zuYTJvsxQBw6JQG/u+3v/sh32m/uQSppdIZPPz2Tv71F493d0+wgLfO/uxFIZsxO76+MvCC+57PHo5vnz0XR0x3t3aKiGFusUgIVVFzmrlfmHpklfy7qbUXvUMpVES10VDiucIK8DE1WnG3r2cI2zvMGxfHzbIPoioVPnyI1vYybXR+Pbxy0T2qKspdBIC5zSDho6c7blFcDGfBDRg9wz2SvbyxOm5aY3idT18RDTu2hYh99lazutlnwRkHNWNiQyV+98lDTT8fZeGAYu6ooHr4c4G5IQaTGdtkdpF0RuXhYo5Kt1DQipsKZ8xvxpKZ2TO2rYhHwvjccTM8zY4HgDG15uul1fUVMM4ZKwu8HbL9XMxuWbe3Fw+9vcPx93fovfwT9pO2Qiq6CWIEGRQUxnLc5fPa002MLLIFMFcMpds4r1mPpNWClMgd1u8uF91Wc5rnTzQsi7kUg7n0dL+/qxv9iTTqK6PY9JOzcP1Zc11/hykislV1KJXmvaFeUvD9cuLssfjccf5bKuSRYdXS3G6/NPCiO3tRvGJbJwDg4IkNXE08TM9A+Owx0wAYKcWDyQz2CMn2J80eUxZOqmMOaMITVx2Lx646Nq/7kZ+LxuoYNrQYifHr9vagRZ/PndI/w7y8b+SNxSafUwKCgKmUTEFkhVRdRcTU4uNlE8EL4t9sNaubFd3HHdiEV647GWfMN7cNWRX0Vj3dw01tXOtnBwzl3Qlxo9lJQKmxULoLTa10PHYhZz0W9nhxXWp1vLLSLbcQ3f+6eeqEDMvvmLyftBVS0U0QI8iATfhIqXOkrhj91+L9U7EsNYJakFdZpJezAmp/6NfKFz/LYJZ0/YFUdLMxSIyDJ9XjSyfMRG1FBB8/YkpOx8WKyWdW78XTH+z19DtvbtF6Dg+bOsoUrugEGyc0KF0XWREaDikmFWakEVWeWCSEo2YYSlVPDknvrACxWhSv2N4JADhEcC384VOH4c+XH4HzdQW4Khbhi+CN+7QisjIaxs0XH+L7WIqV+RPrbWeUe6Wh0ij0qmNh1FZEcd7CCfx79y7bhiN+8iwAw17uqegWrnFfP3UWnvr68XkdZy7UcqXb3NNdWxExWbllG3iuRIQsA6u+bt7nbFOIjqrOfi1ZkTtcRakVoZDCszK8BE+yojseCTket3j9GjNMmzKyoGO1qdc7lMLtL2wAANtr7NjaOP711eNM2QpV0uhGRVFMrS9iKj1jX88Qbnl6HXZ1DmCb/nk1pQAJ9MVI8Xx6EcR+SH+iPIvu3196GF7Z2IqT51BfbinwueOmY8O+3ryTYLkNOZHmqdrMmkdFtz3zxtfhg93dOHOB90yD2c1a0b2ptQ+ZjMoL2zZ9RNvY2jgOn9aIb58zF+PrK/H2dz6U82g4USn/96rd+JCQImzHW1s6AACH+7Bss55duehm/dwNlVHPBfxwICrdYUUxPU8DOVzbGyqNfly5P5bN2l4gPMao6liWbXxMbRxb2/q5cvuheeNMi2DCvMk4Trd///DD8/Hezi5sEuYYp9IZJDMsvdyDvVy4xp1w4JiC5A64wYvuIfOc7rrKqEnpjgb4PopHQkgl0pYjQln4om3Rbal0syC1kXVHjW+oREvPEHZ2DmD51nY8vGInbvqvgzGnOTuU0MqabYU4MuygCe6haUERi4T462PVvvLUe3u4w+DgiQ2W96EoWiDj0TNH49k1LQCs080f+vLRWLe3B1/883J09iezAtWue+hdPLumBf95fw8vtqfk2TJSKtAqiCBGkMuOnQ5A63cqJ+qrojhrwfjARrwQhaW2IorbPnFolvXPL0xlS2dUrnowy+FIL6CKmT9dfgR+dtECfO/cgzz/zsSGSkRCChKpDPZ0G1bi9j5NlTlx9hjcdsmhPBwsn1nsM8fU4NrTZwPwvlG4vkUrEuf7WFiy68WgZFNli3Avs96HE9FOzoqzh768BMfNasJXT5nl+/7EYlAOO2IL5dEu6hhT59brRXc52MqDxlR066OT6qui+MSRZifIgJCP4OX9IxbdI6XSsp7hbW39ePCNbdiltxkUSukGjNYhq95nN6XbaqpFZxHYywFggr4h8+dlW/C/j3+Ad3d04en3rZ0+TJ2vdXFhiD8XN9AKzX++fjx3IFoFNbLPkCOmNeLiwydb3oeie7HEueJVFkX39KZqfGjuOG7P7+gzF/msYF+zp4eHQxZi1noxQkU3QYwgh04Zhbe+fSpuv+RQ9xsTRJEjfgAzizkPUiOl25ammjg+evgU26RYKyLhEF+oiH3cbfoCp7E6WJWNjSnzMjYslc7wxdT0MdUutzZgRfdAUnNKMDr6tEViIfq580HcSEqmtONdPLURf778SExv8v53MyLhEFfK5IWxVbKwFQeM0cKVNu3TzgkqurMRnxPRBSA/VwOJNE/b9jICTrzGjdRMdKakPvn+Hlz38Cqs1NsSauNRU2hZUD3dgFEcW9rLh5zHaFltxrJAtpEuutmG5Ssb2vj3+m1aAlnROrbW+borOiYWeBgPFhTTmqp5bkWHhb2cuYkWTW2wvQ926IuEFpeZY62vc6GQwj+DWnvNRbf4urKebrKXEwQxLMgzdgmiVAmHFFRGwxhIptGfSGM0hCA1KroDZ+roKmxu7cPWtn4cPVP7Hls8jQ5YFTbC1NyL7p2dA0imVcQjIYyv857eLI6EGkpleBHe0qMtaK36P4uFRDqYVqFRVTH0DKayLKBei+4rTjoAj7yzk7/vqOjORrQAi24s+bkSx0V5UYaLwc1jp7TWVkRMha/X9HIvGEW3hdI95Kx0O9/vyD6fExqyr112bSO7Ogf133EOBJsxxkgcH+fj2hgEzFXQZWEvb+vVHFJNDpu1rDf8vIUTMGtsLRQFmKO3OVkxujqG1t4htPWZe+IromG+QdOnh23uL0U3rYIIgiCIwGCW216udGuLlKBm7BIG00ZrKoOodLdzpTvYotsqmd4O1hc7vanaVw+2WACJSchMrZs3fviUIb/IY9pyxSrBPJnO8NBNt57RKaOreKo5QEW3FeI5KV6X5Oeqa8DYYPKrdKsjNC3T7vyor4ryoEIgWCWeFcdyenkileHXCzfbtRUj7Y6yKqDtNh13d2mBYeNdRsSNqY3j2W+egDdvODX/A/QJczpYKd3MIWXVvnLUDM2WfonefqEoCuZNqMPc8XWOU3fYfbVL9vKMNKv9jIOa82p/KiX2j7+SIAiCGBaYRZopc0Z6+cirQOXGVD18ZosQ/sSC1Nx6f/3Ck+k92Ms37zOKbj9EwyFeDAwKqtnybVoo22KhmCw2ghqPyBLMH3xzO/68bCsA80g/pxnADPF5r6Oi2xExible6i/uEZVuD8pwOKRgyYzROHBcDWZ5nKEcNHYztZtq4qZNLS+bCF5h87Rle/k+XT2NhhXLedyMK06aiZp4BN8+2zxWcKTdUVYFtN2mI1O6x3uYNz1zTE3W7OzhYGxdBSIhBV0DSfx9uXl+div/3Mg+rjs/fTju//yR+Owx0309Hrsv0V7en0hlTXY475AJ2F+gopsgCIIIjAl6H9yODq1Xi0aGFY4D9IW9ODaMWflGB9zTzVJq+z3M6t7UqoV45dLXzIogZuNs6R7E9vYBhBRg4eTiU7ovXDQRigJ85uhpgdxfo16cPP3BXnzn0fewra2fF39VsbAnRUh83knptuaiQyehMhrGZ481Colse7k/pRsA7v/8kfj3144PNKjMD1aFYkjR2hZEVT/QIDUbe3kL73OucFRErz19DlZ+7zRT+j8wcn3xDEul22bTkSndE1yU7pGkriKKK08+AABw58ubTD9r558b2Zs2NfEIjp7Z5Pv1YPfFrOsAsKdr0HSbYw4YjaNnjsb+Aq2CCIIgiMBgwVmbW/uQSmd4ENFIWwXLkUMmN0BRgO3tA2jpHoSqqoa9PGilO2ZuG3Bic2tuSjcgjA3TF/DLt2oq9+zmupwsqoXm5osX4oPvn4Gpo/3/rVbI99M1kDTNWvbCjDFUdLvxi48cjLe/8yFMFAqrrKJbD7OLhBTHolFEUZQRLRabLYq+0TVxhEMKTxkHvI1A8wq3l0tKN5tv7UXVDYcULJrSgNOEcYQrd3QFdoy50FQTz3qe2KbjYDKN+1/fhj+8tAmfufsNvL9L2/hk4WvFypn6hJKWHqMQVlW1IA4po+g2lG52TtTEI7jl4oW469OHe35vlQO0CiIIgiACY4ZeaG1q7TMtwsheHjy1FVHMHqcF2by9rQM9QyneWxx0kBprGxhKZZBysVIze/kMH8nlDHaeDCYzWLe3B7e/sBEAsNghVXckURQFlbHgzm35OeseTPLiz+umw/Qmw9pcV0l5uVZYvW6V0ohLFqQWZNJ3oYmGQ1mqPAtrrTTZy4Nb/rNzbF+POTBrnx6AOK7Om+smHgnj9586jG+EzJ+QPQ97OAmHlKywM6Z03/PqFnzrkVX48b9W44W1+/jPrcLXigkWRtnZn+Ab4t0DKaT0fweZBWLYy43zgoVizp9YhwsPnbTfjZWlopsgCIIIDKZubt7XxxOUAVK6CwXrc16+tQMtuopQFQsHvpgRZ1Lbjc0BNFs4mw0sFn9e4Up3Mo3L7nkTq3Zqalcx93MHiewO6OxPcptznUele9IoQ22j6RjeURQFlx41lX/drQeplVrIU4W0wVmtby6I14QgNxIOGKtt/K3f22v6PlM1/aZ0P/Tlo/H1U2fh2jNmB3OAeSAX0aynmzlwRGaNrSl6Zwmb1Z5RNRcNYPTe11ZEAt0cb67Xrj1snBpguEcaKotr/ONwUVpXEoIgCKKoYSNRNrf2cYtwNDyylstyRiy61+zRLI4HFCDEKRYOcaulU183S1JvqIrmpJow9XEgkcaOjgH+/cVTGn3fVykyTS66BxK8p9ur0h0Nh7Ds+lPwynUn73dKUr788Pz5OHuBZsFlSnepFd1x6TVn115TT3eAI8MOHKddb9bu7TF9n6mabrOrZZrrK/D1Uw/E2NqRV41luzhLL7cKyvv8cTOK3iodDYf45h1rRfrNs+sBGMGcQdFcpz13Yh8330DcTx04++dfTRAEQRQEprINJNM80ZWs5YWDFd3v7ezGim2dAICDCmDLVBQFVbEwugdTjn3d+fRzA4ZKJ46ZOWRyAyY3FnevZFDIc7g7+5PcFuy1pxuw7u0lvME2flgvfZD9z8OBPJ6RqdqFSi9nLS7r9/ZAVVVeeDKle+wwz6MOkvE2SndaGnt11ckH4MJDJw7bceVDY3UM3YMp7OocwO6uATy2chfCIQX/e+5BgT4Ouwa19SUwlErjiZW7cdfSzQByGyFXDpTW9h1BEARR1ETDIR661aEXTmQtLxxTGqvQVBNDIp3BX97cDgCYN6EwKd/VfFa3fdG9rb2fH1cusMKAWR7DIQWPfOXooleQguTiwybxf4tBajT+a3hgmxzMCltqSvdFh04yfR3S3zui7TzI9PJpTdWIhhX0Se4UFtblV+kuJtg0DmbR70ukoKoqBoUWm//7zGH45mmzRyyx3i+jdAfSVx9cgUvvegMAcOiUBhw2LVg30aiqKP/s37SvD9/820q+mSpvLu4vlMYZQhAEQZQMVfp4qfZ+7QOWxoUVDkVRsGiKpnYzBboQSjdgJJj3OdjLd3fqo3M8zKu1gql0rD+9riKyXxXcAPCziw7GV/XRPp39CW5z9qN0E7nDznPDXl5a598VJx2AX350If/6+FljAMj28uD+pmg4xFP32aYboJ27QPDjC4cT5tiZqdvJVVWbOz2gF93XnHYgTp4zzvb3ixEWstnZb8yhXzKzKfDHURSFj7B7Z3un6Wf7q72cVkIEQRBEoLDQLaZ0U9FdWOaON4rskALMbS5M0V1jo3SL83lZiFqu82pZPyrrB90fbYiKonBLbntfgi+O91d1aLipkJTuUlEwGbFICBcsmoTnrzkRPzp/Pj5zzDQAUpBawJZ5Vlzt6jSUbnbeNlSV7nl77AFN+MkFC/CLjxibGIf/+Bm06mOw2AZzKcHC1ESOmxV80Q0Azfp17B299YmxP17XAerpJgiCIAKmWl+ItPGim3q6CwnrqQS0ILsgR1iJsAVmX8Iosh98Yxu+84/38PtLD8NJc8Zid5e26M51Xi2z9rLxQ/urussKlWdWt/DviankROFgSvdGffRdqfV0M6Y3VZuyFf5/e/ceHGV973H8s5tNNjdyJ1kCCQSrXIRSJEIjeGlNActoLbZWTtSMMmWscAR0FG2LeqZjQZx65mApSGdO9UxtaTkjtmSUmZRbDhUxJKByMWClgEJEGnLhnmR/54+wj7uboAF2s7vPvl8zmSHP82Pzeybf7D7f5/u7uP0r3SGu3vuGYR+98NDtbHunVQ3OjOGk2+l06N8mFHc7Xn+oa/XyWFyoMHiBy/+441pdH+Kh5T6ei1W64/R9PbYe3wEAop6v0r2y5mNJgTd7CD3f6sFSYNU71Hy/11N+C6k9+foHau80evDVWknS0QuL5wUvQNRbviGw/tvYxKOettQpusx58rg0wQ+t7LImhX+CaMyXNLwMvr9330M33yiBBKdD/dz2+xv2VfFTkmIvNvyT7ty0JFXeMCRsP6sou+s9K3hl+3hdnyL2ogUAENWCh9wlxdjwzFjjv81UdhirSlalu4fVy43pqm75RjcUXmal27fY0+fWnO74vDnrab/fy12cDpcmJah6GauV7mD+19XhDW3W7ft7//TCQ7fmM74pEfZekyE4VmKBb1tP6fLX3uit6wZn9Xg8Xh+mcicEAAgpX0XUJ9RDGREoMcFp3cTcOiJ8i/r4foZvr9Vgvv1YkxOdlz2P01eNa7uQ2Mfr3L+etvzKvYx9z3HpgivdsTan+2L8V2Hv6PSG9LWtSveFOd1fzOe2d8wG74keCyZ+Ldf695ftRBEK4wbnqKdnLvH6MNUe7yQAgKgRXOn2XyUV4fHmIzfq1QfH6+Zr+oftZ/iqr76ho8F8iygVZqZcdnUrOOGJ14pI/35urX6oLOCYnSuG0eSE3x7xkj1H6oS60j0gaE63b+XynkZsxKoflRZ1OxaLlW7/z+dwPxTJTEkMWHPEh+HlAACEQHrQHD7fjRjCpygnNawJt/TFjdLFku7DJ05bfblcwRXyeL05k6Trh+RouKf7DSvC68arA/+O7DhSpzPUw8uzkuVwdG1buO+zNmt4eSyvXB5s0fTRml9+TcCxWEy6JWnNwzfom0NztGj66LD/rFk3DQ3Yrk7qfo8QL0i6AQAhlRpUrWy5SJKG2GJVus/2/Ps8cLwr6b6SucfBSWa8rnLr858/+oaGe/rp5fvGRborcWNIXlrAKAOX0363yu2doU26U5Ncmjyya2rLik3/UItveLmNHpo5nQ6V9E8LOBaLq5dL0tjibK2aVaZreqhCh9r06wZp+8+/o8enDLOOJdhknYRLZb93EgBARKXF6VNsu/PNw/M9RDFBSyC/d2FbmCtJuocF7TEer8PLfUYMyNC6eTdpyrWeSHclrlzlt9iUHfODTm9o53RL0sxJQyVJm/Z9ruYzXcPL7TanOy898HpitdLd19LdLk0d1fUeZpfdAC5HfH+aAQBCLrjSvXTG2Aj1BKH0xZzursV3/PfrlqS6C3vXXsnw8nS3SzlpSWo65ZsTaq+bdsQG/4c95zpCn6BGWqjndEvS0AtV4KZT53W8zX5zuiUpv5874PvkGNwyLFKu6p+udfNuVF66+6sb2xTRAgAIqTS/hVpWP1SmO8YURrA3CJWMlK7fq6/S3RY0zPz8heTkSre2urbwi2r3DX4r7QJ9xX+l7zPtnV/SMjaFek63JGWnJlnDhj8+flKS/ZLugozAXQWodF+a4Z6MuE66qXQDAELKf3h5OPeNRt8KntPtq3gHK8q5sr1f55Vfo5TEBD06+Zq43VoG0eOsDZPucFS6E5wO5aQl6fO2c2pobJMk5dhsm7vgLQxjdU43IoOkGwAQUil+Q+4YHmwfvgT49PlOtXd6rUr3oOwU/XzaSP1v3Sca7ul3xXtrjxucrZX3l15xf4FQOHPePkm3wyEZI10/JDssr5+X7tbnbefUerbrgZzdkm6pa1738ZNdw+cTbbidHMKHpBsAEFKdflMg7Ta8MJ75z3NtPdOutgs31tmpSZo6ymMtlAPYiZ2Gl2987Bat//CYKiYUh+X1+/dza+/RL763Y9Kdm+a2km7gUvCIBgAQUl6/Va3jeaVSu3ElOK39VVvPduj/9h+XxArjsDc7DS8fkpemmZNKwjYsOnh179x0+yXdef3sd03oG3xSAgBC6uZr+utr+ekaMygr0l1BiGWmJOrkuQ69/Y/j+u+/H5BE0g17s9Pw8nDrH7S6tx0r3fG8EBiuDJ+UAICQSk5MUPX8m+Rw2HCD2zjnS7D//tFx69g3h7LCOOyrODct0l2IGf39EtJ0t0tul/0WGhvm6RfpLiBGkXQDAEKOhNue+vdz68PGNu081CxJmjG+WA9MLIlsp4AwqPr3SVq28SM9MXV4pLsSM/y31LJjlVuSHpxYoobGNn17eH6ku4IYQ9INAAB6xTd89EjLWUnS4Nwr25MbiFajBmZq+b3jIt2NmPKNoizr36lJ9qtyS10juf7rnrGR7gZiECvcAACAXvGvZEld24UBgBT4fvDJiTMR7AkQfUi6AQBAr+QHLZQ0KJtKN4Au/tOK/HexAEDSDQAAeolKN4Av8z8PjldeepJevPsbke4KEFWY0w0AAHrFv9Kd7nYp16aLJQG4PDdd01/bf/6dSHcDiDpUugEAQK/4V7onfi2XVeoBAOgFkm4AANAr/f0q3eNL2J8bAIDeIOkGAAC9kpyYoNEDM5XudumOMYWR7g4AADGBOd0AAKDXVs36ps51eJXDfG4AAHqFpBsAAPRamtulNPdXtwMAAF0YXg4AAAAAQJiQdAMAAAAAECYk3QAAAAAAhElEk+6amhrdfvvtKiwslMPh0BtvvGGda29v14IFCzR69GilpaWpsLBQ999/v44cORLwGk1NTaqoqFBGRoaysrI0c+ZMnTx5so+vBAAAAACA7iKadJ86dUpjxozRsmXLup07ffq06uvrtXDhQtXX1+v1119XQ0OD7rjjjoB2FRUV2r17t6qrq1VVVaWamhrNmjWrry4BAAAAAICLchhjTKQ7IUkOh0Nr1qzRnXfeedE2tbW1Gj9+vA4ePKji4mLt3btXI0eOVG1trUpLSyVJ69at03e/+1198sknKizs3R6ira2tyszMVEtLizIyMkJxOQAAAAAAG+ttHhlTc7pbWlrkcDiUlZUlSdq6dauysrKshFuSysvL5XQ6tW3btou+zrlz59Ta2hrwBQAAAABAqMVM0n327FktWLBAM2bMsJ4iNDY2Kj8/P6Cdy+VSTk6OGhsbL/paixYtUmZmpvVVVFQU1r4DAAAAAOJTTCTd7e3tuvvuu2WM0fLly6/49Z566im1tLRYX4cPHw5BLwEAAAAACOSKdAe+ii/hPnjwoDZs2BAwVt7j8ejYsWMB7Ts6OtTU1CSPx3PR13S73XK73WHrMwAAAAAAUpRXun0J9/79+/W3v/1Nubm5AefLysrU3Nysuro669iGDRvk9Xo1YcKEvu4uAAAAAAABIlrpPnnypD766CPr+wMHDmjnzp3KycnRgAED9IMf/ED19fWqqqpSZ2enNU87JydHSUlJGjFihKZOnaof//jHWrFihdrb2zVnzhzdc889vV65HAAAAACAcInolmGbNm3St771rW7HKysr9eyzz6qkpKTH/7dx40bdcsstkqSmpibNmTNHa9euldPp1F133aWlS5cqPT291/1gyzAAAAAAwKXobR4ZNft0RxJJNwAAAADgUthyn24AAAAAAGIJSTcAAAAAAGFC0g0AAAAAQJiQdAMAAAAAECYk3QAAAAAAhAlJNwAAAAAAYULSDQAAAABAmLgi3YFo4NuqvLW1NcI9AQAAAADEAl/+6MsnL4akW1JbW5skqaioKMI9AQAAAADEkra2NmVmZl70vMN8VVoeB7xer44cOaJ+/frJ4XBEujs9am1tVVFRkQ4fPqyMjIxIdwcIQHwimhGfiGbEJ6IZ8YloFg3xaYxRW1ubCgsL5XRefOY2lW5JTqdTgwYNinQ3eiUjI4M3PUQt4hPRjPhENCM+Ec2IT0SzSMfnl1W4fVhIDQAAAACAMCHpBgAAAAAgTEi6Y4Tb7dYzzzwjt9sd6a4A3RCfiGbEJ6IZ8YloRnwimsVSfLKQGgAAAAAAYUKlGwAAAACAMCHpBgAAAAAgTEi6AQAAAAAIE5JuAAAAAADChKQ7RixbtkxDhgxRcnKyJkyYoHfffTfSXYLNLVq0SNdff7369eun/Px83XnnnWpoaAhoc/bsWc2ePVu5ublKT0/XXXfdpc8++yygzaFDhzRt2jSlpqYqPz9fjz/+uDo6OvryUhAHFi9eLIfDoXnz5lnHiE9E0qeffqp7771Xubm5SklJ0ejRo7V9+3brvDFGTz/9tAYMGKCUlBSVl5dr//79Aa/R1NSkiooKZWRkKCsrSzNnztTJkyf7+lJgM52dnVq4cKFKSkqUkpKiq666Sr/4xS/kv7Yy8Ym+UlNTo9tvv12FhYVyOBx64403As6HKhbff/993XjjjUpOTlZRUZGWLFkS7ksLQNIdA/70pz/p0Ucf1TPPPKP6+nqNGTNGU6ZM0bFjxyLdNdjY5s2bNXv2bL3zzjuqrq5We3u7Jk+erFOnTllt5s+fr7Vr12r16tXavHmzjhw5ounTp1vnOzs7NW3aNJ0/f15vv/22Xn31Vb3yyit6+umnI3FJsKna2lq9/PLL+vrXvx5wnPhEpJw4cUITJ05UYmKi3nrrLe3Zs0e/+tWvlJ2dbbVZsmSJli5dqhUrVmjbtm1KS0vTlClTdPbsWatNRUWFdu/ererqalVVVammpkazZs2KxCXBRp5//nktX75cv/71r7V37149//zzWrJkiV566SWrDfGJvnLq1CmNGTNGy5Yt6/F8KGKxtbVVkydP1uDBg1VXV6cXXnhBzz77rFauXBn267MYRL3x48eb2bNnW993dnaawsJCs2jRogj2CvHm2LFjRpLZvHmzMcaY5uZmk5iYaFavXm212bt3r5Fktm7daowx5s033zROp9M0NjZabZYvX24yMjLMuXPn+vYCYEttbW3m6quvNtXV1ebmm282c+fONcYQn4isBQsWmEmTJl30vNfrNR6Px7zwwgvWsebmZuN2u80f//hHY4wxe/bsMZJMbW2t1eatt94yDofDfPrpp+HrPGxv2rRp5sEHHww4Nn36dFNRUWGMIT4ROZLMmjVrrO9DFYu/+c1vTHZ2dsBn+4IFC8ywYcPCfEVfoNId5c6fP6+6ujqVl5dbx5xOp8rLy7V169YI9gzxpqWlRZKUk5MjSaqrq1N7e3tAbA4fPlzFxcVWbG7dulWjR49WQUGB1WbKlClqbW3V7t27+7D3sKvZs2dr2rRpAXEoEZ+IrL/+9a8qLS3VD3/4Q+Xn52vs2LH67W9/a50/cOCAGhsbA+IzMzNTEyZMCIjPrKwslZaWWm3Ky8vldDq1bdu2vrsY2M4NN9yg9evXa9++fZKk9957T1u2bNFtt90mifhE9AhVLG7dulU33XSTkpKSrDZTpkxRQ0ODTpw40SfX4uqTn4LLdvz4cXV2dgbcFEpSQUGBPvzwwwj1CvHG6/Vq3rx5mjhxokaNGiVJamxsVFJSkrKysgLaFhQUqLGx0WrTU+z6zgFXYtWqVaqvr1dtbW23c8QnIunjjz/W8uXL9eijj+qnP/2pamtr9cgjjygpKUmVlZVWfPUUf/7xmZ+fH3De5XIpJyeH+MQVefLJJ9Xa2qrhw4crISFBnZ2deu6551RRUSFJxCeiRqhisbGxUSUlJd1ew3fOf+pPuJB0A/hKs2fP1q5du7Rly5ZIdwWQJB0+fFhz585VdXW1kpOTI90dIIDX61Vpaal++ctfSpLGjh2rXbt2acWKFaqsrIxw7xDv/vznP+u1117TH/7wB1177bXauXOn5s2bp8LCQuITCBOGl0e5vLw8JSQkdFtx97PPPpPH44lQrxBP5syZo6qqKm3cuFGDBg2yjns8Hp0/f17Nzc0B7f1j0+Px9Bi7vnPA5aqrq9OxY8d03XXXyeVyyeVyafPmzVq6dKlcLpcKCgqIT0TMgAEDNHLkyIBjI0aM0KFDhyR9EV9f9tnu8Xi6LZja0dGhpqYm4hNX5PHHH9eTTz6pe+65R6NHj9Z9992n+fPna9GiRZKIT0SPUMViNHzek3RHuaSkJI0bN07r16+3jnm9Xq1fv15lZWUR7BnszhijOXPmaM2aNdqwYUO3YTnjxo1TYmJiQGw2NDTo0KFDVmyWlZXpgw8+CHgzrK6uVkZGRrcbUuBS3Hrrrfrggw+0c+dO66u0tFQVFRXWv4lPRMrEiRO7bbG4b98+DR48WJJUUlIij8cTEJ+tra3atm1bQHw2Nzerrq7OarNhwwZ5vV5NmDChD64CdnX69Gk5nYEpQEJCgrxeryTiE9EjVLFYVlammpoatbe3W22qq6s1bNiwPhlaLonVy2PBqlWrjNvtNq+88orZs2ePmTVrlsnKygpYcRcItZ/85CcmMzPTbNq0yRw9etT6On36tNXmoYceMsXFxWbDhg1m+/btpqyszJSVlVnnOzo6zKhRo8zkyZPNzp07zbp160z//v3NU089FYlLgs35r15uDPGJyHn33XeNy+Uyzz33nNm/f7957bXXTGpqqvn9739vtVm8eLHJysoyf/nLX8z7779vvve975mSkhJz5swZq83UqVPN2LFjzbZt28yWLVvM1VdfbWbMmBGJS4KNVFZWmoEDB5qqqipz4MAB8/rrr5u8vDzzxBNPWG2IT/SVtrY2s2PHDrNjxw4jybz44otmx44d5uDBg8aY0MRic3OzKSgoMPfdd5/ZtWuXWbVqlUlNTTUvv/xyn10nSXeMeOmll0xxcbFJSkoy48ePN++8806kuwSbk9Tj1+9+9zurzZkzZ8zDDz9ssrOzTWpqqvn+979vjh49GvA6//znP81tt91mUlJSTF5ennnsscdMe3t7H18N4kFw0k18IpLWrl1rRo0aZdxutxk+fLhZuXJlwHmv12sWLlxoCgoKjNvtNrfeeqtpaGgIaPOvf/3LzJgxw6Snp5uMjAzzwAMPmLa2tr68DNhQa2urmTt3rikuLjbJyclm6NCh5mc/+1nAdkrEJ/rKxo0be7zfrKysNMaELhbfe+89M2nSJON2u83AgQPN4sWL++oSjTHGOIwxpm9q6gAAAAAAxBfmdAMAAAAAECYk3QAAAAAAhAlJNwAAAAAAYULSDQAAAABAmJB0AwAAAAAQJiTdAAAAAACECUk3AAAAAABhQtINAAAAAECYkHQDAAAAABAmJN0AAAAAAIQJSTcAAAAAAGFC0g0AAAAAQJj8P+AFjIn17xyKAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1000x1200 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate rolling averages\n",
    "window_size = 10\n",
    "rolling_rewards = np.convolve(rewards_history, np.ones(window_size)/window_size, mode='valid')\n",
    "rolling_steps = np.convolve(steps_history, np.ones(window_size)/window_size, mode='valid')\n",
    "rolling_success = np.convolve([1 if s else 0 for s in success_history], np.ones(window_size)/window_size, mode='valid') * 100\n",
    "\n",
    "# Create the plots\n",
    "# fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(10, 12), sharex=True)\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 12), sharex=True)\n",
    "\n",
    "# Plot rewards\n",
    "ax1.plot(rolling_rewards)\n",
    "ax1.set_ylabel('Average Reward')\n",
    "ax1.set_title('Training Progress (10-episode rolling average)')\n",
    "\n",
    "# Plot steps\n",
    "ax2.plot(rolling_steps)\n",
    "ax2.set_ylabel('Average Steps')\n",
    "\n",
    "# Plot success rate\n",
    "#ax3.plot(rolling_success)\n",
    "#ax3.set_ylabel('Success Rate (%)')\n",
    "#ax3.set_xlabel('Episode')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Trained Agent\n",
    "\n",
    "Now let's evaluate our trained agent with exploration turned off to see how well it performs on unseen seeds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the trained agent on 5 new episodes...\n",
      "Test Episode 1: Steps=141, Reward=100.0, Position=[16 30], Goal reached=True\n",
      "Test Episode 2: Steps=228, Reward=100.0, Position=[17 31], Goal reached=True\n",
      "Test Episode 3: Steps=97, Reward=100.0, Position=[17 30], Goal reached=True\n",
      "Test Episode 4: Steps=140, Reward=100.0, Position=[16 30], Goal reached=True\n",
      "Test Episode 5: Steps=188, Reward=100.0, Position=[17 31], Goal reached=True\n"
     ]
    }
   ],
   "source": [
    "# Turn off exploration for evaluation\n",
    "ql_agent_full.exploration_rate = 0\n",
    "\n",
    "# Create test environment\n",
    "test_env = SailingEnv(**get_initial_windfield('training_1'))\n",
    "\n",
    "# Test parameters\n",
    "num_test_episodes = 5\n",
    "max_steps = 1000\n",
    "\n",
    "print(\"Testing the trained agent on 5 new episodes...\")\n",
    "# Testing loop\n",
    "for episode in range(num_test_episodes):\n",
    "    # Reset environment\n",
    "    observation, info = test_env.reset(seed=1000 + episode)  # Different seeds from training\n",
    "    \n",
    "    total_reward = 0\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        # Select action using learned policy\n",
    "        action = ql_agent_full.act(observation)\n",
    "        observation, reward, done, truncated, info = test_env.step(action)\n",
    "        \n",
    "        total_reward += reward\n",
    "        \n",
    "        # Break if episode is done\n",
    "        if done or truncated:\n",
    "            break\n",
    "    \n",
    "    print(f\"Test Episode {episode+1}: Steps={step+1}, Reward={total_reward}, \" +\n",
    "          f\"Position={info['position']}, Goal reached={done}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Your Agent's Behavior\n",
    "\n",
    "While training metrics provide quantitative insights, actually seeing your agent navigate through the environment can reveal critical information about its behavior and strategy.\n",
    "\n",
    "#### Using evaluate_agent.ipynb for Visualization\n",
    "\n",
    "The `evaluate_agent.ipynb` notebook provides powerful visualization tools that let you see:\n",
    "- Complete trajectories across different initial windfields\n",
    "- How your agent responds to wind conditions\n",
    "- Frame-by-frame animations of navigation decisions\n",
    "\n",
    "To use these visualizations, you'll need to save your agent in the proper format first, which we'll do in the next section. Once saved, you can:\n",
    "\n",
    "1. Open `evaluate_agent.ipynb`\n",
    "2. Set `AGENT_PATH` to your saved agent file\n",
    "3. Run the evaluation cells to generate visualizations\n",
    "\n",
    "These visual insights can help you identify patterns, diagnose issues, and refine your agent's strategy in ways that metrics alone cannot reveal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Your Agent for Submission\n",
    "\n",
    "Now let's save our trained agent as a Python file that can be used for evaluation and submission. This step is crucial for three key reasons:\n",
    "\n",
    "1. **Visualization and Testing**: Saving allows you to use the `evaluate_agent.ipynb` notebook to visualize trajectories and test performance across different scenarios.\n",
    "\n",
    "2. **Validation and Evaluation**: The saved agent can be validated with `validate_agent.ipynb` and thoroughly evaluated using different seeds and initial windfields with `evaluate_agent.ipynb`. These notebooks provide important metrics and visualizations to understand your agent's performance.\n",
    "\n",
    "3. **Submission Format**: Any agent submitted to the evaluator **must** be in this format - a single standalone Python (.py) file with a class that inherits from `BaseAgent` and implements all required methods. This is the official submission format for the challenge.\n",
    "\n",
    "For Q-learning agents like ours, we've created a utility function `save_qlearning_agent()` in `src/utils/agent_utils.py` that handles the process of saving the agent with all its learned parameters. This creates a standalone Python file ready for submission.\n",
    "\n",
    "This utility function:\n",
    "1. Extracts the Q-table from your trained agent\n",
    "2. Creates a new Python file with a clean agent implementation\n",
    "3. Embeds the learned Q-values directly in the code\n",
    "4. Includes all the necessary methods (act, reset, seed, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent saved to ../src/agents/agent_trained_example.py\n",
      "The file contains 514 state-action pairs.\n",
      "You can now use this file with validate_agent.ipynb and evaluate_agent.ipynb\n"
     ]
    }
   ],
   "source": [
    "# Import the utility function for saving Q-learning agents\n",
    "from src.utils.agent_utils import save_qlearning_agent\n",
    "\n",
    "# Save our trained agent\n",
    "save_qlearning_agent(\n",
    "    agent=ql_agent_full,\n",
    "    output_path=\"../src/agents/agent_trained_example.py\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extending the Utility for Your Own Agents\n",
    "\n",
    "If you implement different types of agents (such as DQN, SARSA, or custom algorithms), you may need to create similar utility functions. Some tips:\n",
    "\n",
    "1. Make sure your save function preserves all necessary parameters and learned values\n",
    "2. Store them in a way that doesn't require additional files (embedded in the code)\n",
    "3. Ensure the saved agent includes all required methods from the BaseAgent interface\n",
    "\n",
    "When extending `save_qlearning_agent()` for different Q-learning variants, you might need to:\n",
    "- Update the state discretization logic\n",
    "- Change how parameters are stored and initialized\n",
    "- Modify the act() method's logic for your specific algorithm\n",
    "\n",
    "### Agent Types and Saving Strategies\n",
    "\n",
    "**For Rule-Based Agents:**\n",
    "- Since rule-based agents don't have learned parameters, you can simply ensure your agent class follows the `BaseAgent` interface\n",
    "- Implement all required methods: `__init__()`, `act(observation)`, `reset()`, and `seed(seed)`\n",
    "- These are typically the simplest to save as the agent's logic is entirely defined in the code\n",
    "\n",
    "**For Deep Learning-Based Agents:**\n",
    "- Include the model architecture definition directly in your Python file\n",
    "- Convert model weights to numpy arrays and include them in your code\n",
    "- Add functions to rebuild the model from these arrays\n",
    "\n",
    "### Key Requirements for Any Submission File\n",
    "\n",
    "Regardless of your agent type, ensure your submission file:\n",
    "1. **Contains everything**: All code, parameters, and data needed to run the agent\n",
    "2. **Is a single file**: No external dependencies beyond standard libraries\n",
    "3. **Follows the interface**: Properly inherits from `BaseAgent` and implements all required methods\n",
    "4. **Requires no arguments**: The agent must initialize without any required arguments\n",
    "5. **Is deterministic**: For a given seed, the agent should behave identically each time\n",
    "\n",
    "## Important Note on Import Paths\n",
    "\n",
    "When creating agent files for submission, make sure to use the correct import paths:\n",
    "- **Use**: `from agents.base_agent import BaseAgent`\n",
    "- **Not**: `from src.agents.base_agent import BaseAgent`\n",
    "\n",
    "This is because the validation and evaluation scripts run from within the `src` directory, so imports should be relative to that location. Our utility function `save_qlearning_agent` already handles this for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automating Agent Training and Evaluation\n",
    "\n",
    "The approach we've taken here is interactive and educational, but for serious agent development, you'll likely want to automate the training process. Here's where you could expand:\n",
    "\n",
    "```python\n",
    "# Your training script could look something like this:\n",
    "def train_agent(agent, initial_windfields, num_episodes, save_path):\n",
    "    # Setup training parameters\n",
    "    # ...\n",
    "    \n",
    "    # Train on multiple initial windfields\n",
    "    for initial_windfield_name, initial_windfield in initial_windfields.items():\n",
    "        # Train agent on this initial_windfields\n",
    "        # ...\n",
    "        \n",
    "    # Save the trained agent\n",
    "    # ...\n",
    "    \n",
    "    return training_metrics\n",
    "```\n",
    "\n",
    "Creating a command-line interface for training and evaluation would allow you to:\n",
    "1. Train agents with different hyperparameters\n",
    "2. Evaluate on multiple initial_windfields \n",
    "3. Create systematic experiments\n",
    "\n",
    "This is left as an exercise for you to implement based on your specific approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and Next Steps\n",
    "\n",
    "In this notebook, we've covered:\n",
    "\n",
    "1. **Agent Requirements**: Understanding the BaseAgent interface \n",
    "2. **The Naive Agent**: Examining a simple rule-based agent \n",
    "3. **Simplified Q-Learning Agent**: Implementing and training a basic RL agent that uses only local information (position, velocity, and local wind)\n",
    "\n",
    "### Next Steps for Developing Your Own Agent\n",
    "\n",
    "Now it's your turn to develop your own agent. Here are some suggestions:\n",
    "\n",
    "1. **Enhance the Q-Learning Agent**:\n",
    "   - Extend the state representation to incorporate the full wind field (not just local wind)\n",
    "   - This would allow the agent to anticipate wind changes and plan better routes\n",
    "   - Hint: Modify the `discretize_state` method to extract and process relevant features from the flattened wind field\n",
    "\n",
    "2. **Algorithmic Improvements**:\n",
    "   - Implement function approximation to handle continuous state spaces better\n",
    "   - Explore other RL algorithms like SARSA, Expected SARSA, or Deep Q-Networks\n",
    "   - Experiment with different exploration strategies that adapt over time\n",
    "\n",
    "3. **Physics-Based Approaches**:\n",
    "   - Leverage your understanding of sailing physics (from challenge_walkthrough notebook)\n",
    "   - Implement rule-based algorithms or path planning (A*, etc.) that take advantage of domain knowledge\n",
    "   - Create hybrid approaches that combine RL with domain-specific rules\n",
    "   \n",
    "### Validating and Evaluating Your Agent\n",
    "\n",
    "After you've developed your agent, the next steps are to:\n",
    "\n",
    "1. **Validate your agent** using the `validate_agent.ipynb` notebook or command-line tool\n",
    "2. **Evaluate your agent** using the `evaluate_agent.ipynb` notebook\n",
    "\n",
    "Remember that agents combining multiple techniques often perform best - consider how you might blend RL with domain knowledge of sailing physics for optimal results!\n",
    "\n",
    "Good luck with the Sailing Challenge!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.7.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Downloading filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/site-packages (from torch) (4.13.2)\n",
      "Collecting setuptools (from torch)\n",
      "  Downloading setuptools-80.4.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch)\n",
      "  Downloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/site-packages (from torch) (2025.3.2)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.6.77 (from torch)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.6.77 (from torch)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.6.80 (from torch)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.5.1.17 (from torch)\n",
      "  Downloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.6.4.1 (from torch)\n",
      "  Downloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.3.0.4 (from torch)\n",
      "  Downloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.7.77 (from torch)\n",
      "  Downloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.7.1.2 (from torch)\n",
      "  Downloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.5.4.2 (from torch)\n",
      "  Downloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.6.3 (from torch)\n",
      "  Downloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting nvidia-nccl-cu12==2.26.2 (from torch)\n",
      "  Downloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.6.77 (from torch)\n",
      "  Downloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.6.85 (from torch)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufile-cu12==1.11.1.6 (from torch)\n",
      "  Downloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting triton==3.3.0 (from torch)\n",
      "  Downloading triton-3.3.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Downloading torch-2.7.0-cp312-cp312-manylinux_2_28_x86_64.whl (865.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m865.0/865.0 MB\u001b[0m \u001b[31m40.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (393.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m393.1/393.1 MB\u001b[0m \u001b[31m62.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m50.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (23.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m83.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (897 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.7/897.7 kB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl (571.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m571.0/571.0 MB\u001b[0m \u001b[31m53.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (200.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.2/200.2 MB\u001b[0m \u001b[31m69.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m81.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (158.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.2/158.2 MB\u001b[0m \u001b[31m73.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (216.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.6/216.6 MB\u001b[0m \u001b[31m76.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl (156.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 MB\u001b[0m \u001b[31m71.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (201.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.3/201.3 MB\u001b[0m \u001b[31m69.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (19.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m68.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "Downloading triton-3.3.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (156.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.5/156.5 MB\u001b[0m \u001b[31m81.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading setuptools-80.4.0-py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m56.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m87.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Downloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m62.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nvidia-cusparselt-cu12, mpmath, sympy, setuptools, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, networkx, filelock, triton, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch\n",
      "\u001b[2K  Attempting uninstall: nvidia-nccl-cu12━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 5/21\u001b[0m [nvidia-nvjitlink-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-nccl-cu12 2.26.5━━━━━━\u001b[0m \u001b[32m 5/21\u001b[0m [nvidia-nvjitlink-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-nccl-cu12-2.26.5:━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 5/21\u001b[0m [nvidia-nvjitlink-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-nccl-cu12-2.26.5━━━━━━━━━━━━\u001b[0m \u001b[32m 6/21\u001b[0m [nvidia-nccl-cu12]]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21/21\u001b[0m [torch]m20/21\u001b[0m [torch]-cusolver-cu12]2]2]\n",
      "\u001b[1A\u001b[2KSuccessfully installed filelock-3.18.0 mpmath-1.3.0 networkx-3.4.2 nvidia-cublas-cu12-12.6.4.1 nvidia-cuda-cupti-cu12-12.6.80 nvidia-cuda-nvrtc-cu12-12.6.77 nvidia-cuda-runtime-cu12-12.6.77 nvidia-cudnn-cu12-9.5.1.17 nvidia-cufft-cu12-11.3.0.4 nvidia-cufile-cu12-1.11.1.6 nvidia-curand-cu12-10.3.7.77 nvidia-cusolver-cu12-11.7.1.2 nvidia-cusparse-cu12-12.5.4.2 nvidia-cusparselt-cu12-0.6.3 nvidia-nccl-cu12-2.26.2 nvidia-nvjitlink-cu12-12.6.85 nvidia-nvtx-cu12-12.6.77 setuptools-80.4.0 sympy-1.14.0 torch-2.7.0 triton-3.3.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "from src.agents.base_agent import BaseAgent\n",
    "\n",
    "class PPOSailingAgent(BaseAgent):\n",
    "    \"\"\"Agent utilisant Proximal Policy Optimization (PPO) pour le Sailing Challenge.\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 learning_rate=0.0003,\n",
    "                 gamma=0.99,\n",
    "                 gae_lambda=0.95,\n",
    "                 clip_param=0.2,\n",
    "                 value_coef=0.5,\n",
    "                 entropy_coef=0.01,\n",
    "                 max_grad_norm=0.5,\n",
    "                 ppo_epochs=4,\n",
    "                 mini_batch_size=64,\n",
    "                 use_sailing_features=True):\n",
    "        \"\"\"\n",
    "        Initialise l'agent PPO.\n",
    "        \n",
    "        Args:\n",
    "            learning_rate: Taux d'apprentissage pour l'optimiseur\n",
    "            gamma: Facteur d'actualisation pour les récompenses\n",
    "            gae_lambda: Paramètre lambda pour l'estimation d'avantage généralisée\n",
    "            clip_param: Paramètre d'écrêtage pour PPO\n",
    "            value_coef: Coefficient pour la perte de la fonction de valeur\n",
    "            entropy_coef: Coefficient pour le bonus d'entropie\n",
    "            max_grad_norm: Norme maximale du gradient pour l'écrêtage\n",
    "            ppo_epochs: Nombre d'époques pour chaque mise à jour PPO\n",
    "            mini_batch_size: Taille des mini-lots pour l'entraînement\n",
    "            use_sailing_features: Utiliser des caractéristiques spécifiques à la voile\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.np_random = np.random.default_rng()\n",
    "        \n",
    "        # Paramètres PPO\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = gamma\n",
    "        self.gae_lambda = gae_lambda\n",
    "        self.clip_param = clip_param\n",
    "        self.value_coef = value_coef\n",
    "        self.entropy_coef = entropy_coef\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "        self.ppo_epochs = ppo_epochs\n",
    "        self.mini_batch_size = mini_batch_size\n",
    "        \n",
    "        # Paramètres d'environnement\n",
    "        self.observation_dim = 2050  # x, y, vx, vy, wx, wy + wind field (32x32x2)\n",
    "        self.action_dim = 9         # 8 directions + stay in place\n",
    "        \n",
    "        # Construction des réseaux\n",
    "        self.use_sailing_features = use_sailing_features\n",
    "        hidden_dim = 256\n",
    "        \n",
    "        # Définir la taille d'entrée en fonction de l'utilisation des caractéristiques de voile\n",
    "        input_dim = self.observation_dim\n",
    "        if self.use_sailing_features:\n",
    "            # Si nous utilisons des caractéristiques extraites, la dimension d'entrée est plus petite\n",
    "            input_dim = 25  # Caractéristiques extraites au lieu de l'observation brute\n",
    "        \n",
    "        # Réseaux pour la politique et la valeur\n",
    "        self.policy_value_network = PolicyValueNetwork(input_dim, hidden_dim, self.action_dim)\n",
    "        \n",
    "        # Optimiseur\n",
    "        self.optimizer = optim.Adam(self.policy_value_network.parameters(), lr=self.lr)\n",
    "        \n",
    "        # Mémoire pour stocker les expériences\n",
    "        self.memory = PPOMemory()\n",
    "        \n",
    "        # Appareil d'exécution (CPU ou GPU)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.policy_value_network.to(self.device)\n",
    "        \n",
    "        # Compteurs\n",
    "        self.training_step = 0\n",
    "        \n",
    "        # Position de l'objectif (sera mise à jour lors de l'initialisation de l'environnement)\n",
    "        self.goal_position = np.array([16, 31])  # Valeur par défaut\n",
    "        \n",
    "        # Indice d'efficacité de navigation\n",
    "        self.sailing_efficiency = self._create_sailing_efficiency()\n",
    "    \n",
    "    def _create_sailing_efficiency(self):\n",
    "        \"\"\"\n",
    "        Crée une table d'efficacité de navigation basée sur l'angle au vent.\n",
    "        Cette connaissance sera utilisée pour l'extraction de caractéristiques.\n",
    "        \"\"\"\n",
    "        efficiency = {}\n",
    "        \n",
    "        # Zone de non-navigation (irons) - très faible efficacité\n",
    "        for angle in range(0, 46):\n",
    "            efficiency[angle] = 0.1\n",
    "            efficiency[360-angle] = 0.1\n",
    "        \n",
    "        # Près (close-hauled) - efficacité modérée\n",
    "        for angle in range(46, 80):\n",
    "            factor = (angle - 45) / 35  # Augmentation progressive de l'efficacité\n",
    "            efficiency[angle] = 0.3 + 0.4 * factor\n",
    "            efficiency[360-angle] = 0.3 + 0.4 * factor\n",
    "        \n",
    "        # Travers (beam reach) - haute efficacité\n",
    "        for angle in range(80, 120):\n",
    "            efficiency[angle] = 0.9 + 0.1 * np.sin((angle - 80) * np.pi / 80)\n",
    "            efficiency[360-angle] = 0.9 + 0.1 * np.sin((angle - 80) * np.pi / 80)\n",
    "        \n",
    "        # Grand largue (broad reach) - très haute efficacité\n",
    "        for angle in range(120, 150):\n",
    "            efficiency[angle] = 0.95\n",
    "            efficiency[360-angle] = 0.95\n",
    "        \n",
    "        # Vent arrière (running) - haute efficacité mais pas optimale\n",
    "        for angle in range(150, 181):\n",
    "            factor = (180 - angle) / 30  # Diminution progressive\n",
    "            efficiency[angle] = 0.85 + 0.1 * factor\n",
    "            efficiency[360-angle] = 0.85 + 0.1 * factor\n",
    "        \n",
    "        return efficiency\n",
    "    \n",
    "    def extract_features(self, observation):\n",
    "        \"\"\"\n",
    "        Extrait des caractéristiques pertinentes de l'observation brute.\n",
    "        Cette étape est cruciale pour réduire la dimensionnalité et incorporer\n",
    "        des connaissances spécifiques à la voile.\n",
    "        \n",
    "        Args:\n",
    "            observation: Observation brute de l'environnement\n",
    "        \n",
    "        Returns:\n",
    "            Vecteur de caractéristiques extraites\n",
    "        \"\"\"\n",
    "        # Position et vitesse actuelles\n",
    "        position = observation[:2]\n",
    "        velocity = observation[2:4]\n",
    "        wind_at_boat = observation[4:6]\n",
    "        \n",
    "        # Calculer la distance et la direction vers l'objectif\n",
    "        direction_to_goal = self.goal_position - position\n",
    "        distance_to_goal = np.linalg.norm(direction_to_goal)\n",
    "        if distance_to_goal > 0:\n",
    "            direction_to_goal = direction_to_goal / distance_to_goal  # Normaliser\n",
    "        \n",
    "        # Calcul des angles\n",
    "        wind_speed = np.linalg.norm(wind_at_boat)\n",
    "        wind_angle = np.degrees(np.arctan2(wind_at_boat[1], wind_at_boat[0])) % 360\n",
    "        \n",
    "        velocity_speed = np.linalg.norm(velocity)\n",
    "        if velocity_speed > 0.01:\n",
    "            velocity_angle = np.degrees(np.arctan2(velocity[1], velocity[0])) % 360\n",
    "        else:\n",
    "            velocity_angle = 0\n",
    "        \n",
    "        goal_angle = np.degrees(np.arctan2(direction_to_goal[1], direction_to_goal[0])) % 360\n",
    "        \n",
    "        # Angle relatif entre le vent et la direction vers l'objectif\n",
    "        wind_to_goal_angle = (goal_angle - wind_angle) % 360\n",
    "        \n",
    "        # Angle relatif entre le vent et la vitesse\n",
    "        wind_to_velocity_angle = (velocity_angle - wind_angle) % 360\n",
    "        \n",
    "        # Obtenir l'efficacité de navigation actuelle\n",
    "        current_efficiency = self.sailing_efficiency.get(int(wind_to_velocity_angle), 0.5)\n",
    "        \n",
    "        # Indicateur de zone de non-navigation (upwind)\n",
    "        upwind_condition = (wind_to_goal_angle < 45 or wind_to_goal_angle > 315)\n",
    "        \n",
    "        # Extraction de caractéristiques du champ de vent\n",
    "        wind_field = observation[6:].reshape(-1, 2)  # Supposons que c'est un champ 2D\n",
    "        \n",
    "        # Calcul de statistiques sur le vent dans différentes régions\n",
    "        # (cela pourrait être plus sophistiqué avec une segmentation appropriée)\n",
    "        mid_point = len(wind_field) // 2\n",
    "        quarter_point = mid_point // 2\n",
    "        \n",
    "        # Moyennes des vents dans différentes régions\n",
    "        wind_north = np.mean(wind_field[mid_point+quarter_point:], axis=0)\n",
    "        wind_south = np.mean(wind_field[:mid_point-quarter_point], axis=0)\n",
    "        wind_east = np.mean(wind_field[mid_point-quarter_point:mid_point+quarter_point], axis=0)\n",
    "        \n",
    "        # Caractéristiques spécifiques à la voile pour différentes manœuvres\n",
    "        # Possibilité de virement de bord (tacking)\n",
    "        port_tack_angle = (wind_angle + 60) % 360\n",
    "        starboard_tack_angle = (wind_angle - 60) % 360\n",
    "        \n",
    "        # Angles relatifs des virements par rapport à l'objectif\n",
    "        port_tack_to_goal = min((port_tack_angle - goal_angle) % 360, (goal_angle - port_tack_angle) % 360)\n",
    "        starboard_tack_to_goal = min((starboard_tack_angle - goal_angle) % 360, (goal_angle - starboard_tack_angle) % 360)\n",
    "        \n",
    "        # Créer le vecteur de caractéristiques\n",
    "        features = np.array([\n",
    "            # Position normalisée\n",
    "            position[0] / 32.0,  # Normaliser par la taille de la grille\n",
    "            position[1] / 32.0,\n",
    "            \n",
    "            # Vitesse et direction\n",
    "            velocity_speed,\n",
    "            np.sin(np.radians(velocity_angle)),\n",
    "            np.cos(np.radians(velocity_angle)),\n",
    "            \n",
    "            # Vent local\n",
    "            wind_speed,\n",
    "            np.sin(np.radians(wind_angle)),\n",
    "            np.cos(np.radians(wind_angle)),\n",
    "            \n",
    "            # Objectif\n",
    "            distance_to_goal / 45.0,  # Normaliser par la distance max possible\n",
    "            np.sin(np.radians(goal_angle)),\n",
    "            np.cos(np.radians(goal_angle)),\n",
    "            \n",
    "            # Angles relatifs\n",
    "            np.sin(np.radians(wind_to_goal_angle)),\n",
    "            np.cos(np.radians(wind_to_goal_angle)),\n",
    "            np.sin(np.radians(wind_to_velocity_angle)),\n",
    "            np.cos(np.radians(wind_to_velocity_angle)),\n",
    "            \n",
    "            # Efficacité de navigation\n",
    "            current_efficiency,\n",
    "            \n",
    "            # Indicateurs de conditions\n",
    "            1.0 if upwind_condition else 0.0,\n",
    "            \n",
    "            # Statistiques régionales du vent\n",
    "            wind_north[0], wind_north[1],\n",
    "            wind_south[0], wind_south[1],\n",
    "            wind_east[0], wind_east[1],\n",
    "            \n",
    "            # Caractéristiques de virement\n",
    "            port_tack_to_goal / 180.0,  # Normaliser\n",
    "            starboard_tack_to_goal / 180.0\n",
    "        ])\n",
    "        \n",
    "        return features\n",
    "        \n",
    "    def act(self, observation):\n",
    "        \"\"\"\n",
    "        Sélectionne une action basée sur l'observation actuelle.\n",
    "        \n",
    "        Args:\n",
    "            observation: Observation de l'environnement\n",
    "        \n",
    "        Returns:\n",
    "            L'action choisie\n",
    "        \"\"\"\n",
    "        # Convertir l'observation en caractéristiques si nécessaire\n",
    "        if self.use_sailing_features:\n",
    "            state = self.extract_features(observation)\n",
    "        else:\n",
    "            state = observation\n",
    "        \n",
    "        # Convertir en tensor et déplacer vers l'appareil d'exécution\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "        \n",
    "        # Passer le state au réseau et obtenir la distribution de politique\n",
    "        with torch.no_grad():\n",
    "            action_probs, state_value = self.policy_value_network(state)\n",
    "        \n",
    "        # Créer la distribution catégorielle\n",
    "        distribution = Categorical(action_probs)\n",
    "        \n",
    "        # Échantillonner une action\n",
    "        action = distribution.sample()\n",
    "        \n",
    "        # Stocker l'information pour l'apprentissage\n",
    "        self.memory.store_action(action.item(), distribution.log_prob(action).item(), state_value.item())\n",
    "        \n",
    "        return action.item()\n",
    "    \n",
    "    def remember(self, observation, action, reward, next_observation, done):\n",
    "        \"\"\"\n",
    "        Stocke une transition dans la mémoire.\n",
    "        \n",
    "        Args:\n",
    "            observation: Observation actuelle\n",
    "            action: Action prise\n",
    "            reward: Récompense reçue\n",
    "            next_observation: Prochaine observation\n",
    "            done: Indicateur de fin d'épisode\n",
    "        \"\"\"\n",
    "        # Convertir l'observation en caractéristiques si nécessaire\n",
    "        if self.use_sailing_features:\n",
    "            state = self.extract_features(observation)\n",
    "            next_state = self.extract_features(next_observation) if next_observation is not None else None\n",
    "        else:\n",
    "            state = observation\n",
    "            next_state = next_observation\n",
    "        \n",
    "        # Stocker la transition dans la mémoire\n",
    "        self.memory.store_transition(state, reward, done)\n",
    "        \n",
    "        # Si l'épisode est terminé, calculer les avantages et préparer pour l'apprentissage\n",
    "        if done:\n",
    "            self.memory.finish_trajectory(last_value=0)\n",
    "        \n",
    "    def learn(self):\n",
    "        \"\"\"\n",
    "        Exécute l'apprentissage PPO sur les expériences collectées.\n",
    "        \"\"\"\n",
    "        # Vérifier s'il y a suffisamment de données pour l'apprentissage\n",
    "        if len(self.memory) < self.mini_batch_size:\n",
    "            return\n",
    "        \n",
    "        # Récupérer les données de la mémoire\n",
    "        states, actions, old_log_probs, returns, advantages = self.memory.get_all()\n",
    "        \n",
    "        # Convertir en tensors\n",
    "        states = torch.FloatTensor(np.vstack(states)).to(self.device)\n",
    "        actions = torch.LongTensor(actions).to(self.device)\n",
    "        old_log_probs = torch.FloatTensor(old_log_probs).to(self.device)\n",
    "        returns = torch.FloatTensor(returns).to(self.device)\n",
    "        advantages = torch.FloatTensor(advantages).to(self.device)\n",
    "        \n",
    "        # Normaliser les avantages (améliore la stabilité)\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "        \n",
    "        # Mise à jour par mini-lots\n",
    "        indices = np.arange(len(states))\n",
    "        \n",
    "        for _ in range(self.ppo_epochs):\n",
    "            # Mélanger les indices\n",
    "            self.np_random.shuffle(indices)\n",
    "            \n",
    "            # Parcourir les mini-lots\n",
    "            for start in range(0, len(indices), self.mini_batch_size):\n",
    "                # Sélectionner le mini-lot\n",
    "                batch_indices = indices[start:start + self.mini_batch_size]\n",
    "                batch_states = states[batch_indices]\n",
    "                batch_actions = actions[batch_indices]\n",
    "                batch_old_log_probs = old_log_probs[batch_indices]\n",
    "                batch_returns = returns[batch_indices]\n",
    "                batch_advantages = advantages[batch_indices]\n",
    "                \n",
    "                # Passer les états au réseau\n",
    "                action_probs, state_values = self.policy_value_network(batch_states)\n",
    "                distribution = Categorical(action_probs)\n",
    "                \n",
    "                # Obtenir les nouveaux log probs et l'entropie\n",
    "                new_log_probs = distribution.log_prob(batch_actions)\n",
    "                entropy = distribution.entropy().mean()\n",
    "                \n",
    "                # Calculer le ratio\n",
    "                ratio = torch.exp(new_log_probs - batch_old_log_probs)\n",
    "                \n",
    "                # Calcul des termes de perte clippés et non clippés\n",
    "                surr1 = ratio * batch_advantages\n",
    "                surr2 = torch.clamp(ratio, 1.0 - self.clip_param, 1.0 + self.clip_param) * batch_advantages\n",
    "                \n",
    "                # Pertes\n",
    "                policy_loss = -torch.min(surr1, surr2).mean()\n",
    "                value_loss = F.mse_loss(state_values.squeeze(), batch_returns)\n",
    "                \n",
    "                # Perte totale\n",
    "                loss = policy_loss + self.value_coef * value_loss - self.entropy_coef * entropy\n",
    "                \n",
    "                # Mise à jour des poids\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                # Écrêtage du gradient (évite les mises à jour trop grandes)\n",
    "                nn.utils.clip_grad_norm_(self.policy_value_network.parameters(), self.max_grad_norm)\n",
    "                self.optimizer.step()\n",
    "        \n",
    "        # Réinitialiser la mémoire\n",
    "        self.memory.clear()\n",
    "        \n",
    "        # Incrémenter le compteur d'étapes d'entraînement\n",
    "        self.training_step += 1\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Réinitialise l'agent pour un nouvel épisode.\"\"\"\n",
    "        # Rien à faire pour PPO ici car la mémoire est gérée séparément\n",
    "        pass\n",
    "    \n",
    "    def seed(self, seed=None):\n",
    "        \"\"\"Définit la graine aléatoire.\"\"\"\n",
    "        self.np_random = np.random.default_rng(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed(seed)\n",
    "    \n",
    "    def save(self, path):\n",
    "        \"\"\"Sauvegarde le modèle.\"\"\"\n",
    "        torch.save({\n",
    "            'policy_value_network': self.policy_value_network.state_dict(),\n",
    "            'optimizer': self.optimizer.state_dict(),\n",
    "            'training_step': self.training_step\n",
    "        }, path)\n",
    "    \n",
    "    def load(self, path):\n",
    "        \"\"\"Charge le modèle.\"\"\"\n",
    "        checkpoint = torch.load(path, map_location=self.device)\n",
    "        self.policy_value_network.load_state_dict(checkpoint['policy_value_network'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        self.training_step = checkpoint['training_step']\n",
    "\n",
    "\n",
    "class PolicyValueNetwork(nn.Module):\n",
    "    \"\"\"Réseau de politique et de valeur pour PPO.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        \"\"\"\n",
    "        Initialise le réseau.\n",
    "        \n",
    "        Args:\n",
    "            input_dim: Dimension de l'entrée (observation)\n",
    "            hidden_dim: Dimension des couches cachées\n",
    "            output_dim: Dimension de la sortie (nombre d'actions)\n",
    "        \"\"\"\n",
    "        super(PolicyValueNetwork, self).__init__()\n",
    "        \n",
    "        # Couches partagées\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Couches de politique (actor)\n",
    "        self.policy = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 2, output_dim),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        \n",
    "        # Couches de valeur (critic)\n",
    "        self.value = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 2, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Passe avant dans le réseau.\n",
    "        \n",
    "        Args:\n",
    "            x: Entrée (observation)\n",
    "        \n",
    "        Returns:\n",
    "            Tuple (probabilités d'action, valeur d'état)\n",
    "        \"\"\"\n",
    "        shared_features = self.shared(x)\n",
    "        action_probs = self.policy(shared_features)\n",
    "        state_value = self.value(shared_features)\n",
    "        \n",
    "        return action_probs, state_value\n",
    "\n",
    "\n",
    "class PPOMemory:\n",
    "    \"\"\"Mémoire pour stocker les expériences pour PPO.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialise la mémoire.\"\"\"\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.log_probs = []\n",
    "        self.rewards = []\n",
    "        self.values = []\n",
    "        self.dones = []\n",
    "        \n",
    "        # Ajouter ces attributs manquants\n",
    "        self.returns = []\n",
    "        self.advantages = []\n",
    "        \n",
    "        # Variables temporaires pour la trajectoire en cours\n",
    "        self.current_states = []\n",
    "        self.current_actions = []\n",
    "        self.current_log_probs = []\n",
    "        self.current_rewards = []\n",
    "        self.current_values = []\n",
    "        self.current_dones = []\n",
    "    \n",
    "    def store_action(self, action, log_prob, value):\n",
    "        \"\"\"\n",
    "        Stocke l'action, son log de probabilité et la valeur d'état.\n",
    "        \n",
    "        Args:\n",
    "            action: Action prise\n",
    "            log_prob: Log de probabilité de l'action\n",
    "            value: Valeur d'état estimée\n",
    "        \"\"\"\n",
    "        self.current_actions.append(action)\n",
    "        self.current_log_probs.append(log_prob)\n",
    "        self.current_values.append(value)\n",
    "    \n",
    "    def store_transition(self, state, reward, done):\n",
    "        \"\"\"\n",
    "        Stocke une transition.\n",
    "        \n",
    "        Args:\n",
    "            state: État actuel\n",
    "            reward: Récompense reçue\n",
    "            done: Indicateur de fin d'épisode\n",
    "        \"\"\"\n",
    "        self.current_states.append(state)\n",
    "        self.current_rewards.append(reward)\n",
    "        self.current_dones.append(done)\n",
    "    \n",
    "    def finish_trajectory(self, last_value=0, gamma=0.99, gae_lambda=0.95):\n",
    "        \"\"\"\n",
    "        Finalise la trajectoire en cours et calcule les avantages et retours.\n",
    "        \n",
    "        Args:\n",
    "            last_value: Valeur d'état du dernier état (0 si épisode terminé)\n",
    "            gamma: Facteur d'actualisation\n",
    "            gae_lambda: Paramètre lambda pour GAE\n",
    "        \"\"\"\n",
    "        # S'assurer qu'il y a des données\n",
    "        if len(self.current_rewards) == 0:\n",
    "            return\n",
    "        \n",
    "        # Calcul des avantages et retours\n",
    "        rewards = np.array(self.current_rewards + [last_value])\n",
    "        values = np.array(self.current_values + [last_value])\n",
    "        dones = np.array(self.current_dones + [1])  # Considérer le dernier état comme terminé\n",
    "        \n",
    "        # Calcul des avantages avec Generalized Advantage Estimation (GAE)\n",
    "        deltas = rewards[:-1] + gamma * values[1:] * (1 - dones[:-1]) - values[:-1]\n",
    "        advantages = []\n",
    "        advantage = 0\n",
    "        \n",
    "        for delta, done in zip(reversed(deltas), reversed(dones[:-1])):\n",
    "            advantage = delta + gamma * gae_lambda * advantage * (1 - done)\n",
    "            advantages.append(advantage)\n",
    "        \n",
    "        advantages.reverse()\n",
    "        \n",
    "        # Calcul des retours (valeur cible)\n",
    "        returns = advantages + np.array(self.current_values)\n",
    "        \n",
    "        # Ajouter aux listes de stockage\n",
    "        self.states.extend(self.current_states)\n",
    "        self.actions.extend(self.current_actions)\n",
    "        self.log_probs.extend(self.current_log_probs)\n",
    "        self.rewards.extend(self.current_rewards)\n",
    "        self.values.extend(self.current_values)\n",
    "        self.dones.extend(self.current_dones)\n",
    "        \n",
    "        # Ajouter les avantages et retours calculés\n",
    "        self.advantages.extend(advantages)\n",
    "        self.returns.extend(returns)\n",
    "        \n",
    "        # Réinitialiser les variables temporaires\n",
    "        self.current_states = []\n",
    "        self.current_actions = []\n",
    "        self.current_log_probs = []\n",
    "        self.current_rewards = []\n",
    "        self.current_values = []\n",
    "        self.current_dones = []\n",
    "    \n",
    "    def get_all(self):\n",
    "        \"\"\"\n",
    "        Récupère toutes les données de la mémoire.\n",
    "        \n",
    "        Returns:\n",
    "            Tuple (états, actions, log_probs, retours, avantages)\n",
    "        \"\"\"\n",
    "        return (\n",
    "            self.states,\n",
    "            self.actions,\n",
    "            self.log_probs,\n",
    "            self.returns,\n",
    "            self.advantages\n",
    "        )\n",
    "    \n",
    "    def clear(self):\n",
    "        \"\"\"Efface la mémoire.\"\"\"\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.log_probs = []\n",
    "        self.rewards = []\n",
    "        self.values = []\n",
    "        self.dones = []\n",
    "        self.returns = []\n",
    "        self.advantages = []\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Retourne la taille de la mémoire.\"\"\"\n",
    "        return len(self.states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting PPO training...\n",
      "Episode 1: Steps=1000, Reward=0.0, Position=[1 0], Goal reached=True\n",
      "Episode 2: Steps=1000, Reward=0.0, Position=[0 3], Goal reached=True\n",
      "Episode 3: Steps=1000, Reward=0.0, Position=[0 7], Goal reached=True\n",
      "Episode 4: Steps=1000, Reward=0.0, Position=[0 5], Goal reached=True\n",
      "Episode 5: Steps=1000, Reward=0.0, Position=[3 0], Goal reached=True\n",
      "Episode 6: Steps=1000, Reward=0.0, Position=[0 8], Goal reached=True\n",
      "Episode 7: Steps=1000, Reward=0.0, Position=[0 5], Goal reached=True\n",
      "Episode 8: Steps=1000, Reward=0.0, Position=[0 8], Goal reached=True\n",
      "Episode 9: Steps=1000, Reward=0.0, Position=[ 2 11], Goal reached=True\n",
      "Episode 10: Steps=1000, Reward=0.0, Position=[ 1 24], Goal reached=True\n",
      "Average over last 10 episodes: Reward=0.00, Length=1000.00\n",
      "Episode 11: Steps=1000, Reward=0.0, Position=[ 0 30], Goal reached=True\n",
      "Episode 12: Steps=1000, Reward=0.0, Position=[ 2 25], Goal reached=True\n",
      "Episode 13: Steps=1000, Reward=0.0, Position=[ 0 31], Goal reached=True\n",
      "Episode 14: Steps=1000, Reward=0.0, Position=[ 0 28], Goal reached=True\n",
      "Episode 15: Steps=1000, Reward=0.0, Position=[ 0 31], Goal reached=True\n",
      "Episode 16: Steps=1000, Reward=0.0, Position=[ 1 31], Goal reached=True\n",
      "Episode 17: Steps=1000, Reward=0.0, Position=[ 0 31], Goal reached=True\n",
      "Episode 18: Steps=1000, Reward=0.0, Position=[ 1 31], Goal reached=True\n",
      "Episode 19: Steps=1000, Reward=0.0, Position=[ 1 31], Goal reached=True\n",
      "Episode 20: Steps=1000, Reward=0.0, Position=[ 0 30], Goal reached=True\n",
      "Average over last 10 episodes: Reward=0.00, Length=1000.00\n",
      "Episode 21: Steps=1000, Reward=0.0, Position=[ 0 30], Goal reached=True\n",
      "Episode 22: Steps=1000, Reward=0.0, Position=[ 0 31], Goal reached=True\n",
      "Episode 23: Steps=1000, Reward=0.0, Position=[ 0 31], Goal reached=True\n",
      "Episode 24: Steps=1000, Reward=0.0, Position=[ 0 31], Goal reached=True\n",
      "Episode 25: Steps=1000, Reward=0.0, Position=[ 0 31], Goal reached=True\n",
      "Episode 26: Steps=1000, Reward=0.0, Position=[ 0 31], Goal reached=True\n",
      "Episode 27: Steps=1000, Reward=0.0, Position=[ 0 31], Goal reached=True\n",
      "Episode 28: Steps=1000, Reward=0.0, Position=[ 0 31], Goal reached=True\n",
      "Episode 29: Steps=1000, Reward=0.0, Position=[ 0 31], Goal reached=True\n",
      "Episode 30: Steps=1000, Reward=0.0, Position=[ 1 31], Goal reached=True\n",
      "Average over last 10 episodes: Reward=0.00, Length=1000.00\n",
      "Episode 31: Steps=1000, Reward=0.0, Position=[ 0 31], Goal reached=True\n",
      "Episode 32: Steps=1000, Reward=0.0, Position=[ 0 30], Goal reached=True\n",
      "Episode 33: Steps=1000, Reward=0.0, Position=[ 0 31], Goal reached=True\n",
      "Episode 34: Steps=1000, Reward=0.0, Position=[ 0 30], Goal reached=True\n",
      "Episode 35: Steps=1000, Reward=0.0, Position=[ 0 22], Goal reached=True\n",
      "Episode 36: Steps=1000, Reward=0.0, Position=[ 0 31], Goal reached=True\n",
      "Episode 37: Steps=1000, Reward=0.0, Position=[ 0 17], Goal reached=True\n",
      "Episode 38: Steps=1000, Reward=0.0, Position=[ 0 28], Goal reached=True\n",
      "Episode 39: Steps=1000, Reward=0.0, Position=[ 0 31], Goal reached=True\n",
      "Episode 40: Steps=1000, Reward=0.0, Position=[ 0 25], Goal reached=True\n",
      "Average over last 10 episodes: Reward=0.00, Length=1000.00\n",
      "Episode 41: Steps=1000, Reward=0.0, Position=[ 0 27], Goal reached=True\n",
      "Episode 42: Steps=1000, Reward=0.0, Position=[ 0 25], Goal reached=True\n",
      "Episode 43: Steps=1000, Reward=0.0, Position=[ 0 24], Goal reached=True\n",
      "Episode 44: Steps=1000, Reward=0.0, Position=[ 0 29], Goal reached=True\n",
      "Episode 45: Steps=1000, Reward=0.0, Position=[ 2 29], Goal reached=True\n",
      "Episode 46: Steps=1000, Reward=0.0, Position=[ 1 29], Goal reached=True\n",
      "Episode 47: Steps=1000, Reward=0.0, Position=[ 1 29], Goal reached=True\n",
      "Episode 48: Steps=1000, Reward=0.0, Position=[ 0 26], Goal reached=True\n",
      "Episode 49: Steps=1000, Reward=0.0, Position=[ 0 31], Goal reached=True\n",
      "Episode 50: Steps=1000, Reward=0.0, Position=[0 9], Goal reached=True\n",
      "Average over last 10 episodes: Reward=0.00, Length=1000.00\n",
      "Episode 51: Steps=1000, Reward=0.0, Position=[0 5], Goal reached=True\n",
      "Episode 52: Steps=1000, Reward=0.0, Position=[ 0 22], Goal reached=True\n",
      "Episode 53: Steps=1000, Reward=0.0, Position=[ 0 24], Goal reached=True\n",
      "Episode 54: Steps=1000, Reward=0.0, Position=[ 1 18], Goal reached=True\n",
      "Episode 55: Steps=1000, Reward=0.0, Position=[ 0 15], Goal reached=True\n",
      "Episode 56: Steps=1000, Reward=0.0, Position=[ 1 11], Goal reached=True\n",
      "Episode 57: Steps=1000, Reward=0.0, Position=[0 8], Goal reached=True\n",
      "Episode 58: Steps=1000, Reward=0.0, Position=[0 1], Goal reached=True\n",
      "Episode 59: Steps=1000, Reward=0.0, Position=[3 6], Goal reached=True\n",
      "Episode 60: Steps=1000, Reward=0.0, Position=[0 6], Goal reached=True\n",
      "Average over last 10 episodes: Reward=0.00, Length=1000.00\n",
      "Episode 61: Steps=1000, Reward=0.0, Position=[1 0], Goal reached=True\n",
      "Episode 62: Steps=1000, Reward=0.0, Position=[0 2], Goal reached=True\n",
      "Episode 63: Steps=1000, Reward=0.0, Position=[0 0], Goal reached=True\n",
      "Episode 64: Steps=1000, Reward=0.0, Position=[1 4], Goal reached=True\n",
      "Episode 65: Steps=1000, Reward=0.0, Position=[0 0], Goal reached=True\n",
      "Episode 66: Steps=1000, Reward=0.0, Position=[1 1], Goal reached=True\n",
      "Episode 67: Steps=1000, Reward=0.0, Position=[0 1], Goal reached=True\n",
      "Episode 68: Steps=1000, Reward=0.0, Position=[0 4], Goal reached=True\n",
      "Episode 69: Steps=1000, Reward=0.0, Position=[0 0], Goal reached=True\n",
      "Episode 70: Steps=1000, Reward=0.0, Position=[0 2], Goal reached=True\n",
      "Average over last 10 episodes: Reward=0.00, Length=1000.00\n",
      "Episode 71: Steps=1000, Reward=0.0, Position=[1 3], Goal reached=True\n",
      "Episode 72: Steps=1000, Reward=0.0, Position=[2 5], Goal reached=True\n",
      "Episode 73: Steps=1000, Reward=0.0, Position=[0 0], Goal reached=True\n",
      "Episode 74: Steps=1000, Reward=0.0, Position=[0 7], Goal reached=True\n",
      "Episode 75: Steps=1000, Reward=0.0, Position=[1 1], Goal reached=True\n",
      "Episode 76: Steps=1000, Reward=0.0, Position=[0 5], Goal reached=True\n",
      "Episode 77: Steps=1000, Reward=0.0, Position=[0 0], Goal reached=True\n",
      "Episode 78: Steps=1000, Reward=0.0, Position=[1 0], Goal reached=True\n",
      "Episode 79: Steps=1000, Reward=0.0, Position=[0 2], Goal reached=True\n",
      "Episode 80: Steps=1000, Reward=0.0, Position=[0 1], Goal reached=True\n",
      "Average over last 10 episodes: Reward=0.00, Length=1000.00\n",
      "Episode 81: Steps=1000, Reward=0.0, Position=[1 0], Goal reached=True\n",
      "Episode 82: Steps=1000, Reward=0.0, Position=[0 1], Goal reached=True\n",
      "Episode 83: Steps=1000, Reward=0.0, Position=[0 0], Goal reached=True\n",
      "Episode 84: Steps=1000, Reward=0.0, Position=[1 0], Goal reached=True\n",
      "Episode 85: Steps=1000, Reward=0.0, Position=[0 2], Goal reached=True\n",
      "Episode 86: Steps=1000, Reward=0.0, Position=[0 0], Goal reached=True\n",
      "Episode 87: Steps=1000, Reward=0.0, Position=[0 0], Goal reached=True\n",
      "Episode 88: Steps=1000, Reward=0.0, Position=[0 0], Goal reached=True\n",
      "Episode 89: Steps=1000, Reward=0.0, Position=[0 2], Goal reached=True\n",
      "Episode 90: Steps=1000, Reward=0.0, Position=[0 1], Goal reached=True\n",
      "Average over last 10 episodes: Reward=0.00, Length=1000.00\n",
      "Episode 91: Steps=1000, Reward=0.0, Position=[0 2], Goal reached=True\n",
      "Episode 92: Steps=1000, Reward=0.0, Position=[0 0], Goal reached=True\n",
      "Episode 93: Steps=1000, Reward=0.0, Position=[0 0], Goal reached=True\n",
      "Episode 94: Steps=1000, Reward=0.0, Position=[0 0], Goal reached=True\n",
      "Episode 95: Steps=1000, Reward=0.0, Position=[0 0], Goal reached=True\n",
      "Episode 96: Steps=1000, Reward=0.0, Position=[1 6], Goal reached=True\n",
      "Episode 97: Steps=1000, Reward=0.0, Position=[1 0], Goal reached=True\n",
      "Episode 98: Steps=1000, Reward=0.0, Position=[0 0], Goal reached=True\n",
      "Episode 99: Steps=1000, Reward=0.0, Position=[0 0], Goal reached=True\n",
      "Episode 100: Steps=1000, Reward=0.0, Position=[0 0], Goal reached=True\n",
      "Average over last 10 episodes: Reward=0.00, Length=1000.00\n",
      "\n",
      "Training completed!\n",
      "Final average reward (last 10 episodes): 0.00\n",
      "\n",
      "Testing trained agent...\n",
      "Test - Step 50: Position=[9 0], Reward=0.0\n",
      "Test - Step 100: Position=[7 0], Reward=0.0\n",
      "Test - Step 150: Position=[0 0], Reward=0.0\n",
      "Test - Step 200: Position=[3 1], Reward=0.0\n",
      "Test - Step 250: Position=[1 2], Reward=0.0\n",
      "Test - Step 300: Position=[0 3], Reward=0.0\n",
      "Test - Step 350: Position=[0 1], Reward=0.0\n",
      "Test - Step 400: Position=[1 0], Reward=0.0\n",
      "Test - Step 450: Position=[0 0], Reward=0.0\n",
      "Test - Step 500: Position=[0 0], Reward=0.0\n",
      "\n",
      "Test completed after 500 steps with reward: 0.0\n",
      "Final position: [0 0]\n",
      "Goal reached: False\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from src.env_sailing import SailingEnv\n",
    "from src.initial_windfields import get_initial_windfield\n",
    "\n",
    "# Créer notre agent PPO\n",
    "ppo_agent = PPOSailingAgent(\n",
    "    learning_rate=0.0003,\n",
    "    gamma=0.99,\n",
    "    gae_lambda=0.95,\n",
    "    clip_param=0.2,\n",
    "    value_coef=0.5,\n",
    "    entropy_coef=0.01,\n",
    "    max_grad_norm=0.5,\n",
    "    ppo_epochs=4,\n",
    "    mini_batch_size=64,\n",
    "    use_sailing_features=True\n",
    ")\n",
    "\n",
    "# Définir une graine fixe pour la reproductibilité\n",
    "np.random.seed(42)\n",
    "ppo_agent.seed(42)\n",
    "\n",
    "# Créer l'environnement avec un champ de vent\n",
    "env = SailingEnv(**get_initial_windfield('simple_static'))\n",
    "\n",
    "# Paramètres d'entraînement\n",
    "num_episodes = 100\n",
    "max_steps = 1000\n",
    "batch_size = 2048  # Taille du lot pour PPO (nombre d'étapes avant la mise à jour)\n",
    "update_frequency = 2048  # Fréquence de mise à jour (en pas)\n",
    "\n",
    "# Variables pour le suivi\n",
    "total_steps = 0\n",
    "episode_rewards = []\n",
    "episode_lengths = []\n",
    "\n",
    "# Boucle d'entraînement\n",
    "print(\"Starting PPO training...\")\n",
    "for episode in range(num_episodes):\n",
    "    # Réinitialiser l'environnement et obtenir l'état initial\n",
    "    observation, info = env.reset(seed=episode)\n",
    "    \n",
    "    # Informer l'agent de la position de l'objectif\n",
    "    ppo_agent.goal_position = env.goal_position if hasattr(env, 'goal_position') else np.array([16, 31])\n",
    "    \n",
    "    episode_reward = 0\n",
    "    episode_length = 0\n",
    "    \n",
    "    # Boucle d'un épisode\n",
    "    for step in range(max_steps):\n",
    "        # Sélectionner et exécuter une action\n",
    "        action = ppo_agent.act(observation)\n",
    "        next_observation, reward, done, truncated, info = env.step(action)\n",
    "        \n",
    "        # Stocker la transition\n",
    "        ppo_agent.remember(observation, action, reward, next_observation, done or truncated)\n",
    "        \n",
    "        # Mettre à jour l'observation et les compteurs\n",
    "        observation = next_observation\n",
    "        episode_reward += reward\n",
    "        episode_length += 1\n",
    "        total_steps += 1\n",
    "        \n",
    "        # Apprendre si c'est le moment\n",
    "        if total_steps % update_frequency == 0:\n",
    "            ppo_agent.learn()\n",
    "        \n",
    "        # Sortir si l'épisode est terminé\n",
    "        if done or truncated:\n",
    "            # Finaliser la trajectoire\n",
    "            if not done:  # Si tronqué mais pas terminé, estimer la valeur finale\n",
    "                # Convertir l'observation en caractéristiques si nécessaire\n",
    "                if ppo_agent.use_sailing_features:\n",
    "                    state = ppo_agent.extract_features(observation)\n",
    "                else:\n",
    "                    state = observation\n",
    "                \n",
    "                # Obtenir la valeur d'état finale\n",
    "                state_tensor = torch.FloatTensor(state).unsqueeze(0).to(ppo_agent.device)\n",
    "                with torch.no_grad():\n",
    "                    _, state_value = ppo_agent.policy_value_network(state_tensor)\n",
    "                last_value = state_value.item()\n",
    "            else:\n",
    "                last_value = 0  # Épisode terminé, pas de récompense future\n",
    "            \n",
    "            ppo_agent.memory.finish_trajectory(last_value=last_value)\n",
    "            break\n",
    "    \n",
    "    # Enregistrer les statistiques de l'épisode\n",
    "    episode_rewards.append(episode_reward)\n",
    "    episode_lengths.append(episode_length)\n",
    "    \n",
    "    # Afficher les progrès\n",
    "    avg_reward = np.mean(episode_rewards[-10:]) if len(episode_rewards) >= 10 else np.mean(episode_rewards)\n",
    "    avg_length = np.mean(episode_lengths[-10:]) if len(episode_lengths) >= 10 else np.mean(episode_lengths)\n",
    "    \n",
    "    print(f\"Episode {episode+1}: Steps={episode_length}, Reward={episode_reward}, \" +\n",
    "          f\"Position={info['position']}, Goal reached={done}\")\n",
    "    \n",
    "    if (episode + 1) % 10 == 0:\n",
    "        print(f\"Average over last 10 episodes: Reward={avg_reward:.2f}, Length={avg_length:.2f}\")\n",
    "        \n",
    "        # Sauvegarder le modèle périodiquement\n",
    "        ppo_agent.save(f\"ppo_sailing_agent_ep{episode+1}.pt\")\n",
    "\n",
    "print(\"\\nTraining completed!\")\n",
    "print(f\"Final average reward (last 10 episodes): {np.mean(episode_rewards[-10:]):.2f}\")\n",
    "\n",
    "# Sauvegarder le modèle final\n",
    "ppo_agent.save(\"ppo_sailing_agent_final.pt\")\n",
    "\n",
    "# Test de l'agent entraîné\n",
    "print(\"\\nTesting trained agent...\")\n",
    "observation, info = env.reset(seed=1000)  # Nouvelle graine pour le test\n",
    "total_reward = 0\n",
    "step_count = 0\n",
    "\n",
    "# Désactiver l'exploration pour le test\n",
    "is_training = False\n",
    "\n",
    "while step_count < 500:\n",
    "    action = ppo_agent.act(observation)\n",
    "    observation, reward, done, truncated, info = env.step(action)\n",
    "    total_reward += reward\n",
    "    step_count += 1\n",
    "    \n",
    "    if step_count % 50 == 0:\n",
    "       print(f\"Test - Step {step_count}: Position={info['position']}, Reward={reward}\")\n",
    "   \n",
    "    if done or truncated:\n",
    "       break\n",
    "\n",
    "print(f\"\\nTest completed after {step_count} steps with reward: {total_reward}\")\n",
    "print(f\"Final position: {info['position']}\")\n",
    "print(f\"Goal reached: {done}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the trained agent on 5 new episodes...\n",
      "Test Episode 1: Steps=146, Reward=100.0, Position=[15 31], Goal reached=True\n",
      "Test Episode 2: Steps=116, Reward=100.0, Position=[15 30], Goal reached=True\n",
      "Test Episode 3: Steps=99, Reward=100.0, Position=[16 30], Goal reached=True\n",
      "Test Episode 4: Steps=143, Reward=100.0, Position=[16 31], Goal reached=True\n",
      "Test Episode 5: Steps=146, Reward=100.0, Position=[16 30], Goal reached=True\n"
     ]
    }
   ],
   "source": [
    "# Turn off exploration for evaluation\n",
    "ppo_agent.evaluation_mode = True\n",
    "\n",
    "# Create test environment\n",
    "test_env = SailingEnv(**get_initial_windfield('training_1'))\n",
    "\n",
    "# Test parameters\n",
    "num_test_episodes = 5\n",
    "max_steps = 1000\n",
    "\n",
    "print(\"Testing the trained agent on 5 new episodes...\")\n",
    "# Testing loop\n",
    "for episode in range(num_test_episodes):\n",
    "    # Reset environment\n",
    "    observation, info = test_env.reset(seed=1000 + episode)  # Different seeds from training\n",
    "    \n",
    "    total_reward = 0\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        # Select action using learned policy\n",
    "        action = ql_agent_full.act(observation)\n",
    "        observation, reward, done, truncated, info = test_env.step(action)\n",
    "        \n",
    "        total_reward += reward\n",
    "        \n",
    "        # Break if episode is done\n",
    "        if done or truncated:\n",
    "            break\n",
    "    \n",
    "    print(f\"Test Episode {episode+1}: Steps={step+1}, Reward={total_reward}, \" +\n",
    "          f\"Position={info['position']}, Goal reached={done}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
